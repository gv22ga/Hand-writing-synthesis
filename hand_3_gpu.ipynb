{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "#import svgwrite\n",
    "from IPython.display import SVG, display\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def get_bounds(data, factor):\n",
    "  min_x = 0\n",
    "  max_x = 0\n",
    "  min_y = 0\n",
    "  max_y = 0\n",
    "    \n",
    "  abs_x = 0\n",
    "  abs_y = 0\n",
    "  for i in range(len(data)):\n",
    "    x = float(data[i,0])/factor\n",
    "    y = float(data[i,1])/factor\n",
    "    abs_x += x\n",
    "    abs_y += y\n",
    "    min_x = min(min_x, abs_x)\n",
    "    min_y = min(min_y, abs_y)\n",
    "    max_x = max(max_x, abs_x)\n",
    "    max_y = max(max_y, abs_y)\n",
    "    \n",
    "  return (min_x, max_x, min_y, max_y)\n",
    "\n",
    "# old version, where each path is entire stroke (smaller svg size, but have to keep same color)\n",
    "def draw_strokes(data, factor=10, svg_filename = 'sample.svg'):\n",
    "  min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "  dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "    \n",
    "  dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "  dwg.add(dwg.rect(insert=(0, 0), size=dims,fill='white'))\n",
    "\n",
    "  lift_pen = 1\n",
    "    \n",
    "  abs_x = 25 - min_x \n",
    "  abs_y = 25 - min_y\n",
    "  p = \"M%s,%s \" % (abs_x, abs_y)\n",
    "    \n",
    "  command = \"m\"\n",
    "\n",
    "  for i in range(len(data)):\n",
    "    if (lift_pen == 1):\n",
    "      command = \"m\"\n",
    "    elif (command != \"l\"):\n",
    "      command = \"l\"\n",
    "    else:\n",
    "      command = \"\"\n",
    "    x = float(data[i,0])/factor\n",
    "    y = float(data[i,1])/factor\n",
    "    lift_pen = data[i, 2]\n",
    "    p += command+str(x)+\",\"+str(y)+\" \"\n",
    "\n",
    "  the_color = \"black\"\n",
    "  stroke_width = 1\n",
    "\n",
    "  dwg.add(dwg.path(p).stroke(the_color,stroke_width).fill(\"none\"))\n",
    "\n",
    "  dwg.save()\n",
    "  display(SVG(dwg.tostring()))\n",
    "\n",
    "def draw_strokes_eos_weighted(stroke, param, factor=10, svg_filename = 'sample_eos.svg'):\n",
    "  c_data_eos = np.zeros((len(stroke), 3))\n",
    "  for i in range(len(param)):\n",
    "    c_data_eos[i, :] = (1-param[i][6][0])*225 # make color gray scale, darker = more likely to eos\n",
    "  draw_strokes_custom_color(stroke, factor = factor, svg_filename = svg_filename, color_data = c_data_eos, stroke_width = 3)\n",
    "\n",
    "def draw_strokes_random_color(stroke, factor=10, svg_filename = 'sample_random_color.svg', per_stroke_mode = True):\n",
    "  c_data = np.array(np.random.rand(len(stroke), 3)*240, dtype=np.uint8)\n",
    "  if per_stroke_mode:\n",
    "    switch_color = False\n",
    "    for i in range(len(stroke)):\n",
    "      if switch_color == False and i > 0:\n",
    "        c_data[i] = c_data[i-1]\n",
    "      if stroke[i, 2] < 1: # same strike\n",
    "        switch_color = False\n",
    "      else:\n",
    "        switch_color = True\n",
    "  draw_strokes_custom_color(stroke, factor = factor, svg_filename = svg_filename, color_data = c_data, stroke_width = 2)\n",
    "\n",
    "def draw_strokes_custom_color(data, factor=10, svg_filename = 'test.svg', color_data = None, stroke_width = 1):\n",
    "  min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "  dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "    \n",
    "  dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "  dwg.add(dwg.rect(insert=(0, 0), size=dims,fill='white'))\n",
    "\n",
    "  lift_pen = 1\n",
    "  abs_x = 25 - min_x \n",
    "  abs_y = 25 - min_y\n",
    "\n",
    "  for i in range(len(data)):\n",
    "\n",
    "    x = float(data[i,0])/factor\n",
    "    y = float(data[i,1])/factor\n",
    "\n",
    "    prev_x = abs_x\n",
    "    prev_y = abs_y\n",
    "\n",
    "    abs_x += x\n",
    "    abs_y += y\n",
    "\n",
    "    if (lift_pen == 1):\n",
    "      p = \"M \"+str(abs_x)+\",\"+str(abs_y)+\" \"\n",
    "    else:\n",
    "      p = \"M +\"+str(prev_x)+\",\"+str(prev_y)+\" L \"+str(abs_x)+\",\"+str(abs_y)+\" \"\n",
    "\n",
    "    lift_pen = data[i, 2]\n",
    "\n",
    "    the_color = \"black\"\n",
    "\n",
    "    if (color_data is not None):\n",
    "      the_color = \"rgb(\"+str(int(color_data[i, 0]))+\",\"+str(int(color_data[i, 1]))+\",\"+str(int(color_data[i, 2]))+\")\"\n",
    "\n",
    "    dwg.add(dwg.path(p).stroke(the_color,stroke_width).fill(the_color))\n",
    "  dwg.save()\n",
    "  display(SVG(dwg.tostring()))\n",
    "\n",
    "def draw_strokes_pdf(data, param, factor=10, svg_filename = 'sample_pdf.svg'):\n",
    "  min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "  dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "\n",
    "  dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "  dwg.add(dwg.rect(insert=(0, 0), size=dims,fill='white'))\n",
    "\n",
    "  abs_x = 25 - min_x \n",
    "  abs_y = 25 - min_y\n",
    "\n",
    "  num_mixture = len(param[0][0])\n",
    "\n",
    "  for i in range(len(data)):\n",
    "\n",
    "    x = float(data[i,0])/factor\n",
    "    y = float(data[i,1])/factor\n",
    "\n",
    "    for k in range(num_mixture):\n",
    "      pi = param[i][0][k]\n",
    "      if pi > 0.01: # optimisation, ignore pi's less than 1% chance\n",
    "        mu1 = param[i][1][k]\n",
    "        mu2 = param[i][2][k]\n",
    "        s1 = param[i][3][k]\n",
    "        s2 = param[i][4][k]\n",
    "        sigma = np.sqrt(s1*s2)\n",
    "        dwg.add(dwg.circle(center=(abs_x+mu1*factor, abs_y+mu2*factor), r=int(sigma*factor)).fill('red', opacity=pi/(sigma*sigma*factor)))\n",
    "\n",
    "    prev_x = abs_x\n",
    "    prev_y = abs_y\n",
    "\n",
    "    abs_x += x\n",
    "    abs_y += y\n",
    "\n",
    "\n",
    "  dwg.save()\n",
    "  display(SVG(dwg.tostring()))\n",
    "\n",
    "\n",
    "\n",
    "class DataLoader():\n",
    "  def __init__(self, batch_size=50, seq_length=300, scale_factor = 10, limit = 500):\n",
    "    self.data_dir = \"/data\"\n",
    "    self.batch_size = batch_size\n",
    "    self.seq_length = seq_length\n",
    "    self.scale_factor = scale_factor # divide data by this factor\n",
    "    self.limit = limit # removes large noisy gaps in the data\n",
    "\n",
    "    data_file = os.path.join(self.data_dir, \"strokes_training_data.cpkl\")\n",
    "    raw_data_dir = self.data_dir+\"/lineStrokes\"\n",
    "\n",
    "    if not (os.path.exists(data_file)) :\n",
    "        print(\"creating training data pkl file from raw source\")\n",
    "        self.preprocess(raw_data_dir, data_file)\n",
    "\n",
    "    self.load_preprocessed(data_file)\n",
    "    self.reset_batch_pointer()\n",
    "\n",
    "  def preprocess(self, data_dir, data_file):\n",
    "    # create data file from raw xml files from iam handwriting source.\n",
    "\n",
    "    # build the list of xml files\n",
    "    filelist = []\n",
    "    # Set the directory you want to start from\n",
    "    rootDir = data_dir\n",
    "    for dirName, subdirList, fileList in os.walk(rootDir):\n",
    "      #print('Found directory: %s' % dirName)\n",
    "      for fname in fileList:\n",
    "        #print('\\t%s' % fname)\n",
    "        filelist.append(dirName+\"/\"+fname)\n",
    "\n",
    "    # function to read each individual xml file\n",
    "    def getStrokes(filename):\n",
    "      tree = ET.parse(filename)\n",
    "      root = tree.getroot()\n",
    "\n",
    "      result = []\n",
    "\n",
    "      x_offset = 1e20\n",
    "      y_offset = 1e20\n",
    "      y_height = 0\n",
    "      for i in range(1, 4):\n",
    "        x_offset = min(x_offset, float(root[0][i].attrib['x']))\n",
    "        y_offset = min(y_offset, float(root[0][i].attrib['y']))\n",
    "        y_height = max(y_height, float(root[0][i].attrib['y']))\n",
    "      y_height -= y_offset\n",
    "      x_offset -= 100\n",
    "      y_offset -= 100\n",
    "\n",
    "      for stroke in root[1].findall('Stroke'):\n",
    "        points = []\n",
    "        for point in stroke.findall('Point'):\n",
    "          points.append([float(point.attrib['x'])-x_offset,float(point.attrib['y'])-y_offset])\n",
    "        result.append(points)\n",
    "\n",
    "      return result\n",
    "\n",
    "    # converts a list of arrays into a 2d numpy int16 array\n",
    "    def convert_stroke_to_array(stroke):\n",
    "\n",
    "      n_point = 0\n",
    "      for i in range(len(stroke)):\n",
    "        n_point += len(stroke[i])\n",
    "      stroke_data = np.zeros((n_point, 3), dtype=np.int16)\n",
    "\n",
    "      prev_x = 0\n",
    "      prev_y = 0\n",
    "      counter = 0\n",
    "\n",
    "      for j in range(len(stroke)):\n",
    "        for k in range(len(stroke[j])):\n",
    "          stroke_data[counter, 0] = int(stroke[j][k][0]) - prev_x\n",
    "          stroke_data[counter, 1] = int(stroke[j][k][1]) - prev_y\n",
    "          prev_x = int(stroke[j][k][0])\n",
    "          prev_y = int(stroke[j][k][1])\n",
    "          stroke_data[counter, 2] = 0\n",
    "          if (k == (len(stroke[j])-1)): # end of stroke\n",
    "            stroke_data[counter, 2] = 1\n",
    "          counter += 1\n",
    "      return stroke_data\n",
    "\n",
    "    # build stroke database of every xml file inside iam database\n",
    "    strokes = []\n",
    "    for i in range(len(filelist)):\n",
    "      if (filelist[i][-3:] == 'xml'):\n",
    "        print('processing '+filelist[i])\n",
    "        strokes.append(convert_stroke_to_array(getStrokes(filelist[i])))\n",
    "\n",
    "    f = open(data_file,\"wb\")\n",
    "    pickle.dump(strokes, f, protocol=2)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "  def load_preprocessed(self, data_file):\n",
    "    f = open(data_file,\"rb\")\n",
    "    self.raw_data = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # goes thru the list, and only keeps the text entries that have more than seq_length points\n",
    "    self.data = []\n",
    "    self.valid_data =[]\n",
    "    counter = 0\n",
    "\n",
    "    # every 1 in 20 (5%) will be used for validation data\n",
    "    cur_data_counter = 0\n",
    "    for data in self.raw_data:\n",
    "      if len(data) > (self.seq_length+2):\n",
    "        # removes large gaps from the data\n",
    "        data = np.minimum(data, self.limit)\n",
    "        data = np.maximum(data, -self.limit)\n",
    "        data = np.array(data,dtype=np.float32)\n",
    "        data[:,0:2] /= self.scale_factor\n",
    "        cur_data_counter = cur_data_counter + 1\n",
    "        if cur_data_counter % 20 == 0:\n",
    "          self.valid_data.append(data)\n",
    "        else:\n",
    "          self.data.append(data)\n",
    "          counter += int(len(data)/((self.seq_length+2))) # number of equiv batches this datapoint is worth\n",
    "\n",
    "    print(\"train data: {}, valid data: {}\".format(len(self.data), len(self.valid_data)))\n",
    "    # minus 1, since we want the ydata to be a shifted version of x data\n",
    "    self.num_batches = int(counter / self.batch_size)\n",
    "\n",
    "  def validation_data(self):\n",
    "    # returns validation data\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for i in range(self.batch_size):\n",
    "      data = self.valid_data[i%len(self.valid_data)]\n",
    "      idx = 0\n",
    "      x_batch.append(np.copy(data[idx:idx+self.seq_length]))\n",
    "      y_batch.append(np.copy(data[idx+1:idx+self.seq_length+1]))\n",
    "    return x_batch, y_batch\n",
    "\n",
    "  def next_batch(self):\n",
    "    # returns a randomised, seq_length sized portion of the training data\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for i in range(self.batch_size):\n",
    "      data = self.data[self.pointer]\n",
    "      n_batch = int(len(data)/((self.seq_length+2))) # number of equiv batches this datapoint is worth\n",
    "      idx = random.randint(0, len(data)-self.seq_length-2)\n",
    "      x_batch.append(np.copy(data[idx:idx+self.seq_length]))\n",
    "      y_batch.append(np.copy(data[idx+1:idx+self.seq_length+1]))\n",
    "      if random.random() < (1.0/float(n_batch)): # adjust sampling probability.\n",
    "        #if this is a long datapoint, sample this data more with higher probability\n",
    "        self.tick_batch_pointer()\n",
    "    return x_batch, y_batch\n",
    "\n",
    "  def tick_batch_pointer(self):\n",
    "    self.pointer += 1\n",
    "    if (self.pointer >= len(self.data)):\n",
    "      self.pointer = 0\n",
    "  def reset_batch_pointer(self):\n",
    "    self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loader = DataLoader(50, 300, 20)\n",
    "print (data_loader.num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "seq_length = 300\n",
    "batch_size = 3\n",
    "H = 256\n",
    "num_layers = 2\n",
    "num_epochs = 500\n",
    "num_mixture = 20\n",
    "NOUT = 1 + num_mixture * 6\n",
    "learning_rate = 0.0005\n",
    "decay_rate = 0.95\n",
    "save_every = 1\n",
    "model_dir = ''\n",
    "'''\n",
    "class FakeArgParse():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = FakeArgParse()\n",
    "\n",
    "args.rnn_size = 256\n",
    "args.num_layers = 3\n",
    "args.batch_size = 50\n",
    "args.seq_length = 300\n",
    "args.num_epochs = 30\n",
    "args.save_every = 100\n",
    "args.model_dir = '/output/'\n",
    "args.grad_clip = 10\n",
    "args.learning_rate = 0.00005\n",
    "args.decay_rate = 0.95\n",
    "args.num_mixture = 20\n",
    "args.data_scale = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, args, infer=False):\n",
    "        self.args = args\n",
    "        \n",
    "        if infer:\n",
    "            args.batch_size = 1\n",
    "            args.seq_length = 1\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        self.input_x = tf.placeholder(tf.float32, [None,args.seq_length,3], name='x_batch')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None,args.seq_length,3], name='y_batch')\n",
    "        N = tf.shape(self.input_x)[0]\n",
    "        NOUT = 1 + args.num_mixture * 6\n",
    "        W = tf.Variable(np.random.rand(args.rnn_size,NOUT),dtype=tf.float32)\n",
    "        b = tf.Variable(np.zeros((1,NOUT)), dtype=tf.float32)\n",
    "        \n",
    "        def lstm_cell():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(args.rnn_size, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(args.num_layers)], state_is_tuple=True)\n",
    "        self.cell = cell\n",
    "        init_state = cell.zero_state(N, tf.float32)\n",
    "        self.init_state = init_state\n",
    "        states_series, current_state = tf.nn.dynamic_rnn(cell, self.input_x, initial_state=self.init_state)\n",
    "        states_series = tf.reshape(states_series, [-1, args.rnn_size])\n",
    "        self.state_out = tf.identity(current_state, name='state_out')\n",
    "        output = tf.matmul(states_series, W) + b\n",
    "        #[x1, x2, eos] = tf.split(axis=1, num_or_size_splits=3, value=flat_data)\n",
    "        #eos = tf.sigmoid(eos)\n",
    "\n",
    "        flat_target_data = tf.reshape(self.input_y,[-1, 3])\n",
    "        [x1_data, x2_data, eos_data] = tf.split(axis=1, num_or_size_splits=3, value=flat_target_data)\n",
    "\n",
    "        #x1_loss = tf.losses.mean_squared_error(x1_data, x1)\n",
    "        #x2_loss = tf.losses.mean_squared_error(x2_data, x2)\n",
    "        #eos_loss = tf.losses.softmax_cross_entropy(eos_data, eos)\n",
    "\n",
    "        def tf_2d_normal(x1, x2, mu1, mu2, s1, s2, rho):\n",
    "            # eq # 24 and 25 of http://arxiv.org/abs/1308.0850\n",
    "            norm1 = tf.subtract(x1, mu1)\n",
    "            norm2 = tf.subtract(x2, mu2)\n",
    "            s1s2 = tf.multiply(s1, s2)\n",
    "            z = tf.square(tf.div(norm1, s1))+tf.square(tf.div(norm2, s2))-2*tf.div(tf.multiply(rho, tf.multiply(norm1, norm2)), s1s2)\n",
    "            negRho = 1-tf.square(rho)\n",
    "            result = tf.exp(tf.div(-z,2*negRho))\n",
    "            denom = 2*np.pi*tf.multiply(s1s2, tf.sqrt(negRho))\n",
    "            result = tf.div(result, denom)\n",
    "            return result\n",
    "\n",
    "        def get_lossfunc(z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr, z_eos, x1_data, x2_data, eos_data):\n",
    "            result0 = tf_2d_normal(x1_data, x2_data, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr)\n",
    "            # implementing eq # 26 of http://arxiv.org/abs/1308.0850\n",
    "            epsilon = 1e-20\n",
    "            result1 = tf.multiply(result0, z_pi)\n",
    "            result1 = tf.reduce_sum(result1, 1, keep_dims=True)\n",
    "            result1 = -tf.log(tf.maximum(result1, 1e-20)) # at the beginning, some errors are exactly zero.\n",
    "\n",
    "            result2 = tf.multiply(z_eos, eos_data) + tf.multiply(1-z_eos, 1-eos_data)\n",
    "            result2 = -tf.log(result2)\n",
    "\n",
    "            result = result1 + result2\n",
    "            return tf.reduce_sum(result)\n",
    "\n",
    "        # below is where we need to do MDN splitting of distribution params\n",
    "        def get_mixture_coef(output):\n",
    "            # returns the tf slices containing mdn dist params\n",
    "            # ie, eq 18 -> 23 of http://arxiv.org/abs/1308.0850\n",
    "            z = output\n",
    "            z_eos = z[:, 0:1]\n",
    "            z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr = tf.split(axis=1, num_or_size_splits=6, value=z[:, 1:])\n",
    "\n",
    "            # process output z's into MDN paramters\n",
    "\n",
    "            # end of stroke signal\n",
    "            z_eos = tf.sigmoid(z_eos) # should be negated, but doesn't matter.\n",
    "\n",
    "            # softmax all the pi's:\n",
    "            max_pi = tf.reduce_max(z_pi, 1, keep_dims=True)\n",
    "            z_pi = tf.subtract(z_pi, max_pi)\n",
    "            z_pi = tf.exp(z_pi)\n",
    "            normalize_pi = tf.reciprocal(tf.reduce_sum(z_pi, 1, keep_dims=True))\n",
    "            z_pi = tf.multiply(normalize_pi, z_pi)\n",
    "\n",
    "            # exponentiate the sigmas and also make corr between -1 and 1.\n",
    "            z_sigma1 = tf.exp(z_sigma1)\n",
    "            z_sigma2 = tf.exp(z_sigma2)\n",
    "            z_corr = tf.tanh(z_corr)\n",
    "\n",
    "            return [z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr, z_eos]\n",
    "\n",
    "        [o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, o_eos] = get_mixture_coef(output)\n",
    "        \n",
    "        self.pi = o_pi\n",
    "        self.mu1 = o_mu1\n",
    "        self.mu2 = o_mu2\n",
    "        self.sigma1 = o_sigma1\n",
    "        self.sigma2 = o_sigma2\n",
    "        self.corr = o_corr\n",
    "        self.eos = o_eos\n",
    "\n",
    "        lossfunc = get_lossfunc(o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, o_eos, x1_data, x2_data, eos_data)\n",
    "        self.cost = lossfunc / (args.batch_size * args.seq_length)\n",
    "\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 10.)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    def sample(self, sess, num=1200):\n",
    "        def get_pi_idx(x, pdf):\n",
    "            N = pdf.size\n",
    "            accumulate = 0\n",
    "            for i in range(0, N):\n",
    "                accumulate += pdf[i]\n",
    "                if (accumulate >= x):\n",
    "                    return i\n",
    "            print('error with sampling ensemble')\n",
    "            return -1\n",
    "\n",
    "        def sample_gaussian_2d(mu1, mu2, s1, s2, rho):\n",
    "            mean = [mu1, mu2]\n",
    "            cov = [[s1*s1, rho*s1*s2], [rho*s1*s2, s2*s2]]\n",
    "            x = np.random.multivariate_normal(mean, cov, 1)\n",
    "            return x[0][0], x[0][1]\n",
    "\n",
    "        prev_x = np.zeros((1, 1, 3), dtype=np.float32)\n",
    "        prev_x[0, 0, 2] = 1 # initially, we want to see beginning of new stroke\n",
    "        prev_state = sess.run(self.cell.zero_state(1, tf.float32))\n",
    "        strokes = np.zeros((num, 3), dtype=np.float32)\n",
    "        mixture_params = []\n",
    "\n",
    "        for i in range(num):\n",
    "\n",
    "            feed = {self.input_x: prev_x, self.init_state:prev_state}\n",
    "\n",
    "            [o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, o_eos, next_state] = sess.run([self.pi, self.mu1, self.mu2, self.sigma1, self.sigma2, self.corr, self.eos, self.state_out],feed)\n",
    "\n",
    "            idx = get_pi_idx(random.random(), o_pi[0])\n",
    "\n",
    "            eos = 1 if random.random() < o_eos[0][0] else 0\n",
    "\n",
    "            next_x1, next_x2 = sample_gaussian_2d(o_mu1[0][idx], o_mu2[0][idx], o_sigma1[0][idx], o_sigma2[0][idx], o_corr[0][idx])\n",
    "\n",
    "            strokes[i,:] = [next_x1, next_x2, eos]\n",
    "\n",
    "            params = [o_pi[0], o_mu1[0], o_mu2[0], o_sigma1[0], o_sigma2[0], o_corr[0], o_eos[0]]\n",
    "            mixture_params.append(params)\n",
    "\n",
    "            prev_x = np.zeros((1, 1, 3), dtype=np.float32)\n",
    "            prev_x[0][0] = np.array([next_x1, next_x2, eos], dtype=np.float32)\n",
    "            \n",
    "            prev_state = tuple(\n",
    "                [tf.contrib.rnn.LSTMStateTuple(next_state[idx][0],next_state[idx][1])\n",
    "                for idx in range(args.num_layers)]\n",
    "            )\n",
    "\n",
    "        strokes[:,0:2] *= self.args.data_scale\n",
    "        return strokes, mixture_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3c37bc8574b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#saver = tf.train.Saver(tf.global_variables())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "model = Model(args)\n",
    "\n",
    "#saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state('/model_data/')\n",
    "    print(\"loading model: \", ckpt.model_checkpoint_path)\n",
    "\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    #tf.global_variables_initializer().run()\n",
    "    #sess.run(tf.assign(lr, learning_rate))\n",
    "    for e in range(args.num_epochs):\n",
    "        sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** e)))\n",
    "        data_loader.reset_batch_pointer()\n",
    "        \n",
    "        #v_x, v_y = data_loader.validation_data()\n",
    "        #valid_feed = {x_batch: v_x, y_batch: v_y}\n",
    "        #for b in range(2):\n",
    "        #    i = e * 1 + b\n",
    "        for b in range(data_loader.num_batches):\n",
    "            i = e * data_loader.num_batches + b\n",
    "            start = time.time()\n",
    "            x, y = data_loader.next_batch()\n",
    "            feed = {model.input_x: x, model.input_y: y}\n",
    "            train_loss, _ = sess.run([model.cost, model.train_op], feed)\n",
    "            #valid_loss, _ = sess.run([total_loss, train_step], valid_feed)\n",
    "            end = time.time()\n",
    "            print(\n",
    "                \"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"  \\\n",
    "                .format(\n",
    "                    i,\n",
    "                    args.num_epochs * data_loader.num_batches,\n",
    "                    e, \n",
    "                    train_loss, end - start))\n",
    "            if (e * data_loader.num_batches + b) % args.save_every == 0 and ((e * data_loader.num_batches + b) > 0):\n",
    "                    checkpoint_path = os.path.join(args.model_dir, 'model')\n",
    "                    saver.save(sess, checkpoint_path)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Model(args, True)\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state('/model_data/')\n",
    "print(\"loading model: \", ckpt.model_checkpoint_path)\n",
    "\n",
    "saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
