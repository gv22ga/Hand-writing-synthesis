{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import svgwrite\n",
    "from IPython.display import SVG, display\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def get_bounds(data, factor):\n",
    "  min_x = 0\n",
    "  max_x = 0\n",
    "  min_y = 0\n",
    "  max_y = 0\n",
    "    \n",
    "  abs_x = 0\n",
    "  abs_y = 0\n",
    "  for i in range(len(data)):\n",
    "    x = float(data[i,0])/factor\n",
    "    y = float(data[i,1])/factor\n",
    "    abs_x += x\n",
    "    abs_y += y\n",
    "    min_x = min(min_x, abs_x)\n",
    "    min_y = min(min_y, abs_y)\n",
    "    max_x = max(max_x, abs_x)\n",
    "    max_y = max(max_y, abs_y)\n",
    "    \n",
    "  return (min_x, max_x, min_y, max_y)\n",
    "\n",
    "# old version, where each path is entire stroke (smaller svg size, but have to keep same color)\n",
    "def draw_strokes(data, factor=10, svg_filename = 'sample.svg'):\n",
    "  min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "  dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "    \n",
    "  dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "  dwg.add(dwg.rect(insert=(0, 0), size=dims,fill='white'))\n",
    "\n",
    "  lift_pen = 1\n",
    "    \n",
    "  abs_x = 25 - min_x \n",
    "  abs_y = 25 - min_y\n",
    "  p = \"M%s,%s \" % (abs_x, abs_y)\n",
    "    \n",
    "  command = \"m\"\n",
    "\n",
    "  for i in range(len(data)):\n",
    "    if (lift_pen == 1):\n",
    "      command = \"m\"\n",
    "    elif (command != \"l\"):\n",
    "      command = \"l\"\n",
    "    else:\n",
    "      command = \"\"\n",
    "    x = float(data[i,0])/factor\n",
    "    y = float(data[i,1])/factor\n",
    "    lift_pen = data[i, 2]\n",
    "    p += command+str(x)+\",\"+str(y)+\" \"\n",
    "\n",
    "  the_color = \"black\"\n",
    "  stroke_width = 1\n",
    "\n",
    "  dwg.add(dwg.path(p).stroke(the_color,stroke_width).fill(\"none\"))\n",
    "\n",
    "  dwg.save()\n",
    "  display(SVG(dwg.tostring()))\n",
    "\n",
    "def draw_strokes_eos_weighted(stroke, param, factor=10, svg_filename = 'sample_eos.svg'):\n",
    "  c_data_eos = np.zeros((len(stroke), 3))\n",
    "  for i in range(len(param)):\n",
    "    c_data_eos[i, :] = (1-param[i][6][0])*225 # make color gray scale, darker = more likely to eos\n",
    "  draw_strokes_custom_color(stroke, factor = factor, svg_filename = svg_filename, color_data = c_data_eos, stroke_width = 3)\n",
    "\n",
    "def draw_strokes_random_color(stroke, factor=10, svg_filename = 'sample_random_color.svg', per_stroke_mode = True):\n",
    "  c_data = np.array(np.random.rand(len(stroke), 3)*240, dtype=np.uint8)\n",
    "  if per_stroke_mode:\n",
    "    switch_color = False\n",
    "    for i in range(len(stroke)):\n",
    "      if switch_color == False and i > 0:\n",
    "        c_data[i] = c_data[i-1]\n",
    "      if stroke[i, 2] < 1: # same strike\n",
    "        switch_color = False\n",
    "      else:\n",
    "        switch_color = True\n",
    "  draw_strokes_custom_color(stroke, factor = factor, svg_filename = svg_filename, color_data = c_data, stroke_width = 2)\n",
    "\n",
    "def draw_strokes_custom_color(data, factor=10, svg_filename = 'test.svg', color_data = None, stroke_width = 1):\n",
    "  min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "  dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "    \n",
    "  dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "  dwg.add(dwg.rect(insert=(0, 0), size=dims,fill='white'))\n",
    "\n",
    "  lift_pen = 1\n",
    "  abs_x = 25 - min_x \n",
    "  abs_y = 25 - min_y\n",
    "\n",
    "  for i in range(len(data)):\n",
    "\n",
    "    x = float(data[i,0])/factor\n",
    "    y = float(data[i,1])/factor\n",
    "\n",
    "    prev_x = abs_x\n",
    "    prev_y = abs_y\n",
    "\n",
    "    abs_x += x\n",
    "    abs_y += y\n",
    "\n",
    "    if (lift_pen == 1):\n",
    "      p = \"M \"+str(abs_x)+\",\"+str(abs_y)+\" \"\n",
    "    else:\n",
    "      p = \"M +\"+str(prev_x)+\",\"+str(prev_y)+\" L \"+str(abs_x)+\",\"+str(abs_y)+\" \"\n",
    "\n",
    "    lift_pen = data[i, 2]\n",
    "\n",
    "    the_color = \"black\"\n",
    "\n",
    "    if (color_data is not None):\n",
    "      the_color = \"rgb(\"+str(int(color_data[i, 0]))+\",\"+str(int(color_data[i, 1]))+\",\"+str(int(color_data[i, 2]))+\")\"\n",
    "\n",
    "    dwg.add(dwg.path(p).stroke(the_color,stroke_width).fill(the_color))\n",
    "  dwg.save()\n",
    "  display(SVG(dwg.tostring()))\n",
    "\n",
    "def draw_strokes_pdf(data, param, factor=10, svg_filename = 'sample_pdf.svg'):\n",
    "  min_x, max_x, min_y, max_y = get_bounds(data, factor)\n",
    "  dims = (50 + max_x - min_x, 50 + max_y - min_y)\n",
    "\n",
    "  dwg = svgwrite.Drawing(svg_filename, size=dims)\n",
    "  dwg.add(dwg.rect(insert=(0, 0), size=dims,fill='white'))\n",
    "\n",
    "  abs_x = 25 - min_x \n",
    "  abs_y = 25 - min_y\n",
    "\n",
    "  num_mixture = len(param[0][0])\n",
    "\n",
    "  for i in range(len(data)):\n",
    "\n",
    "    x = float(data[i,0])/factor\n",
    "    y = float(data[i,1])/factor\n",
    "\n",
    "    for k in range(num_mixture):\n",
    "      pi = param[i][0][k]\n",
    "      if pi > 0.01: # optimisation, ignore pi's less than 1% chance\n",
    "        mu1 = param[i][1][k]\n",
    "        mu2 = param[i][2][k]\n",
    "        s1 = param[i][3][k]\n",
    "        s2 = param[i][4][k]\n",
    "        sigma = np.sqrt(s1*s2)\n",
    "        dwg.add(dwg.circle(center=(abs_x+mu1*factor, abs_y+mu2*factor), r=int(sigma*factor)).fill('red', opacity=pi/(sigma*sigma*factor)))\n",
    "\n",
    "    prev_x = abs_x\n",
    "    prev_y = abs_y\n",
    "\n",
    "    abs_x += x\n",
    "    abs_y += y\n",
    "\n",
    "\n",
    "  dwg.save()\n",
    "  display(SVG(dwg.tostring()))\n",
    "\n",
    "\n",
    "\n",
    "class DataLoader():\n",
    "  def __init__(self, data_dir, batch_size=50, seq_length=300, scale_factor = 10, limit = 500):\n",
    "    self.data_dir = data_dir\n",
    "    self.batch_size = batch_size\n",
    "    self.seq_length = seq_length\n",
    "    self.scale_factor = scale_factor # divide data by this factor\n",
    "    self.limit = limit # removes large noisy gaps in the data\n",
    "\n",
    "    data_file = os.path.join(self.data_dir, \"strokes_training_data.cpkl\")\n",
    "    raw_data_dir = self.data_dir+\"/lineStrokes\"\n",
    "\n",
    "    if not (os.path.exists(data_file)) :\n",
    "        print(\"creating training data pkl file from raw source\")\n",
    "        self.preprocess(raw_data_dir, data_file)\n",
    "\n",
    "    self.load_preprocessed(data_file)\n",
    "    self.reset_batch_pointer()\n",
    "\n",
    "  def preprocess(self, data_dir, data_file):\n",
    "    # create data file from raw xml files from iam handwriting source.\n",
    "\n",
    "    # build the list of xml files\n",
    "    filelist = []\n",
    "    # Set the directory you want to start from\n",
    "    rootDir = data_dir\n",
    "    for dirName, subdirList, fileList in os.walk(rootDir):\n",
    "      #print('Found directory: %s' % dirName)\n",
    "      for fname in fileList:\n",
    "        #print('\\t%s' % fname)\n",
    "        filelist.append(dirName+\"/\"+fname)\n",
    "\n",
    "    # function to read each individual xml file\n",
    "    def getStrokes(filename):\n",
    "      tree = ET.parse(filename)\n",
    "      root = tree.getroot()\n",
    "\n",
    "      result = []\n",
    "\n",
    "      x_offset = 1e20\n",
    "      y_offset = 1e20\n",
    "      y_height = 0\n",
    "      for i in range(1, 4):\n",
    "        x_offset = min(x_offset, float(root[0][i].attrib['x']))\n",
    "        y_offset = min(y_offset, float(root[0][i].attrib['y']))\n",
    "        y_height = max(y_height, float(root[0][i].attrib['y']))\n",
    "      y_height -= y_offset\n",
    "      x_offset -= 100\n",
    "      y_offset -= 100\n",
    "\n",
    "      for stroke in root[1].findall('Stroke'):\n",
    "        points = []\n",
    "        for point in stroke.findall('Point'):\n",
    "          points.append([float(point.attrib['x'])-x_offset,float(point.attrib['y'])-y_offset])\n",
    "        result.append(points)\n",
    "\n",
    "      return result\n",
    "\n",
    "    # converts a list of arrays into a 2d numpy int16 array\n",
    "    def convert_stroke_to_array(stroke):\n",
    "\n",
    "      n_point = 0\n",
    "      for i in range(len(stroke)):\n",
    "        n_point += len(stroke[i])\n",
    "      stroke_data = np.zeros((n_point, 3), dtype=np.int16)\n",
    "\n",
    "      prev_x = 0\n",
    "      prev_y = 0\n",
    "      counter = 0\n",
    "\n",
    "      for j in range(len(stroke)):\n",
    "        for k in range(len(stroke[j])):\n",
    "          stroke_data[counter, 0] = int(stroke[j][k][0]) - prev_x\n",
    "          stroke_data[counter, 1] = int(stroke[j][k][1]) - prev_y\n",
    "          prev_x = int(stroke[j][k][0])\n",
    "          prev_y = int(stroke[j][k][1])\n",
    "          stroke_data[counter, 2] = 0\n",
    "          if (k == (len(stroke[j])-1)): # end of stroke\n",
    "            stroke_data[counter, 2] = 1\n",
    "          counter += 1\n",
    "      return stroke_data\n",
    "\n",
    "    # build stroke database of every xml file inside iam database\n",
    "    strokes = []\n",
    "    for i in range(len(filelist)):\n",
    "      if (filelist[i][-3:] == 'xml'):\n",
    "        print('processing '+filelist[i])\n",
    "        strokes.append(convert_stroke_to_array(getStrokes(filelist[i])))\n",
    "\n",
    "    f = open(data_file,\"wb\")\n",
    "    pickle.dump(strokes, f, protocol=2)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "  def load_preprocessed(self, data_file):\n",
    "    f = open(data_file,\"rb\")\n",
    "    self.ra_data = pickle.load(f)\n",
    "    self.raw_data = self.ra_data[:50]\n",
    "    f.close()\n",
    "\n",
    "    # goes thru the list, and only keeps the text entries that have more than seq_length points\n",
    "    self.data = []\n",
    "    self.valid_data =[]\n",
    "    counter = 0\n",
    "\n",
    "    # every 1 in 20 (5%) will be used for validation data\n",
    "    cur_data_counter = 0\n",
    "    for data in self.raw_data:\n",
    "      if len(data) > (self.seq_length+2):\n",
    "        # removes large gaps from the data\n",
    "        data = np.minimum(data, self.limit)\n",
    "        data = np.maximum(data, -self.limit)\n",
    "        data = np.array(data,dtype=np.float32)\n",
    "        data[:,0:2] /= self.scale_factor\n",
    "        cur_data_counter = cur_data_counter + 1\n",
    "        if cur_data_counter % 20 == 0:\n",
    "          self.valid_data.append(data)\n",
    "        else:\n",
    "          self.data.append(data)\n",
    "          counter += int(len(data)/((self.seq_length+2))) # number of equiv batches this datapoint is worth\n",
    "\n",
    "    print(\"train data: {}, valid data: {}\".format(len(self.data), len(self.valid_data)))\n",
    "    # minus 1, since we want the ydata to be a shifted version of x data\n",
    "    self.num_batches = int(counter / self.batch_size)\n",
    "\n",
    "  def validation_data(self):\n",
    "    # returns validation data\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for i in range(self.batch_size):\n",
    "      data = self.valid_data[i%len(self.valid_data)]\n",
    "      idx = 0\n",
    "      x_batch.append(np.copy(data[idx:idx+self.seq_length]))\n",
    "      y_batch.append(np.copy(data[idx+1:idx+self.seq_length+1]))\n",
    "    return x_batch, y_batch\n",
    "\n",
    "  def next_batch(self):\n",
    "    # returns a randomised, seq_length sized portion of the training data\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for i in range(self.batch_size):\n",
    "      data = self.data[self.pointer]\n",
    "      n_batch = int(len(data)/((self.seq_length+2))) # number of equiv batches this datapoint is worth\n",
    "      idx = random.randint(0, len(data)-self.seq_length-2)\n",
    "      x_batch.append(np.copy(data[idx:idx+self.seq_length]))\n",
    "      y_batch.append(np.copy(data[idx+1:idx+self.seq_length+1]))\n",
    "      if random.random() < (1.0/float(n_batch)): # adjust sampling probability.\n",
    "        #if this is a long datapoint, sample this data more with higher probability\n",
    "        self.tick_batch_pointer()\n",
    "    return x_batch, y_batch\n",
    "\n",
    "  def tick_batch_pointer(self):\n",
    "    self.pointer += 1\n",
    "    if (self.pointer >= len(self.data)):\n",
    "      self.pointer = 0\n",
    "  def reset_batch_pointer(self):\n",
    "    self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FakeArgParse():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "args = FakeArgParse()\n",
    "\n",
    "args.rnn_size = 256\n",
    "args.num_layers = 2\n",
    "args.batch_size = 25\n",
    "args.seq_length = 300\n",
    "args.num_epochs = 500\n",
    "args.save_every = 50\n",
    "args.model_dir = ''\n",
    "args.grad_clip = 10\n",
    "args.learning_rate = 0.0005\n",
    "args.decay_rate = 0.99\n",
    "args.num_mixture = 10\n",
    "args.data_scale = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 46, valid data: 2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader('./data', 25, 300, 20)\n",
    "print (data_loader.num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, args, infer=False):\n",
    "        self.args = args\n",
    "        \n",
    "        if infer:\n",
    "            args.batch_size = 1\n",
    "            args.seq_length = 1\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        self.input_x = tf.placeholder(tf.float32, [None,args.seq_length,3], name='x_batch')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None,args.seq_length,3], name='y_batch')\n",
    "        N = tf.shape(self.input_x)[0]\n",
    "        NOUT = 1 + args.num_mixture * 6\n",
    "        W = tf.Variable(np.random.rand(args.rnn_size,NOUT),dtype=tf.float32)\n",
    "        b = tf.Variable(np.zeros((1,NOUT)), dtype=tf.float32)\n",
    "        \n",
    "        def lstm_cell():\n",
    "            return tf.contrib.rnn.BasicLSTMCell(args.rnn_size, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(args.num_layers)], state_is_tuple=True)\n",
    "        self.cell = cell\n",
    "        init_state = cell.zero_state(args.batch_size, tf.float32)\n",
    "        self.state_in = tf.identity(init_state, name='state_in')\n",
    "        \n",
    "        states_series, current_state = tf.nn.dynamic_rnn(cell, self.input_x, initial_state=init_state)\n",
    "        states_series = tf.reshape(states_series, [-1, args.rnn_size])\n",
    "        self.state_out = tf.identity(current_state, name='state_out')\n",
    "        output = tf.matmul(states_series, W) + b\n",
    "        #[x1, x2, eos] = tf.split(axis=1, num_or_size_splits=3, value=flat_data)\n",
    "        #eos = tf.sigmoid(eos)\n",
    "\n",
    "        flat_target_data = tf.reshape(self.input_y,[-1, 3])\n",
    "        [x1_data, x2_data, eos_data] = tf.split(axis=1, num_or_size_splits=3, value=flat_target_data)\n",
    "\n",
    "        #x1_loss = tf.losses.mean_squared_error(x1_data, x1)\n",
    "        #x2_loss = tf.losses.mean_squared_error(x2_data, x2)\n",
    "        #eos_loss = tf.losses.softmax_cross_entropy(eos_data, eos)\n",
    "\n",
    "        def tf_2d_normal(x1, x2, mu1, mu2, s1, s2, rho):\n",
    "            # eq # 24 and 25 of http://arxiv.org/abs/1308.0850\n",
    "            norm1 = tf.subtract(x1, mu1)\n",
    "            norm2 = tf.subtract(x2, mu2)\n",
    "            s1s2 = tf.multiply(s1, s2)\n",
    "            z = tf.square(tf.div(norm1, s1))+tf.square(tf.div(norm2, s2))-2*tf.div(tf.multiply(rho, tf.multiply(norm1, norm2)), s1s2)\n",
    "            negRho = 1-tf.square(rho)\n",
    "            result = tf.exp(tf.div(-z,2*negRho))\n",
    "            denom = 2*np.pi*tf.multiply(s1s2, tf.sqrt(negRho))\n",
    "            result = tf.div(result, denom)\n",
    "            return result\n",
    "\n",
    "        def get_lossfunc(z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr, z_eos, x1_data, x2_data, eos_data):\n",
    "            result0 = tf_2d_normal(x1_data, x2_data, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr)\n",
    "            # implementing eq # 26 of http://arxiv.org/abs/1308.0850\n",
    "            epsilon = 1e-20\n",
    "            result1 = tf.multiply(result0, z_pi)\n",
    "            result1 = tf.reduce_sum(result1, 1, keep_dims=True)\n",
    "            result1 = -tf.log(tf.maximum(result1, 1e-20)) # at the beginning, some errors are exactly zero.\n",
    "\n",
    "            result2 = tf.multiply(z_eos, eos_data) + tf.multiply(1-z_eos, 1-eos_data)\n",
    "            result2 = -tf.log(result2)\n",
    "\n",
    "            result = result1 + result2\n",
    "            return tf.reduce_sum(result)\n",
    "\n",
    "        # below is where we need to do MDN splitting of distribution params\n",
    "        def get_mixture_coef(output):\n",
    "            # returns the tf slices containing mdn dist params\n",
    "            # ie, eq 18 -> 23 of http://arxiv.org/abs/1308.0850\n",
    "            z = output\n",
    "            z_eos = z[:, 0:1]\n",
    "            z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr = tf.split(axis=1, num_or_size_splits=6, value=z[:, 1:])\n",
    "\n",
    "            # process output z's into MDN paramters\n",
    "\n",
    "            # end of stroke signal\n",
    "            z_eos = tf.sigmoid(z_eos) # should be negated, but doesn't matter.\n",
    "\n",
    "            # softmax all the pi's:\n",
    "            max_pi = tf.reduce_max(z_pi, 1, keep_dims=True)\n",
    "            z_pi = tf.subtract(z_pi, max_pi)\n",
    "            z_pi = tf.exp(z_pi)\n",
    "            normalize_pi = tf.reciprocal(tf.reduce_sum(z_pi, 1, keep_dims=True))\n",
    "            z_pi = tf.multiply(normalize_pi, z_pi)\n",
    "\n",
    "            # exponentiate the sigmas and also make corr between -1 and 1.\n",
    "            z_sigma1 = tf.exp(z_sigma1)\n",
    "            z_sigma2 = tf.exp(z_sigma2)\n",
    "            z_corr = tf.tanh(z_corr)\n",
    "\n",
    "            return [z_pi, z_mu1, z_mu2, z_sigma1, z_sigma2, z_corr, z_eos]\n",
    "\n",
    "        [o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, o_eos] = get_mixture_coef(output)\n",
    "        \n",
    "        self.pi = o_pi\n",
    "        self.mu1 = o_mu1\n",
    "        self.mu2 = o_mu2\n",
    "        self.sigma1 = o_sigma1\n",
    "        self.sigma2 = o_sigma2\n",
    "        self.corr = o_corr\n",
    "        self.eos = o_eos\n",
    "\n",
    "        lossfunc = get_lossfunc(o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, o_eos, x1_data, x2_data, eos_data)\n",
    "        self.cost = lossfunc / (args.batch_size * args.seq_length)\n",
    "\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), 10.)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    def sample(self, sess, num=1200):\n",
    "        def get_pi_idx(x, pdf):\n",
    "            N = pdf.size\n",
    "            accumulate = 0\n",
    "            for i in range(0, N):\n",
    "                accumulate += pdf[i]\n",
    "                if (accumulate >= x):\n",
    "                    return i\n",
    "            print('error with sampling ensemble')\n",
    "            return -1\n",
    "\n",
    "        def sample_gaussian_2d(mu1, mu2, s1, s2, rho):\n",
    "            mean = [mu1, mu2]\n",
    "            cov = [[s1*s1, rho*s1*s2], [rho*s1*s2, s2*s2]]\n",
    "            x = np.random.multivariate_normal(mean, cov, 1)\n",
    "            return x[0][0], x[0][1]\n",
    "\n",
    "        prev_x = np.zeros((1, 1, 3), dtype=np.float32)\n",
    "        prev_x[0, 0, 2] = 1 # initially, we want to see beginning of new stroke\n",
    "        prev_state = sess.run(self.cell.zero_state(1, tf.float32))\n",
    "        strokes = np.zeros((num, 3), dtype=np.float32)\n",
    "        mixture_params = []\n",
    "\n",
    "        for i in range(num):\n",
    "\n",
    "            feed = {self.input_x: prev_x, self.state_in:prev_state}\n",
    "\n",
    "            [o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, o_eos, next_state] = sess.run([self.pi, self.mu1, self.mu2, self.sigma1, self.sigma2, self.corr, self.eos, self.state_out],feed)\n",
    "\n",
    "            idx = get_pi_idx(random.random(), o_pi[0])\n",
    "\n",
    "            eos = 1 if random.random() < o_eos[0][0] else 0\n",
    "\n",
    "            next_x1, next_x2 = sample_gaussian_2d(o_mu1[0][idx], o_mu2[0][idx], o_sigma1[0][idx], o_sigma2[0][idx], o_corr[0][idx])\n",
    "\n",
    "            strokes[i,:] = [next_x1, next_x2, eos]\n",
    "\n",
    "            params = [o_pi[0], o_mu1[0], o_mu2[0], o_sigma1[0], o_sigma2[0], o_corr[0], o_eos[0]]\n",
    "            mixture_params.append(params)\n",
    "\n",
    "            prev_x = np.zeros((1, 1, 3), dtype=np.float32)\n",
    "            prev_x[0][0] = np.array([next_x1, next_x2, eos], dtype=np.float32)\n",
    "            \n",
    "            prev_state = tuple(\n",
    "                [tf.contrib.rnn.LSTMStateTuple(next_state[idx][0],next_state[idx][1])\n",
    "                for idx in range(args.num_layers)]\n",
    "            )\n",
    "\n",
    "        strokes[:,0:2] *= self.args.data_scale\n",
    "        return strokes, mixture_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1500 (epoch 0), train_loss = 4.877, time/batch = 3.075\n",
      "1/1500 (epoch 0), train_loss = 4.572, time/batch = 2.706\n",
      "2/1500 (epoch 0), train_loss = 4.237, time/batch = 2.520\n",
      "3/1500 (epoch 1), train_loss = 4.311, time/batch = 2.227\n",
      "4/1500 (epoch 1), train_loss = 4.088, time/batch = 2.238\n",
      "5/1500 (epoch 1), train_loss = 3.983, time/batch = 2.233\n",
      "6/1500 (epoch 2), train_loss = 3.988, time/batch = 2.228\n",
      "7/1500 (epoch 2), train_loss = 3.558, time/batch = 2.314\n",
      "8/1500 (epoch 2), train_loss = 3.299, time/batch = 2.321\n",
      "9/1500 (epoch 3), train_loss = 3.278, time/batch = 2.520\n",
      "10/1500 (epoch 3), train_loss = 3.255, time/batch = 2.305\n",
      "11/1500 (epoch 3), train_loss = 3.460, time/batch = 2.409\n",
      "12/1500 (epoch 4), train_loss = 3.308, time/batch = 2.294\n",
      "13/1500 (epoch 4), train_loss = 3.191, time/batch = 2.303\n",
      "14/1500 (epoch 4), train_loss = 2.605, time/batch = 2.280\n",
      "15/1500 (epoch 5), train_loss = 3.265, time/batch = 2.286\n",
      "16/1500 (epoch 5), train_loss = 2.982, time/batch = 2.328\n",
      "17/1500 (epoch 5), train_loss = 2.620, time/batch = 2.294\n",
      "18/1500 (epoch 6), train_loss = 3.406, time/batch = 2.278\n",
      "19/1500 (epoch 6), train_loss = 3.244, time/batch = 2.289\n",
      "20/1500 (epoch 6), train_loss = 2.724, time/batch = 2.328\n",
      "21/1500 (epoch 7), train_loss = 2.976, time/batch = 2.290\n",
      "22/1500 (epoch 7), train_loss = 2.792, time/batch = 2.277\n",
      "23/1500 (epoch 7), train_loss = 2.877, time/batch = 2.290\n",
      "24/1500 (epoch 8), train_loss = 2.813, time/batch = 2.265\n",
      "25/1500 (epoch 8), train_loss = 2.797, time/batch = 2.286\n",
      "26/1500 (epoch 8), train_loss = 2.531, time/batch = 2.283\n",
      "27/1500 (epoch 9), train_loss = 2.817, time/batch = 2.276\n",
      "28/1500 (epoch 9), train_loss = 2.697, time/batch = 2.283\n",
      "29/1500 (epoch 9), train_loss = 2.531, time/batch = 2.292\n",
      "30/1500 (epoch 10), train_loss = 2.734, time/batch = 2.277\n",
      "31/1500 (epoch 10), train_loss = 2.633, time/batch = 2.408\n",
      "32/1500 (epoch 10), train_loss = 2.467, time/batch = 2.300\n",
      "33/1500 (epoch 11), train_loss = 2.622, time/batch = 2.326\n",
      "34/1500 (epoch 11), train_loss = 2.478, time/batch = 2.324\n",
      "35/1500 (epoch 11), train_loss = 2.473, time/batch = 2.294\n",
      "36/1500 (epoch 12), train_loss = 2.733, time/batch = 2.283\n",
      "37/1500 (epoch 12), train_loss = 2.364, time/batch = 2.337\n",
      "38/1500 (epoch 12), train_loss = 2.452, time/batch = 2.322\n",
      "39/1500 (epoch 13), train_loss = 2.685, time/batch = 2.277\n",
      "40/1500 (epoch 13), train_loss = 2.530, time/batch = 2.297\n",
      "41/1500 (epoch 13), train_loss = 2.431, time/batch = 2.301\n",
      "42/1500 (epoch 14), train_loss = 2.690, time/batch = 2.271\n",
      "43/1500 (epoch 14), train_loss = 2.693, time/batch = 2.286\n",
      "44/1500 (epoch 14), train_loss = 2.322, time/batch = 2.296\n",
      "45/1500 (epoch 15), train_loss = 2.680, time/batch = 2.273\n",
      "46/1500 (epoch 15), train_loss = 2.565, time/batch = 2.287\n",
      "47/1500 (epoch 15), train_loss = 2.348, time/batch = 2.337\n",
      "48/1500 (epoch 16), train_loss = 2.621, time/batch = 2.294\n",
      "49/1500 (epoch 16), train_loss = 2.416, time/batch = 2.288\n",
      "50/1500 (epoch 16), train_loss = 2.355, time/batch = 2.309\n",
      "model saved to model\n",
      "51/1500 (epoch 17), train_loss = 2.750, time/batch = 2.316\n",
      "52/1500 (epoch 17), train_loss = 2.712, time/batch = 2.289\n",
      "53/1500 (epoch 17), train_loss = 2.082, time/batch = 2.300\n",
      "54/1500 (epoch 18), train_loss = 2.728, time/batch = 2.324\n",
      "55/1500 (epoch 18), train_loss = 2.650, time/batch = 2.311\n",
      "56/1500 (epoch 18), train_loss = 2.349, time/batch = 2.363\n",
      "57/1500 (epoch 19), train_loss = 2.551, time/batch = 2.351\n",
      "58/1500 (epoch 19), train_loss = 2.693, time/batch = 2.484\n",
      "59/1500 (epoch 19), train_loss = 2.279, time/batch = 2.536\n",
      "60/1500 (epoch 20), train_loss = 2.623, time/batch = 2.367\n",
      "61/1500 (epoch 20), train_loss = 2.530, time/batch = 2.466\n",
      "62/1500 (epoch 20), train_loss = 2.254, time/batch = 2.245\n",
      "63/1500 (epoch 21), train_loss = 2.444, time/batch = 2.473\n",
      "64/1500 (epoch 21), train_loss = 2.743, time/batch = 2.472\n",
      "65/1500 (epoch 21), train_loss = 2.267, time/batch = 2.246\n",
      "66/1500 (epoch 22), train_loss = 2.446, time/batch = 2.236\n",
      "67/1500 (epoch 22), train_loss = 2.369, time/batch = 2.298\n",
      "68/1500 (epoch 22), train_loss = 2.325, time/batch = 2.244\n",
      "69/1500 (epoch 23), train_loss = 2.456, time/batch = 2.225\n",
      "70/1500 (epoch 23), train_loss = 2.705, time/batch = 2.257\n",
      "71/1500 (epoch 23), train_loss = 2.123, time/batch = 2.249\n",
      "72/1500 (epoch 24), train_loss = 2.387, time/batch = 2.236\n",
      "73/1500 (epoch 24), train_loss = 2.406, time/batch = 2.244\n",
      "74/1500 (epoch 24), train_loss = 2.204, time/batch = 2.249\n",
      "75/1500 (epoch 25), train_loss = 2.441, time/batch = 2.228\n",
      "76/1500 (epoch 25), train_loss = 2.347, time/batch = 2.252\n",
      "77/1500 (epoch 25), train_loss = 2.132, time/batch = 2.247\n",
      "78/1500 (epoch 26), train_loss = 2.370, time/batch = 2.242\n",
      "79/1500 (epoch 26), train_loss = 2.383, time/batch = 2.281\n",
      "80/1500 (epoch 26), train_loss = 2.152, time/batch = 2.255\n",
      "81/1500 (epoch 27), train_loss = 2.388, time/batch = 2.233\n",
      "82/1500 (epoch 27), train_loss = 2.520, time/batch = 2.303\n",
      "83/1500 (epoch 27), train_loss = 2.535, time/batch = 2.261\n",
      "84/1500 (epoch 28), train_loss = 2.459, time/batch = 2.235\n",
      "85/1500 (epoch 28), train_loss = 2.359, time/batch = 2.249\n",
      "86/1500 (epoch 28), train_loss = 2.171, time/batch = 2.343\n",
      "87/1500 (epoch 29), train_loss = 2.439, time/batch = 2.870\n",
      "88/1500 (epoch 29), train_loss = 2.279, time/batch = 2.730\n",
      "89/1500 (epoch 29), train_loss = 2.050, time/batch = 2.260\n",
      "90/1500 (epoch 30), train_loss = 2.363, time/batch = 2.391\n",
      "91/1500 (epoch 30), train_loss = 2.169, time/batch = 2.443\n",
      "92/1500 (epoch 30), train_loss = 2.108, time/batch = 2.485\n",
      "93/1500 (epoch 31), train_loss = 2.344, time/batch = 2.715\n",
      "94/1500 (epoch 31), train_loss = 2.111, time/batch = 2.691\n",
      "95/1500 (epoch 31), train_loss = 2.280, time/batch = 2.669\n",
      "96/1500 (epoch 32), train_loss = 2.319, time/batch = 2.549\n",
      "97/1500 (epoch 32), train_loss = 2.290, time/batch = 2.286\n",
      "98/1500 (epoch 32), train_loss = 2.054, time/batch = 2.284\n",
      "99/1500 (epoch 33), train_loss = 2.231, time/batch = 2.370\n",
      "100/1500 (epoch 33), train_loss = 2.158, time/batch = 2.550\n",
      "model saved to model\n",
      "101/1500 (epoch 33), train_loss = 2.081, time/batch = 3.073\n",
      "102/1500 (epoch 34), train_loss = 2.191, time/batch = 2.703\n",
      "103/1500 (epoch 34), train_loss = 2.173, time/batch = 2.555\n",
      "104/1500 (epoch 34), train_loss = 2.173, time/batch = 2.307\n",
      "105/1500 (epoch 35), train_loss = 2.185, time/batch = 2.408\n",
      "106/1500 (epoch 35), train_loss = 2.189, time/batch = 2.385\n",
      "107/1500 (epoch 35), train_loss = 2.059, time/batch = 2.332\n",
      "108/1500 (epoch 36), train_loss = 2.017, time/batch = 2.621\n",
      "109/1500 (epoch 36), train_loss = 2.386, time/batch = 2.734\n",
      "110/1500 (epoch 36), train_loss = 2.083, time/batch = 2.390\n",
      "111/1500 (epoch 37), train_loss = 2.301, time/batch = 2.578\n",
      "112/1500 (epoch 37), train_loss = 2.045, time/batch = 2.372\n",
      "113/1500 (epoch 37), train_loss = 2.135, time/batch = 2.367\n",
      "114/1500 (epoch 38), train_loss = 2.236, time/batch = 2.283\n",
      "115/1500 (epoch 38), train_loss = 2.131, time/batch = 2.302\n",
      "116/1500 (epoch 38), train_loss = 2.091, time/batch = 2.317\n",
      "117/1500 (epoch 39), train_loss = 2.166, time/batch = 2.405\n",
      "118/1500 (epoch 39), train_loss = 2.182, time/batch = 2.237\n",
      "119/1500 (epoch 39), train_loss = 1.781, time/batch = 2.694\n",
      "120/1500 (epoch 40), train_loss = 2.176, time/batch = 2.628\n",
      "121/1500 (epoch 40), train_loss = 2.017, time/batch = 2.232\n",
      "122/1500 (epoch 40), train_loss = 2.019, time/batch = 2.244\n",
      "123/1500 (epoch 41), train_loss = 2.171, time/batch = 2.369\n",
      "124/1500 (epoch 41), train_loss = 1.897, time/batch = 2.316\n",
      "125/1500 (epoch 41), train_loss = 1.864, time/batch = 2.669\n",
      "126/1500 (epoch 42), train_loss = 2.171, time/batch = 2.476\n",
      "127/1500 (epoch 42), train_loss = 2.150, time/batch = 2.333\n",
      "128/1500 (epoch 42), train_loss = 1.673, time/batch = 2.470\n",
      "129/1500 (epoch 43), train_loss = 2.147, time/batch = 3.355\n",
      "130/1500 (epoch 43), train_loss = 1.773, time/batch = 5.034\n",
      "131/1500 (epoch 43), train_loss = 1.987, time/batch = 5.407\n",
      "132/1500 (epoch 44), train_loss = 2.258, time/batch = 4.890\n",
      "133/1500 (epoch 44), train_loss = 1.975, time/batch = 5.310\n",
      "134/1500 (epoch 44), train_loss = 2.021, time/batch = 4.924\n",
      "135/1500 (epoch 45), train_loss = 2.202, time/batch = 4.768\n",
      "136/1500 (epoch 45), train_loss = 1.913, time/batch = 4.647\n",
      "137/1500 (epoch 45), train_loss = 1.858, time/batch = 4.660\n",
      "138/1500 (epoch 46), train_loss = 2.045, time/batch = 4.868\n",
      "139/1500 (epoch 46), train_loss = 2.105, time/batch = 5.322\n",
      "140/1500 (epoch 46), train_loss = 1.992, time/batch = 4.603\n",
      "141/1500 (epoch 47), train_loss = 2.075, time/batch = 4.829\n",
      "142/1500 (epoch 47), train_loss = 1.930, time/batch = 5.210\n",
      "143/1500 (epoch 47), train_loss = 2.111, time/batch = 4.773\n",
      "144/1500 (epoch 48), train_loss = 1.985, time/batch = 2.388\n",
      "145/1500 (epoch 48), train_loss = 2.121, time/batch = 2.718\n",
      "146/1500 (epoch 48), train_loss = 1.936, time/batch = 2.384\n",
      "147/1500 (epoch 49), train_loss = 1.968, time/batch = 2.482\n",
      "148/1500 (epoch 49), train_loss = 1.880, time/batch = 2.596\n",
      "149/1500 (epoch 49), train_loss = 2.003, time/batch = 2.678\n",
      "150/1500 (epoch 50), train_loss = 1.939, time/batch = 3.573\n",
      "model saved to model\n",
      "151/1500 (epoch 50), train_loss = 2.173, time/batch = 5.556\n",
      "152/1500 (epoch 50), train_loss = 1.873, time/batch = 4.818\n",
      "153/1500 (epoch 51), train_loss = 1.974, time/batch = 4.356\n",
      "154/1500 (epoch 51), train_loss = 1.760, time/batch = 4.469\n",
      "155/1500 (epoch 51), train_loss = 1.978, time/batch = 4.562\n",
      "156/1500 (epoch 52), train_loss = 1.975, time/batch = 4.378\n",
      "157/1500 (epoch 52), train_loss = 1.901, time/batch = 4.350\n",
      "158/1500 (epoch 52), train_loss = 1.821, time/batch = 4.415\n",
      "159/1500 (epoch 53), train_loss = 1.987, time/batch = 4.371\n",
      "160/1500 (epoch 53), train_loss = 1.893, time/batch = 4.406\n",
      "161/1500 (epoch 53), train_loss = 1.814, time/batch = 4.475\n",
      "162/1500 (epoch 54), train_loss = 1.950, time/batch = 4.447\n",
      "163/1500 (epoch 54), train_loss = 1.676, time/batch = 4.358\n",
      "164/1500 (epoch 54), train_loss = 1.678, time/batch = 4.397\n",
      "165/1500 (epoch 55), train_loss = 2.034, time/batch = 4.289\n",
      "166/1500 (epoch 55), train_loss = 1.777, time/batch = 4.362\n",
      "167/1500 (epoch 55), train_loss = 1.803, time/batch = 4.193\n",
      "168/1500 (epoch 56), train_loss = 1.914, time/batch = 4.399\n",
      "169/1500 (epoch 56), train_loss = 1.580, time/batch = 4.491\n",
      "170/1500 (epoch 56), train_loss = 1.680, time/batch = 4.405\n",
      "171/1500 (epoch 57), train_loss = 1.809, time/batch = 4.368\n",
      "172/1500 (epoch 57), train_loss = 1.855, time/batch = 4.457\n",
      "173/1500 (epoch 57), train_loss = 1.691, time/batch = 4.303\n",
      "174/1500 (epoch 58), train_loss = 1.944, time/batch = 4.304\n",
      "175/1500 (epoch 58), train_loss = 1.861, time/batch = 4.396\n",
      "176/1500 (epoch 58), train_loss = 1.693, time/batch = 4.393\n",
      "177/1500 (epoch 59), train_loss = 1.774, time/batch = 4.348\n",
      "178/1500 (epoch 59), train_loss = 1.937, time/batch = 4.412\n",
      "179/1500 (epoch 59), train_loss = 1.648, time/batch = 4.357\n",
      "180/1500 (epoch 60), train_loss = 1.700, time/batch = 4.344\n",
      "181/1500 (epoch 60), train_loss = 1.815, time/batch = 4.408\n",
      "182/1500 (epoch 60), train_loss = 1.634, time/batch = 4.394\n",
      "183/1500 (epoch 61), train_loss = 1.537, time/batch = 4.308\n",
      "184/1500 (epoch 61), train_loss = 2.137, time/batch = 4.370\n",
      "185/1500 (epoch 61), train_loss = 1.765, time/batch = 4.401\n",
      "186/1500 (epoch 62), train_loss = 1.741, time/batch = 4.355\n",
      "187/1500 (epoch 62), train_loss = 1.700, time/batch = 4.410\n",
      "188/1500 (epoch 62), train_loss = 1.625, time/batch = 4.422\n",
      "189/1500 (epoch 63), train_loss = 1.764, time/batch = 4.429\n",
      "190/1500 (epoch 63), train_loss = 1.634, time/batch = 4.417\n",
      "191/1500 (epoch 63), train_loss = 1.260, time/batch = 4.450\n",
      "192/1500 (epoch 64), train_loss = 1.868, time/batch = 4.345\n",
      "193/1500 (epoch 64), train_loss = 1.923, time/batch = 4.421\n",
      "194/1500 (epoch 64), train_loss = 1.638, time/batch = 4.427\n",
      "195/1500 (epoch 65), train_loss = 1.877, time/batch = 4.390\n",
      "196/1500 (epoch 65), train_loss = 1.548, time/batch = 4.462\n",
      "197/1500 (epoch 65), train_loss = 1.681, time/batch = 4.449\n",
      "198/1500 (epoch 66), train_loss = 1.705, time/batch = 4.331\n",
      "199/1500 (epoch 66), train_loss = 1.603, time/batch = 4.490\n",
      "200/1500 (epoch 66), train_loss = 1.592, time/batch = 4.396\n",
      "model saved to model\n",
      "201/1500 (epoch 67), train_loss = 1.787, time/batch = 4.311\n",
      "202/1500 (epoch 67), train_loss = 1.662, time/batch = 4.284\n",
      "203/1500 (epoch 67), train_loss = 1.544, time/batch = 4.456\n",
      "204/1500 (epoch 68), train_loss = 1.804, time/batch = 4.395\n",
      "205/1500 (epoch 68), train_loss = 1.530, time/batch = 4.357\n",
      "206/1500 (epoch 68), train_loss = 1.594, time/batch = 4.412\n",
      "207/1500 (epoch 69), train_loss = 1.635, time/batch = 4.446\n",
      "208/1500 (epoch 69), train_loss = 1.644, time/batch = 4.350\n",
      "209/1500 (epoch 69), train_loss = 1.661, time/batch = 4.384\n",
      "210/1500 (epoch 70), train_loss = 1.414, time/batch = 4.330\n",
      "211/1500 (epoch 70), train_loss = 1.609, time/batch = 4.386\n",
      "212/1500 (epoch 70), train_loss = 1.607, time/batch = 4.585\n",
      "213/1500 (epoch 71), train_loss = 1.698, time/batch = 4.408\n",
      "214/1500 (epoch 71), train_loss = 1.273, time/batch = 4.385\n",
      "215/1500 (epoch 71), train_loss = 1.402, time/batch = 4.388\n",
      "216/1500 (epoch 72), train_loss = 1.820, time/batch = 4.398\n",
      "217/1500 (epoch 72), train_loss = 1.638, time/batch = 4.382\n",
      "218/1500 (epoch 72), train_loss = 1.533, time/batch = 4.283\n",
      "219/1500 (epoch 73), train_loss = 1.648, time/batch = 4.403\n",
      "220/1500 (epoch 73), train_loss = 1.449, time/batch = 4.388\n",
      "221/1500 (epoch 73), train_loss = 1.459, time/batch = 4.393\n",
      "222/1500 (epoch 74), train_loss = 1.709, time/batch = 4.459\n",
      "223/1500 (epoch 74), train_loss = 1.700, time/batch = 4.386\n",
      "224/1500 (epoch 74), train_loss = 1.503, time/batch = 4.395\n",
      "225/1500 (epoch 75), train_loss = 1.802, time/batch = 4.422\n",
      "226/1500 (epoch 75), train_loss = 1.413, time/batch = 4.375\n",
      "227/1500 (epoch 75), train_loss = 1.615, time/batch = 4.484\n",
      "228/1500 (epoch 76), train_loss = 1.595, time/batch = 4.331\n",
      "229/1500 (epoch 76), train_loss = 1.537, time/batch = 4.446\n",
      "230/1500 (epoch 76), train_loss = 1.477, time/batch = 4.480\n",
      "231/1500 (epoch 77), train_loss = 1.602, time/batch = 4.427\n",
      "232/1500 (epoch 77), train_loss = 1.398, time/batch = 4.450\n",
      "233/1500 (epoch 77), train_loss = 1.626, time/batch = 4.417\n",
      "234/1500 (epoch 78), train_loss = 1.620, time/batch = 4.377\n",
      "235/1500 (epoch 78), train_loss = 1.544, time/batch = 4.335\n",
      "236/1500 (epoch 78), train_loss = 1.428, time/batch = 4.387\n",
      "237/1500 (epoch 79), train_loss = 1.415, time/batch = 4.452\n",
      "238/1500 (epoch 79), train_loss = 1.495, time/batch = 4.461\n",
      "239/1500 (epoch 79), train_loss = 0.996, time/batch = 4.446\n",
      "240/1500 (epoch 80), train_loss = 1.664, time/batch = 4.439\n",
      "241/1500 (epoch 80), train_loss = 1.512, time/batch = 4.391\n",
      "242/1500 (epoch 80), train_loss = 1.738, time/batch = 4.421\n",
      "243/1500 (epoch 81), train_loss = 1.723, time/batch = 4.460\n",
      "244/1500 (epoch 81), train_loss = 1.163, time/batch = 4.415\n",
      "245/1500 (epoch 81), train_loss = 1.435, time/batch = 4.345\n",
      "246/1500 (epoch 82), train_loss = 1.321, time/batch = 4.460\n",
      "247/1500 (epoch 82), train_loss = 1.458, time/batch = 4.450\n",
      "248/1500 (epoch 82), train_loss = 1.384, time/batch = 4.370\n",
      "249/1500 (epoch 83), train_loss = 1.501, time/batch = 4.470\n",
      "250/1500 (epoch 83), train_loss = 1.187, time/batch = 4.410\n",
      "model saved to model\n",
      "251/1500 (epoch 83), train_loss = 1.141, time/batch = 4.539\n",
      "252/1500 (epoch 84), train_loss = 1.627, time/batch = 4.359\n",
      "253/1500 (epoch 84), train_loss = 1.413, time/batch = 4.352\n",
      "254/1500 (epoch 84), train_loss = 0.993, time/batch = 4.367\n",
      "255/1500 (epoch 85), train_loss = 1.520, time/batch = 4.384\n",
      "256/1500 (epoch 85), train_loss = 1.311, time/batch = 4.334\n",
      "257/1500 (epoch 85), train_loss = 1.291, time/batch = 4.570\n",
      "258/1500 (epoch 86), train_loss = 1.582, time/batch = 4.544\n",
      "259/1500 (epoch 86), train_loss = 1.333, time/batch = 4.444\n",
      "260/1500 (epoch 86), train_loss = 1.280, time/batch = 4.473\n",
      "261/1500 (epoch 87), train_loss = 1.428, time/batch = 4.474\n",
      "262/1500 (epoch 87), train_loss = 1.459, time/batch = 4.399\n",
      "263/1500 (epoch 87), train_loss = 1.294, time/batch = 4.373\n",
      "264/1500 (epoch 88), train_loss = 1.525, time/batch = 4.409\n",
      "265/1500 (epoch 88), train_loss = 1.181, time/batch = 4.308\n",
      "266/1500 (epoch 88), train_loss = 1.299, time/batch = 4.546\n",
      "267/1500 (epoch 89), train_loss = 1.498, time/batch = 4.380\n",
      "268/1500 (epoch 89), train_loss = 1.454, time/batch = 4.309\n",
      "269/1500 (epoch 89), train_loss = 1.262, time/batch = 4.424\n",
      "270/1500 (epoch 90), train_loss = 1.500, time/batch = 4.419\n",
      "271/1500 (epoch 90), train_loss = 1.457, time/batch = 4.354\n",
      "272/1500 (epoch 90), train_loss = 0.816, time/batch = 4.480\n",
      "273/1500 (epoch 91), train_loss = 1.447, time/batch = 4.344\n",
      "274/1500 (epoch 91), train_loss = 1.261, time/batch = 4.400\n",
      "275/1500 (epoch 91), train_loss = 1.291, time/batch = 4.341\n",
      "276/1500 (epoch 92), train_loss = 1.527, time/batch = 4.400\n",
      "277/1500 (epoch 92), train_loss = 1.326, time/batch = 4.370\n",
      "278/1500 (epoch 92), train_loss = 1.195, time/batch = 4.404\n",
      "279/1500 (epoch 93), train_loss = 1.434, time/batch = 4.742\n",
      "280/1500 (epoch 93), train_loss = 1.226, time/batch = 4.349\n",
      "281/1500 (epoch 93), train_loss = 1.289, time/batch = 4.422\n",
      "282/1500 (epoch 94), train_loss = 1.337, time/batch = 4.309\n",
      "283/1500 (epoch 94), train_loss = 1.269, time/batch = 4.245\n",
      "284/1500 (epoch 94), train_loss = 1.105, time/batch = 4.450\n",
      "285/1500 (epoch 95), train_loss = 1.509, time/batch = 4.391\n",
      "286/1500 (epoch 95), train_loss = 1.316, time/batch = 4.389\n",
      "287/1500 (epoch 95), train_loss = 1.278, time/batch = 4.376\n",
      "288/1500 (epoch 96), train_loss = 1.515, time/batch = 4.373\n",
      "289/1500 (epoch 96), train_loss = 1.327, time/batch = 4.257\n",
      "290/1500 (epoch 96), train_loss = 1.197, time/batch = 4.430\n",
      "291/1500 (epoch 97), train_loss = 1.330, time/batch = 4.323\n",
      "292/1500 (epoch 97), train_loss = 1.296, time/batch = 4.359\n",
      "293/1500 (epoch 97), train_loss = 1.146, time/batch = 4.423\n",
      "294/1500 (epoch 98), train_loss = 1.447, time/batch = 4.472\n",
      "295/1500 (epoch 98), train_loss = 1.207, time/batch = 4.314\n",
      "296/1500 (epoch 98), train_loss = 1.096, time/batch = 4.372\n",
      "297/1500 (epoch 99), train_loss = 1.287, time/batch = 4.390\n",
      "298/1500 (epoch 99), train_loss = 1.198, time/batch = 4.460\n",
      "299/1500 (epoch 99), train_loss = 1.170, time/batch = 4.464\n",
      "300/1500 (epoch 100), train_loss = 1.289, time/batch = 4.411\n",
      "model saved to model\n",
      "301/1500 (epoch 100), train_loss = 0.916, time/batch = 4.501\n",
      "302/1500 (epoch 100), train_loss = 1.204, time/batch = 4.353\n",
      "303/1500 (epoch 101), train_loss = 1.610, time/batch = 4.481\n",
      "304/1500 (epoch 101), train_loss = 1.064, time/batch = 4.418\n",
      "305/1500 (epoch 101), train_loss = 1.146, time/batch = 4.378\n",
      "306/1500 (epoch 102), train_loss = 1.319, time/batch = 4.431\n",
      "307/1500 (epoch 102), train_loss = 1.199, time/batch = 4.413\n",
      "308/1500 (epoch 102), train_loss = 0.786, time/batch = 4.459\n",
      "309/1500 (epoch 103), train_loss = 1.332, time/batch = 4.447\n",
      "310/1500 (epoch 103), train_loss = 1.132, time/batch = 4.483\n",
      "311/1500 (epoch 103), train_loss = 1.017, time/batch = 4.429\n",
      "312/1500 (epoch 104), train_loss = 1.298, time/batch = 4.438\n",
      "313/1500 (epoch 104), train_loss = 1.414, time/batch = 4.475\n",
      "314/1500 (epoch 104), train_loss = 1.004, time/batch = 4.430\n",
      "315/1500 (epoch 105), train_loss = 1.361, time/batch = 4.474\n",
      "316/1500 (epoch 105), train_loss = 1.088, time/batch = 4.514\n",
      "317/1500 (epoch 105), train_loss = 1.369, time/batch = 4.346\n",
      "318/1500 (epoch 106), train_loss = 1.258, time/batch = 4.524\n",
      "319/1500 (epoch 106), train_loss = 1.418, time/batch = 4.470\n",
      "320/1500 (epoch 106), train_loss = 1.040, time/batch = 4.416\n",
      "321/1500 (epoch 107), train_loss = 1.315, time/batch = 4.520\n",
      "322/1500 (epoch 107), train_loss = 1.334, time/batch = 4.404\n",
      "323/1500 (epoch 107), train_loss = 1.005, time/batch = 4.480\n",
      "324/1500 (epoch 108), train_loss = 1.351, time/batch = 4.364\n",
      "325/1500 (epoch 108), train_loss = 1.624, time/batch = 4.479\n",
      "326/1500 (epoch 108), train_loss = 1.084, time/batch = 4.391\n",
      "327/1500 (epoch 109), train_loss = 1.300, time/batch = 4.429\n",
      "328/1500 (epoch 109), train_loss = 1.281, time/batch = 4.482\n",
      "329/1500 (epoch 109), train_loss = 1.114, time/batch = 4.316\n",
      "330/1500 (epoch 110), train_loss = 1.339, time/batch = 4.385\n",
      "331/1500 (epoch 110), train_loss = 1.308, time/batch = 4.425\n",
      "332/1500 (epoch 110), train_loss = 1.242, time/batch = 4.481\n",
      "333/1500 (epoch 111), train_loss = 1.163, time/batch = 4.460\n",
      "334/1500 (epoch 111), train_loss = 0.859, time/batch = 4.552\n",
      "335/1500 (epoch 111), train_loss = 1.228, time/batch = 4.342\n",
      "336/1500 (epoch 112), train_loss = 1.201, time/batch = 4.480\n",
      "337/1500 (epoch 112), train_loss = 0.886, time/batch = 4.540\n",
      "338/1500 (epoch 112), train_loss = 1.016, time/batch = 4.436\n",
      "339/1500 (epoch 113), train_loss = 1.318, time/batch = 4.323\n",
      "340/1500 (epoch 113), train_loss = 1.127, time/batch = 4.430\n",
      "341/1500 (epoch 113), train_loss = 0.873, time/batch = 4.389\n",
      "342/1500 (epoch 114), train_loss = 1.296, time/batch = 4.303\n",
      "343/1500 (epoch 114), train_loss = 1.214, time/batch = 4.448\n",
      "344/1500 (epoch 114), train_loss = 1.102, time/batch = 4.387\n",
      "345/1500 (epoch 115), train_loss = 1.085, time/batch = 4.493\n",
      "346/1500 (epoch 115), train_loss = 1.226, time/batch = 4.333\n",
      "347/1500 (epoch 115), train_loss = 1.170, time/batch = 4.511\n",
      "348/1500 (epoch 116), train_loss = 1.184, time/batch = 4.316\n",
      "349/1500 (epoch 116), train_loss = 1.005, time/batch = 4.478\n",
      "350/1500 (epoch 116), train_loss = 0.940, time/batch = 4.288\n",
      "model saved to model\n",
      "351/1500 (epoch 117), train_loss = 1.312, time/batch = 4.481\n",
      "352/1500 (epoch 117), train_loss = 1.028, time/batch = 4.505\n",
      "353/1500 (epoch 117), train_loss = 0.982, time/batch = 4.315\n",
      "354/1500 (epoch 118), train_loss = 1.235, time/batch = 4.378\n",
      "355/1500 (epoch 118), train_loss = 1.091, time/batch = 4.517\n",
      "356/1500 (epoch 118), train_loss = 0.972, time/batch = 4.244\n",
      "357/1500 (epoch 119), train_loss = 1.092, time/batch = 4.361\n",
      "358/1500 (epoch 119), train_loss = 0.890, time/batch = 4.529\n",
      "359/1500 (epoch 119), train_loss = 1.176, time/batch = 4.340\n",
      "360/1500 (epoch 120), train_loss = 1.334, time/batch = 4.441\n",
      "361/1500 (epoch 120), train_loss = 0.888, time/batch = 4.488\n",
      "362/1500 (epoch 120), train_loss = 1.253, time/batch = 4.289\n",
      "363/1500 (epoch 121), train_loss = 1.224, time/batch = 4.494\n",
      "364/1500 (epoch 121), train_loss = 0.935, time/batch = 4.376\n",
      "365/1500 (epoch 121), train_loss = 1.068, time/batch = 4.304\n",
      "366/1500 (epoch 122), train_loss = 1.197, time/batch = 4.414\n",
      "367/1500 (epoch 122), train_loss = 1.087, time/batch = 4.385\n",
      "368/1500 (epoch 122), train_loss = 0.909, time/batch = 4.332\n",
      "369/1500 (epoch 123), train_loss = 1.238, time/batch = 4.295\n",
      "370/1500 (epoch 123), train_loss = 1.150, time/batch = 4.319\n",
      "371/1500 (epoch 123), train_loss = 0.908, time/batch = 4.287\n",
      "372/1500 (epoch 124), train_loss = 1.086, time/batch = 4.368\n",
      "373/1500 (epoch 124), train_loss = 1.083, time/batch = 4.365\n",
      "374/1500 (epoch 124), train_loss = 0.927, time/batch = 4.382\n",
      "375/1500 (epoch 125), train_loss = 1.100, time/batch = 4.451\n",
      "376/1500 (epoch 125), train_loss = 1.028, time/batch = 4.434\n",
      "377/1500 (epoch 125), train_loss = 0.946, time/batch = 4.401\n",
      "378/1500 (epoch 126), train_loss = 1.148, time/batch = 4.440\n",
      "379/1500 (epoch 126), train_loss = 0.842, time/batch = 4.412\n",
      "380/1500 (epoch 126), train_loss = 0.898, time/batch = 4.403\n",
      "381/1500 (epoch 127), train_loss = 1.131, time/batch = 4.655\n",
      "382/1500 (epoch 127), train_loss = 0.973, time/batch = 4.448\n",
      "383/1500 (epoch 127), train_loss = 0.872, time/batch = 4.476\n",
      "384/1500 (epoch 128), train_loss = 1.231, time/batch = 4.387\n",
      "385/1500 (epoch 128), train_loss = 1.036, time/batch = 4.403\n",
      "386/1500 (epoch 128), train_loss = 0.915, time/batch = 4.341\n",
      "387/1500 (epoch 129), train_loss = 1.174, time/batch = 4.474\n",
      "388/1500 (epoch 129), train_loss = 0.796, time/batch = 4.453\n",
      "389/1500 (epoch 129), train_loss = 0.864, time/batch = 4.425\n",
      "390/1500 (epoch 130), train_loss = 1.104, time/batch = 4.533\n",
      "391/1500 (epoch 130), train_loss = 0.839, time/batch = 4.496\n",
      "392/1500 (epoch 130), train_loss = 0.891, time/batch = 4.422\n",
      "393/1500 (epoch 131), train_loss = 1.148, time/batch = 4.339\n",
      "394/1500 (epoch 131), train_loss = 0.823, time/batch = 4.350\n",
      "395/1500 (epoch 131), train_loss = 1.023, time/batch = 4.464\n",
      "396/1500 (epoch 132), train_loss = 1.371, time/batch = 4.469\n",
      "397/1500 (epoch 132), train_loss = 0.964, time/batch = 4.458\n",
      "398/1500 (epoch 132), train_loss = 1.020, time/batch = 4.465\n",
      "399/1500 (epoch 133), train_loss = 0.883, time/batch = 4.507\n",
      "400/1500 (epoch 133), train_loss = 1.071, time/batch = 4.414\n",
      "model saved to model\n",
      "401/1500 (epoch 133), train_loss = 0.473, time/batch = 4.516\n",
      "402/1500 (epoch 134), train_loss = 1.100, time/batch = 4.507\n",
      "403/1500 (epoch 134), train_loss = 1.037, time/batch = 4.332\n",
      "404/1500 (epoch 134), train_loss = 0.917, time/batch = 4.473\n",
      "405/1500 (epoch 135), train_loss = 1.153, time/batch = 4.425\n",
      "406/1500 (epoch 135), train_loss = 0.952, time/batch = 4.581\n",
      "407/1500 (epoch 135), train_loss = 0.848, time/batch = 4.519\n",
      "408/1500 (epoch 136), train_loss = 1.174, time/batch = 4.543\n",
      "409/1500 (epoch 136), train_loss = 0.826, time/batch = 4.573\n",
      "410/1500 (epoch 136), train_loss = 0.996, time/batch = 4.499\n",
      "411/1500 (epoch 137), train_loss = 1.145, time/batch = 4.482\n",
      "412/1500 (epoch 137), train_loss = 0.762, time/batch = 4.456\n",
      "413/1500 (epoch 137), train_loss = 0.988, time/batch = 4.406\n",
      "414/1500 (epoch 138), train_loss = 1.083, time/batch = 4.650\n",
      "415/1500 (epoch 138), train_loss = 0.824, time/batch = 4.440\n",
      "416/1500 (epoch 138), train_loss = 1.097, time/batch = 4.412\n",
      "417/1500 (epoch 139), train_loss = 1.066, time/batch = 4.423\n",
      "418/1500 (epoch 139), train_loss = 0.754, time/batch = 4.422\n",
      "419/1500 (epoch 139), train_loss = 0.876, time/batch = 4.370\n",
      "420/1500 (epoch 140), train_loss = 1.120, time/batch = 4.359\n",
      "421/1500 (epoch 140), train_loss = 0.871, time/batch = 4.485\n",
      "422/1500 (epoch 140), train_loss = 0.897, time/batch = 4.527\n",
      "423/1500 (epoch 141), train_loss = 1.142, time/batch = 4.438\n",
      "424/1500 (epoch 141), train_loss = 1.004, time/batch = 4.532\n",
      "425/1500 (epoch 141), train_loss = 0.816, time/batch = 4.505\n",
      "426/1500 (epoch 142), train_loss = 1.064, time/batch = 4.353\n",
      "427/1500 (epoch 142), train_loss = 0.983, time/batch = 4.451\n",
      "428/1500 (epoch 142), train_loss = 1.118, time/batch = 4.480\n",
      "429/1500 (epoch 143), train_loss = 1.134, time/batch = 4.356\n",
      "430/1500 (epoch 143), train_loss = 0.850, time/batch = 4.434\n",
      "431/1500 (epoch 143), train_loss = 0.913, time/batch = 4.470\n",
      "432/1500 (epoch 144), train_loss = 1.023, time/batch = 4.305\n",
      "433/1500 (epoch 144), train_loss = 0.851, time/batch = 4.484\n",
      "434/1500 (epoch 144), train_loss = 0.816, time/batch = 4.440\n",
      "435/1500 (epoch 145), train_loss = 1.110, time/batch = 4.371\n",
      "436/1500 (epoch 145), train_loss = 0.816, time/batch = 4.432\n",
      "437/1500 (epoch 145), train_loss = 0.973, time/batch = 4.428\n",
      "438/1500 (epoch 146), train_loss = 1.211, time/batch = 4.363\n",
      "439/1500 (epoch 146), train_loss = 0.799, time/batch = 4.513\n",
      "440/1500 (epoch 146), train_loss = 1.163, time/batch = 4.253\n",
      "441/1500 (epoch 147), train_loss = 1.184, time/batch = 4.383\n",
      "442/1500 (epoch 147), train_loss = 0.995, time/batch = 4.539\n",
      "443/1500 (epoch 147), train_loss = 0.950, time/batch = 4.218\n",
      "444/1500 (epoch 148), train_loss = 1.120, time/batch = 4.386\n",
      "445/1500 (epoch 148), train_loss = 0.728, time/batch = 4.470\n",
      "446/1500 (epoch 148), train_loss = 0.757, time/batch = 4.493\n",
      "447/1500 (epoch 149), train_loss = 1.175, time/batch = 4.324\n",
      "448/1500 (epoch 149), train_loss = 0.991, time/batch = 4.473\n",
      "449/1500 (epoch 149), train_loss = 0.885, time/batch = 4.368\n",
      "450/1500 (epoch 150), train_loss = 1.040, time/batch = 4.242\n",
      "model saved to model\n",
      "451/1500 (epoch 150), train_loss = 0.928, time/batch = 4.421\n",
      "452/1500 (epoch 150), train_loss = 0.772, time/batch = 4.319\n",
      "453/1500 (epoch 151), train_loss = 1.101, time/batch = 4.390\n",
      "454/1500 (epoch 151), train_loss = 0.826, time/batch = 4.082\n",
      "455/1500 (epoch 151), train_loss = 0.836, time/batch = 4.338\n",
      "456/1500 (epoch 152), train_loss = 1.161, time/batch = 4.727\n",
      "457/1500 (epoch 152), train_loss = 1.104, time/batch = 4.482\n",
      "458/1500 (epoch 152), train_loss = 0.890, time/batch = 4.732\n",
      "459/1500 (epoch 153), train_loss = 1.179, time/batch = 4.615\n",
      "460/1500 (epoch 153), train_loss = 0.739, time/batch = 4.447\n",
      "461/1500 (epoch 153), train_loss = 0.889, time/batch = 4.455\n",
      "462/1500 (epoch 154), train_loss = 1.074, time/batch = 4.406\n",
      "463/1500 (epoch 154), train_loss = 0.716, time/batch = 4.480\n",
      "464/1500 (epoch 154), train_loss = 0.792, time/batch = 4.519\n",
      "465/1500 (epoch 155), train_loss = 1.034, time/batch = 4.319\n",
      "466/1500 (epoch 155), train_loss = 0.834, time/batch = 4.570\n",
      "467/1500 (epoch 155), train_loss = 1.052, time/batch = 4.403\n",
      "468/1500 (epoch 156), train_loss = 1.135, time/batch = 4.441\n",
      "469/1500 (epoch 156), train_loss = 0.703, time/batch = 4.444\n",
      "470/1500 (epoch 156), train_loss = 0.838, time/batch = 4.453\n",
      "471/1500 (epoch 157), train_loss = 1.034, time/batch = 4.570\n",
      "472/1500 (epoch 157), train_loss = 1.137, time/batch = 4.496\n",
      "473/1500 (epoch 157), train_loss = 0.760, time/batch = 4.533\n",
      "474/1500 (epoch 158), train_loss = 1.057, time/batch = 4.400\n",
      "475/1500 (epoch 158), train_loss = 0.684, time/batch = 4.482\n",
      "476/1500 (epoch 158), train_loss = 1.033, time/batch = 4.546\n",
      "477/1500 (epoch 159), train_loss = 1.072, time/batch = 4.456\n",
      "478/1500 (epoch 159), train_loss = 0.907, time/batch = 4.332\n",
      "479/1500 (epoch 159), train_loss = 0.865, time/batch = 4.362\n",
      "480/1500 (epoch 160), train_loss = 1.051, time/batch = 4.489\n",
      "481/1500 (epoch 160), train_loss = 0.727, time/batch = 4.444\n",
      "482/1500 (epoch 160), train_loss = 0.875, time/batch = 4.363\n",
      "483/1500 (epoch 161), train_loss = 0.954, time/batch = 4.488\n",
      "484/1500 (epoch 161), train_loss = 0.847, time/batch = 4.435\n",
      "485/1500 (epoch 161), train_loss = 0.798, time/batch = 4.408\n",
      "486/1500 (epoch 162), train_loss = 1.008, time/batch = 4.394\n",
      "487/1500 (epoch 162), train_loss = 0.712, time/batch = 4.294\n",
      "488/1500 (epoch 162), train_loss = 0.749, time/batch = 4.455\n",
      "489/1500 (epoch 163), train_loss = 1.058, time/batch = 4.415\n",
      "490/1500 (epoch 163), train_loss = 0.707, time/batch = 4.474\n",
      "491/1500 (epoch 163), train_loss = 0.838, time/batch = 4.427\n",
      "492/1500 (epoch 164), train_loss = 0.904, time/batch = 4.297\n",
      "493/1500 (epoch 164), train_loss = 0.996, time/batch = 4.472\n",
      "494/1500 (epoch 164), train_loss = 0.734, time/batch = 4.688\n",
      "495/1500 (epoch 165), train_loss = 1.203, time/batch = 4.537\n",
      "496/1500 (epoch 165), train_loss = 0.785, time/batch = 4.483\n",
      "497/1500 (epoch 165), train_loss = 0.763, time/batch = 4.676\n",
      "498/1500 (epoch 166), train_loss = 1.132, time/batch = 4.597\n",
      "499/1500 (epoch 166), train_loss = 0.988, time/batch = 4.398\n",
      "500/1500 (epoch 166), train_loss = 0.752, time/batch = 4.691\n",
      "model saved to model\n",
      "501/1500 (epoch 167), train_loss = 1.135, time/batch = 4.703\n",
      "502/1500 (epoch 167), train_loss = 1.048, time/batch = 4.334\n",
      "503/1500 (epoch 167), train_loss = 0.812, time/batch = 4.400\n",
      "504/1500 (epoch 168), train_loss = 1.059, time/batch = 4.471\n",
      "505/1500 (epoch 168), train_loss = 0.966, time/batch = 3.670\n",
      "506/1500 (epoch 168), train_loss = 0.737, time/batch = 4.391\n",
      "507/1500 (epoch 169), train_loss = 0.970, time/batch = 4.530\n",
      "508/1500 (epoch 169), train_loss = 1.103, time/batch = 4.595\n",
      "509/1500 (epoch 169), train_loss = 0.854, time/batch = 4.605\n",
      "510/1500 (epoch 170), train_loss = 1.213, time/batch = 4.400\n",
      "511/1500 (epoch 170), train_loss = 0.864, time/batch = 4.593\n",
      "512/1500 (epoch 170), train_loss = 0.837, time/batch = 4.578\n",
      "513/1500 (epoch 171), train_loss = 1.073, time/batch = 4.707\n",
      "514/1500 (epoch 171), train_loss = 0.709, time/batch = 4.493\n",
      "515/1500 (epoch 171), train_loss = 0.726, time/batch = 4.624\n",
      "516/1500 (epoch 172), train_loss = 1.221, time/batch = 4.639\n",
      "517/1500 (epoch 172), train_loss = 0.937, time/batch = 4.472\n",
      "518/1500 (epoch 172), train_loss = 0.669, time/batch = 4.903\n",
      "519/1500 (epoch 173), train_loss = 0.893, time/batch = 4.427\n",
      "520/1500 (epoch 173), train_loss = 0.898, time/batch = 4.379\n",
      "521/1500 (epoch 173), train_loss = 0.754, time/batch = 4.505\n",
      "522/1500 (epoch 174), train_loss = 0.845, time/batch = 4.435\n",
      "523/1500 (epoch 174), train_loss = 0.904, time/batch = 4.444\n",
      "524/1500 (epoch 174), train_loss = 0.638, time/batch = 4.562\n",
      "525/1500 (epoch 175), train_loss = 0.936, time/batch = 4.471\n",
      "526/1500 (epoch 175), train_loss = 0.874, time/batch = 4.727\n",
      "527/1500 (epoch 175), train_loss = 0.930, time/batch = 4.405\n",
      "528/1500 (epoch 176), train_loss = 0.784, time/batch = 4.479\n",
      "529/1500 (epoch 176), train_loss = 0.850, time/batch = 4.642\n",
      "530/1500 (epoch 176), train_loss = 0.838, time/batch = 4.362\n",
      "531/1500 (epoch 177), train_loss = 1.020, time/batch = 4.468\n",
      "532/1500 (epoch 177), train_loss = 0.671, time/batch = 4.480\n",
      "533/1500 (epoch 177), train_loss = 0.729, time/batch = 4.553\n",
      "534/1500 (epoch 178), train_loss = 1.222, time/batch = 4.732\n",
      "535/1500 (epoch 178), train_loss = 0.753, time/batch = 4.540\n",
      "536/1500 (epoch 178), train_loss = 0.620, time/batch = 4.623\n",
      "537/1500 (epoch 179), train_loss = 1.178, time/batch = 4.628\n",
      "538/1500 (epoch 179), train_loss = 0.956, time/batch = 4.460\n",
      "539/1500 (epoch 179), train_loss = 0.845, time/batch = 4.630\n",
      "540/1500 (epoch 180), train_loss = 0.842, time/batch = 4.441\n",
      "541/1500 (epoch 180), train_loss = 1.001, time/batch = 4.699\n",
      "542/1500 (epoch 180), train_loss = 0.677, time/batch = 4.526\n",
      "543/1500 (epoch 181), train_loss = 0.865, time/batch = 4.518\n",
      "544/1500 (epoch 181), train_loss = 0.812, time/batch = 4.447\n",
      "545/1500 (epoch 181), train_loss = 0.882, time/batch = 4.584\n",
      "546/1500 (epoch 182), train_loss = 0.975, time/batch = 4.479\n",
      "547/1500 (epoch 182), train_loss = 0.940, time/batch = 5.027\n",
      "548/1500 (epoch 182), train_loss = 0.876, time/batch = 4.312\n",
      "549/1500 (epoch 183), train_loss = 1.013, time/batch = 4.372\n",
      "550/1500 (epoch 183), train_loss = 0.787, time/batch = 4.423\n",
      "model saved to model\n",
      "551/1500 (epoch 183), train_loss = 0.984, time/batch = 4.433\n",
      "552/1500 (epoch 184), train_loss = 0.996, time/batch = 4.561\n",
      "553/1500 (epoch 184), train_loss = 0.713, time/batch = 4.729\n",
      "554/1500 (epoch 184), train_loss = 0.674, time/batch = 4.426\n",
      "555/1500 (epoch 185), train_loss = 0.759, time/batch = 4.339\n",
      "556/1500 (epoch 185), train_loss = 1.091, time/batch = 4.307\n",
      "557/1500 (epoch 185), train_loss = 0.976, time/batch = 4.435\n",
      "558/1500 (epoch 186), train_loss = 0.851, time/batch = 4.512\n",
      "559/1500 (epoch 186), train_loss = 0.689, time/batch = 4.625\n",
      "560/1500 (epoch 186), train_loss = 0.689, time/batch = 4.515\n",
      "561/1500 (epoch 187), train_loss = 0.992, time/batch = 4.789\n",
      "562/1500 (epoch 187), train_loss = 0.937, time/batch = 4.521\n",
      "563/1500 (epoch 187), train_loss = 0.755, time/batch = 4.635\n",
      "564/1500 (epoch 188), train_loss = 1.031, time/batch = 4.527\n",
      "565/1500 (epoch 188), train_loss = 0.748, time/batch = 4.610\n",
      "566/1500 (epoch 188), train_loss = 0.736, time/batch = 4.556\n",
      "567/1500 (epoch 189), train_loss = 1.073, time/batch = 4.457\n",
      "568/1500 (epoch 189), train_loss = 0.811, time/batch = 4.376\n",
      "569/1500 (epoch 189), train_loss = 0.881, time/batch = 4.304\n",
      "570/1500 (epoch 190), train_loss = 1.106, time/batch = 4.427\n",
      "571/1500 (epoch 190), train_loss = 0.696, time/batch = 4.592\n",
      "572/1500 (epoch 190), train_loss = 0.771, time/batch = 4.465\n",
      "573/1500 (epoch 191), train_loss = 0.980, time/batch = 4.499\n",
      "574/1500 (epoch 191), train_loss = 1.024, time/batch = 4.533\n",
      "575/1500 (epoch 191), train_loss = 0.441, time/batch = 4.198\n",
      "576/1500 (epoch 192), train_loss = 0.930, time/batch = 4.559\n",
      "577/1500 (epoch 192), train_loss = 0.702, time/batch = 4.484\n",
      "578/1500 (epoch 192), train_loss = 0.918, time/batch = 4.373\n",
      "579/1500 (epoch 193), train_loss = 1.021, time/batch = 4.478\n",
      "580/1500 (epoch 193), train_loss = 0.662, time/batch = 4.361\n",
      "581/1500 (epoch 193), train_loss = 0.818, time/batch = 4.328\n",
      "582/1500 (epoch 194), train_loss = 1.120, time/batch = 4.415\n",
      "583/1500 (epoch 194), train_loss = 0.546, time/batch = 4.410\n",
      "584/1500 (epoch 194), train_loss = 0.824, time/batch = 4.506\n",
      "585/1500 (epoch 195), train_loss = 1.046, time/batch = 4.393\n",
      "586/1500 (epoch 195), train_loss = 0.630, time/batch = 4.381\n",
      "587/1500 (epoch 195), train_loss = 0.826, time/batch = 4.403\n",
      "588/1500 (epoch 196), train_loss = 1.092, time/batch = 4.413\n",
      "589/1500 (epoch 196), train_loss = 0.966, time/batch = 4.429\n",
      "590/1500 (epoch 196), train_loss = 0.673, time/batch = 4.446\n",
      "591/1500 (epoch 197), train_loss = 0.885, time/batch = 4.435\n",
      "592/1500 (epoch 197), train_loss = 0.586, time/batch = 4.512\n",
      "593/1500 (epoch 197), train_loss = 0.661, time/batch = 4.397\n",
      "594/1500 (epoch 198), train_loss = 1.024, time/batch = 4.453\n",
      "595/1500 (epoch 198), train_loss = 0.737, time/batch = 4.483\n",
      "596/1500 (epoch 198), train_loss = 0.652, time/batch = 4.389\n",
      "597/1500 (epoch 199), train_loss = 1.014, time/batch = 4.355\n",
      "598/1500 (epoch 199), train_loss = 0.767, time/batch = 4.348\n",
      "599/1500 (epoch 199), train_loss = 0.778, time/batch = 4.477\n",
      "600/1500 (epoch 200), train_loss = 0.939, time/batch = 4.398\n",
      "model saved to model\n",
      "601/1500 (epoch 200), train_loss = 0.703, time/batch = 4.459\n",
      "602/1500 (epoch 200), train_loss = 0.657, time/batch = 4.392\n",
      "603/1500 (epoch 201), train_loss = 0.984, time/batch = 4.297\n",
      "604/1500 (epoch 201), train_loss = 0.737, time/batch = 4.445\n",
      "605/1500 (epoch 201), train_loss = 0.742, time/batch = 4.309\n",
      "606/1500 (epoch 202), train_loss = 1.123, time/batch = 4.412\n",
      "607/1500 (epoch 202), train_loss = 0.831, time/batch = 4.320\n",
      "608/1500 (epoch 202), train_loss = 0.846, time/batch = 4.474\n",
      "609/1500 (epoch 203), train_loss = 1.002, time/batch = 4.385\n",
      "610/1500 (epoch 203), train_loss = 0.692, time/batch = 4.397\n",
      "611/1500 (epoch 203), train_loss = 0.646, time/batch = 4.459\n",
      "612/1500 (epoch 204), train_loss = 1.089, time/batch = 4.314\n",
      "613/1500 (epoch 204), train_loss = 0.907, time/batch = 4.571\n",
      "614/1500 (epoch 204), train_loss = 0.670, time/batch = 4.616\n",
      "615/1500 (epoch 205), train_loss = 0.916, time/batch = 4.433\n",
      "616/1500 (epoch 205), train_loss = 0.671, time/batch = 4.530\n",
      "617/1500 (epoch 205), train_loss = 0.651, time/batch = 4.486\n",
      "618/1500 (epoch 206), train_loss = 0.989, time/batch = 4.240\n",
      "619/1500 (epoch 206), train_loss = 0.642, time/batch = 4.466\n",
      "620/1500 (epoch 206), train_loss = 0.695, time/batch = 4.499\n",
      "621/1500 (epoch 207), train_loss = 1.001, time/batch = 4.333\n",
      "622/1500 (epoch 207), train_loss = 0.684, time/batch = 4.446\n",
      "623/1500 (epoch 207), train_loss = 0.726, time/batch = 4.402\n",
      "624/1500 (epoch 208), train_loss = 0.912, time/batch = 4.269\n",
      "625/1500 (epoch 208), train_loss = 0.888, time/batch = 4.417\n",
      "626/1500 (epoch 208), train_loss = 0.661, time/batch = 4.470\n",
      "627/1500 (epoch 209), train_loss = 0.952, time/batch = 4.348\n",
      "628/1500 (epoch 209), train_loss = 0.734, time/batch = 4.558\n",
      "629/1500 (epoch 209), train_loss = 0.893, time/batch = 4.417\n",
      "630/1500 (epoch 210), train_loss = 0.966, time/batch = 4.379\n",
      "631/1500 (epoch 210), train_loss = 0.822, time/batch = 4.400\n",
      "632/1500 (epoch 210), train_loss = 0.625, time/batch = 4.435\n",
      "633/1500 (epoch 211), train_loss = 0.997, time/batch = 4.293\n",
      "634/1500 (epoch 211), train_loss = 0.657, time/batch = 4.487\n",
      "635/1500 (epoch 211), train_loss = 0.703, time/batch = 4.376\n",
      "636/1500 (epoch 212), train_loss = 0.962, time/batch = 4.267\n",
      "637/1500 (epoch 212), train_loss = 0.635, time/batch = 4.330\n",
      "638/1500 (epoch 212), train_loss = 0.615, time/batch = 4.415\n",
      "639/1500 (epoch 213), train_loss = 1.075, time/batch = 4.338\n",
      "640/1500 (epoch 213), train_loss = 0.681, time/batch = 4.472\n",
      "641/1500 (epoch 213), train_loss = 0.822, time/batch = 4.420\n",
      "642/1500 (epoch 214), train_loss = 1.069, time/batch = 4.231\n",
      "643/1500 (epoch 214), train_loss = 0.764, time/batch = 4.343\n",
      "644/1500 (epoch 214), train_loss = 0.825, time/batch = 4.301\n",
      "645/1500 (epoch 215), train_loss = 0.908, time/batch = 4.320\n",
      "646/1500 (epoch 215), train_loss = 0.528, time/batch = 4.437\n",
      "647/1500 (epoch 215), train_loss = 0.881, time/batch = 4.363\n",
      "648/1500 (epoch 216), train_loss = 0.919, time/batch = 4.218\n",
      "649/1500 (epoch 216), train_loss = 0.668, time/batch = 4.472\n",
      "650/1500 (epoch 216), train_loss = 0.720, time/batch = 4.432\n",
      "model saved to model\n",
      "651/1500 (epoch 217), train_loss = 0.963, time/batch = 4.323\n",
      "652/1500 (epoch 217), train_loss = 0.537, time/batch = 4.494\n",
      "653/1500 (epoch 217), train_loss = 0.684, time/batch = 4.394\n",
      "654/1500 (epoch 218), train_loss = 0.980, time/batch = 4.360\n",
      "655/1500 (epoch 218), train_loss = 0.489, time/batch = 4.474\n",
      "656/1500 (epoch 218), train_loss = 1.007, time/batch = 4.444\n",
      "657/1500 (epoch 219), train_loss = 1.149, time/batch = 4.229\n",
      "658/1500 (epoch 219), train_loss = 0.749, time/batch = 4.429\n",
      "659/1500 (epoch 219), train_loss = 0.689, time/batch = 4.416\n",
      "660/1500 (epoch 220), train_loss = 1.012, time/batch = 4.536\n",
      "661/1500 (epoch 220), train_loss = 0.588, time/batch = 4.492\n",
      "662/1500 (epoch 220), train_loss = 0.681, time/batch = 4.538\n",
      "663/1500 (epoch 221), train_loss = 0.977, time/batch = 4.459\n",
      "664/1500 (epoch 221), train_loss = 0.652, time/batch = 4.330\n",
      "665/1500 (epoch 221), train_loss = 0.907, time/batch = 4.417\n",
      "666/1500 (epoch 222), train_loss = 1.058, time/batch = 4.422\n",
      "667/1500 (epoch 222), train_loss = 0.590, time/batch = 4.395\n",
      "668/1500 (epoch 222), train_loss = 0.617, time/batch = 4.493\n",
      "669/1500 (epoch 223), train_loss = 1.030, time/batch = 4.454\n",
      "670/1500 (epoch 223), train_loss = 0.564, time/batch = 4.388\n",
      "671/1500 (epoch 223), train_loss = 0.878, time/batch = 4.427\n",
      "672/1500 (epoch 224), train_loss = 0.903, time/batch = 4.429\n",
      "673/1500 (epoch 224), train_loss = 0.648, time/batch = 4.394\n",
      "674/1500 (epoch 224), train_loss = 0.762, time/batch = 4.470\n",
      "675/1500 (epoch 225), train_loss = 0.850, time/batch = 4.468\n",
      "676/1500 (epoch 225), train_loss = 0.860, time/batch = 4.390\n",
      "677/1500 (epoch 225), train_loss = 0.837, time/batch = 4.471\n",
      "678/1500 (epoch 226), train_loss = 1.085, time/batch = 4.395\n",
      "679/1500 (epoch 226), train_loss = 0.749, time/batch = 4.402\n",
      "680/1500 (epoch 226), train_loss = 0.833, time/batch = 4.531\n",
      "681/1500 (epoch 227), train_loss = 0.939, time/batch = 4.388\n",
      "682/1500 (epoch 227), train_loss = 0.517, time/batch = 4.433\n",
      "683/1500 (epoch 227), train_loss = 0.641, time/batch = 4.401\n",
      "684/1500 (epoch 228), train_loss = 0.633, time/batch = 4.437\n",
      "685/1500 (epoch 228), train_loss = 0.891, time/batch = 4.393\n",
      "686/1500 (epoch 228), train_loss = 0.429, time/batch = 4.391\n",
      "687/1500 (epoch 229), train_loss = 0.868, time/batch = 4.455\n",
      "688/1500 (epoch 229), train_loss = 0.778, time/batch = 4.357\n",
      "689/1500 (epoch 229), train_loss = 0.554, time/batch = 4.442\n",
      "690/1500 (epoch 230), train_loss = 0.820, time/batch = 4.315\n",
      "691/1500 (epoch 230), train_loss = 0.655, time/batch = 4.224\n",
      "692/1500 (epoch 230), train_loss = 0.642, time/batch = 4.369\n",
      "693/1500 (epoch 231), train_loss = 0.953, time/batch = 4.440\n",
      "694/1500 (epoch 231), train_loss = 0.661, time/batch = 4.285\n",
      "695/1500 (epoch 231), train_loss = 0.670, time/batch = 4.283\n",
      "696/1500 (epoch 232), train_loss = 0.955, time/batch = 4.457\n",
      "697/1500 (epoch 232), train_loss = 0.733, time/batch = 4.333\n",
      "698/1500 (epoch 232), train_loss = 0.732, time/batch = 4.323\n",
      "699/1500 (epoch 233), train_loss = 0.858, time/batch = 4.490\n",
      "700/1500 (epoch 233), train_loss = 0.727, time/batch = 4.330\n",
      "model saved to model\n",
      "701/1500 (epoch 233), train_loss = 0.628, time/batch = 4.503\n",
      "702/1500 (epoch 234), train_loss = 1.071, time/batch = 4.356\n",
      "703/1500 (epoch 234), train_loss = 0.759, time/batch = 4.286\n",
      "704/1500 (epoch 234), train_loss = 0.670, time/batch = 4.430\n",
      "705/1500 (epoch 235), train_loss = 0.909, time/batch = 4.290\n",
      "706/1500 (epoch 235), train_loss = 0.733, time/batch = 4.253\n",
      "707/1500 (epoch 235), train_loss = 0.606, time/batch = 4.369\n",
      "708/1500 (epoch 236), train_loss = 0.778, time/batch = 4.291\n",
      "709/1500 (epoch 236), train_loss = 0.743, time/batch = 4.458\n",
      "710/1500 (epoch 236), train_loss = 0.667, time/batch = 4.431\n",
      "711/1500 (epoch 237), train_loss = 0.701, time/batch = 4.408\n",
      "712/1500 (epoch 237), train_loss = 0.920, time/batch = 4.376\n",
      "713/1500 (epoch 237), train_loss = 0.326, time/batch = 4.532\n",
      "714/1500 (epoch 238), train_loss = 1.033, time/batch = 4.476\n",
      "715/1500 (epoch 238), train_loss = 0.849, time/batch = 4.477\n",
      "716/1500 (epoch 238), train_loss = 0.630, time/batch = 4.481\n",
      "717/1500 (epoch 239), train_loss = 0.923, time/batch = 4.392\n",
      "718/1500 (epoch 239), train_loss = 0.660, time/batch = 4.387\n",
      "719/1500 (epoch 239), train_loss = 1.026, time/batch = 4.344\n",
      "720/1500 (epoch 240), train_loss = 0.855, time/batch = 4.407\n",
      "721/1500 (epoch 240), train_loss = 0.816, time/batch = 4.467\n",
      "722/1500 (epoch 240), train_loss = 0.643, time/batch = 4.475\n",
      "723/1500 (epoch 241), train_loss = 0.854, time/batch = 4.403\n",
      "724/1500 (epoch 241), train_loss = 0.717, time/batch = 4.411\n",
      "725/1500 (epoch 241), train_loss = 0.575, time/batch = 4.371\n",
      "726/1500 (epoch 242), train_loss = 0.905, time/batch = 4.501\n",
      "727/1500 (epoch 242), train_loss = 0.574, time/batch = 4.498\n",
      "728/1500 (epoch 242), train_loss = 0.545, time/batch = 4.406\n",
      "729/1500 (epoch 243), train_loss = 0.999, time/batch = 4.324\n",
      "730/1500 (epoch 243), train_loss = 0.718, time/batch = 4.365\n",
      "731/1500 (epoch 243), train_loss = 0.665, time/batch = 4.440\n",
      "732/1500 (epoch 244), train_loss = 0.952, time/batch = 4.446\n",
      "733/1500 (epoch 244), train_loss = 0.734, time/batch = 4.497\n",
      "734/1500 (epoch 244), train_loss = 0.612, time/batch = 4.335\n",
      "735/1500 (epoch 245), train_loss = 0.848, time/batch = 4.373\n",
      "736/1500 (epoch 245), train_loss = 0.781, time/batch = 4.428\n",
      "737/1500 (epoch 245), train_loss = 0.571, time/batch = 4.408\n",
      "738/1500 (epoch 246), train_loss = 0.956, time/batch = 4.397\n",
      "739/1500 (epoch 246), train_loss = 0.660, time/batch = 4.493\n",
      "740/1500 (epoch 246), train_loss = 0.785, time/batch = 4.223\n",
      "741/1500 (epoch 247), train_loss = 0.818, time/batch = 4.335\n",
      "742/1500 (epoch 247), train_loss = 0.919, time/batch = 4.439\n",
      "743/1500 (epoch 247), train_loss = 0.530, time/batch = 4.417\n",
      "744/1500 (epoch 248), train_loss = 1.041, time/batch = 4.400\n",
      "745/1500 (epoch 248), train_loss = 0.768, time/batch = 4.345\n",
      "746/1500 (epoch 248), train_loss = 0.560, time/batch = 4.488\n",
      "747/1500 (epoch 249), train_loss = 1.023, time/batch = 4.330\n",
      "748/1500 (epoch 249), train_loss = 0.724, time/batch = 4.569\n",
      "749/1500 (epoch 249), train_loss = 0.657, time/batch = 4.258\n",
      "750/1500 (epoch 250), train_loss = 0.958, time/batch = 4.319\n",
      "model saved to model\n",
      "751/1500 (epoch 250), train_loss = 0.625, time/batch = 4.458\n",
      "752/1500 (epoch 250), train_loss = 0.738, time/batch = 4.317\n",
      "753/1500 (epoch 251), train_loss = 0.975, time/batch = 4.316\n",
      "754/1500 (epoch 251), train_loss = 0.861, time/batch = 4.398\n",
      "755/1500 (epoch 251), train_loss = 0.643, time/batch = 4.351\n",
      "756/1500 (epoch 252), train_loss = 0.945, time/batch = 4.382\n",
      "757/1500 (epoch 252), train_loss = 0.531, time/batch = 4.374\n",
      "758/1500 (epoch 252), train_loss = 0.609, time/batch = 4.313\n",
      "759/1500 (epoch 253), train_loss = 0.867, time/batch = 4.223\n",
      "760/1500 (epoch 253), train_loss = 0.628, time/batch = 4.442\n",
      "761/1500 (epoch 253), train_loss = 0.644, time/batch = 4.422\n",
      "762/1500 (epoch 254), train_loss = 0.910, time/batch = 4.355\n",
      "763/1500 (epoch 254), train_loss = 0.837, time/batch = 4.359\n",
      "764/1500 (epoch 254), train_loss = 0.145, time/batch = 4.399\n",
      "765/1500 (epoch 255), train_loss = 0.865, time/batch = 4.400\n",
      "766/1500 (epoch 255), train_loss = 0.660, time/batch = 4.413\n",
      "767/1500 (epoch 255), train_loss = 0.623, time/batch = 4.387\n",
      "768/1500 (epoch 256), train_loss = 0.888, time/batch = 4.461\n",
      "769/1500 (epoch 256), train_loss = 0.722, time/batch = 4.432\n",
      "770/1500 (epoch 256), train_loss = 0.579, time/batch = 4.428\n",
      "771/1500 (epoch 257), train_loss = 1.027, time/batch = 4.369\n",
      "772/1500 (epoch 257), train_loss = 0.660, time/batch = 4.566\n",
      "773/1500 (epoch 257), train_loss = 0.602, time/batch = 4.567\n",
      "774/1500 (epoch 258), train_loss = 1.007, time/batch = 4.364\n",
      "775/1500 (epoch 258), train_loss = 0.799, time/batch = 4.446\n",
      "776/1500 (epoch 258), train_loss = 0.615, time/batch = 4.517\n",
      "777/1500 (epoch 259), train_loss = 0.855, time/batch = 4.365\n",
      "778/1500 (epoch 259), train_loss = 0.964, time/batch = 4.440\n",
      "779/1500 (epoch 259), train_loss = 0.570, time/batch = 4.383\n",
      "780/1500 (epoch 260), train_loss = 1.071, time/batch = 4.260\n",
      "781/1500 (epoch 260), train_loss = 0.839, time/batch = 4.491\n",
      "782/1500 (epoch 260), train_loss = 0.646, time/batch = 4.379\n",
      "783/1500 (epoch 261), train_loss = 0.843, time/batch = 4.427\n",
      "784/1500 (epoch 261), train_loss = 0.744, time/batch = 4.458\n",
      "785/1500 (epoch 261), train_loss = 0.627, time/batch = 4.579\n",
      "786/1500 (epoch 262), train_loss = 0.842, time/batch = 4.320\n",
      "787/1500 (epoch 262), train_loss = 0.752, time/batch = 4.382\n",
      "788/1500 (epoch 262), train_loss = 0.915, time/batch = 4.471\n",
      "789/1500 (epoch 263), train_loss = 0.892, time/batch = 4.318\n",
      "790/1500 (epoch 263), train_loss = 0.502, time/batch = 4.323\n",
      "791/1500 (epoch 263), train_loss = 0.770, time/batch = 4.437\n",
      "792/1500 (epoch 264), train_loss = 1.069, time/batch = 4.369\n",
      "793/1500 (epoch 264), train_loss = 0.539, time/batch = 4.404\n",
      "794/1500 (epoch 264), train_loss = 0.694, time/batch = 4.438\n",
      "795/1500 (epoch 265), train_loss = 0.904, time/batch = 4.406\n",
      "796/1500 (epoch 265), train_loss = 0.648, time/batch = 4.409\n",
      "797/1500 (epoch 265), train_loss = 0.653, time/batch = 4.379\n",
      "798/1500 (epoch 266), train_loss = 0.872, time/batch = 4.363\n",
      "799/1500 (epoch 266), train_loss = 0.934, time/batch = 4.530\n",
      "800/1500 (epoch 266), train_loss = 0.584, time/batch = 4.334\n",
      "model saved to model\n",
      "801/1500 (epoch 267), train_loss = 0.874, time/batch = 4.410\n",
      "802/1500 (epoch 267), train_loss = 0.585, time/batch = 4.458\n",
      "803/1500 (epoch 267), train_loss = 0.580, time/batch = 4.461\n",
      "804/1500 (epoch 268), train_loss = 0.911, time/batch = 4.383\n",
      "805/1500 (epoch 268), train_loss = 0.615, time/batch = 4.430\n",
      "806/1500 (epoch 268), train_loss = 0.726, time/batch = 4.478\n",
      "807/1500 (epoch 269), train_loss = 0.902, time/batch = 4.389\n",
      "808/1500 (epoch 269), train_loss = 0.788, time/batch = 4.539\n",
      "809/1500 (epoch 269), train_loss = 0.273, time/batch = 4.452\n",
      "810/1500 (epoch 270), train_loss = 0.936, time/batch = 4.112\n",
      "811/1500 (epoch 270), train_loss = 0.796, time/batch = 4.536\n",
      "812/1500 (epoch 270), train_loss = 0.650, time/batch = 4.427\n",
      "813/1500 (epoch 271), train_loss = 1.033, time/batch = 4.431\n",
      "814/1500 (epoch 271), train_loss = 0.631, time/batch = 4.464\n",
      "815/1500 (epoch 271), train_loss = 0.681, time/batch = 4.415\n",
      "816/1500 (epoch 272), train_loss = 0.985, time/batch = 4.526\n",
      "817/1500 (epoch 272), train_loss = 0.776, time/batch = 4.453\n",
      "818/1500 (epoch 272), train_loss = 0.813, time/batch = 4.354\n",
      "819/1500 (epoch 273), train_loss = 0.985, time/batch = 4.417\n",
      "820/1500 (epoch 273), train_loss = 0.655, time/batch = 4.623\n",
      "821/1500 (epoch 273), train_loss = 0.669, time/batch = 4.455\n",
      "822/1500 (epoch 274), train_loss = 0.897, time/batch = 4.180\n",
      "823/1500 (epoch 274), train_loss = 0.764, time/batch = 4.361\n",
      "824/1500 (epoch 274), train_loss = 0.611, time/batch = 4.431\n",
      "825/1500 (epoch 275), train_loss = 1.023, time/batch = 4.359\n",
      "826/1500 (epoch 275), train_loss = 0.917, time/batch = 4.331\n",
      "827/1500 (epoch 275), train_loss = 0.751, time/batch = 4.509\n",
      "828/1500 (epoch 276), train_loss = 0.929, time/batch = 4.178\n",
      "829/1500 (epoch 276), train_loss = 0.592, time/batch = 4.623\n",
      "830/1500 (epoch 276), train_loss = 0.567, time/batch = 4.426\n",
      "831/1500 (epoch 277), train_loss = 0.835, time/batch = 4.297\n",
      "832/1500 (epoch 277), train_loss = 0.816, time/batch = 4.321\n",
      "833/1500 (epoch 277), train_loss = 0.598, time/batch = 4.380\n",
      "834/1500 (epoch 278), train_loss = 0.891, time/batch = 4.238\n",
      "835/1500 (epoch 278), train_loss = 0.568, time/batch = 4.375\n",
      "836/1500 (epoch 278), train_loss = 0.730, time/batch = 4.395\n",
      "837/1500 (epoch 279), train_loss = 0.908, time/batch = 4.293\n",
      "838/1500 (epoch 279), train_loss = 0.580, time/batch = 4.440\n",
      "839/1500 (epoch 279), train_loss = 0.759, time/batch = 4.444\n",
      "840/1500 (epoch 280), train_loss = 0.880, time/batch = 4.341\n",
      "841/1500 (epoch 280), train_loss = 0.737, time/batch = 4.433\n",
      "842/1500 (epoch 280), train_loss = 0.687, time/batch = 4.410\n",
      "843/1500 (epoch 281), train_loss = 0.899, time/batch = 4.388\n",
      "844/1500 (epoch 281), train_loss = 0.600, time/batch = 4.422\n",
      "845/1500 (epoch 281), train_loss = 0.596, time/batch = 4.424\n",
      "846/1500 (epoch 282), train_loss = 0.823, time/batch = 4.378\n",
      "847/1500 (epoch 282), train_loss = 0.646, time/batch = 4.422\n",
      "848/1500 (epoch 282), train_loss = 0.780, time/batch = 4.508\n",
      "849/1500 (epoch 283), train_loss = 0.926, time/batch = 4.375\n",
      "850/1500 (epoch 283), train_loss = 0.537, time/batch = 4.441\n",
      "model saved to model\n",
      "851/1500 (epoch 283), train_loss = 0.739, time/batch = 4.475\n",
      "852/1500 (epoch 284), train_loss = 0.883, time/batch = 4.463\n",
      "853/1500 (epoch 284), train_loss = 0.663, time/batch = 4.429\n",
      "854/1500 (epoch 284), train_loss = 0.558, time/batch = 4.499\n",
      "855/1500 (epoch 285), train_loss = 0.906, time/batch = 4.350\n",
      "856/1500 (epoch 285), train_loss = 0.589, time/batch = 4.429\n",
      "857/1500 (epoch 285), train_loss = 0.769, time/batch = 4.469\n",
      "858/1500 (epoch 286), train_loss = 0.929, time/batch = 4.424\n",
      "859/1500 (epoch 286), train_loss = 0.570, time/batch = 4.414\n",
      "860/1500 (epoch 286), train_loss = 0.656, time/batch = 4.450\n",
      "861/1500 (epoch 287), train_loss = 0.813, time/batch = 4.292\n",
      "862/1500 (epoch 287), train_loss = 0.809, time/batch = 4.405\n",
      "863/1500 (epoch 287), train_loss = 0.574, time/batch = 4.457\n",
      "864/1500 (epoch 288), train_loss = 0.935, time/batch = 4.503\n",
      "865/1500 (epoch 288), train_loss = 0.599, time/batch = 4.394\n",
      "866/1500 (epoch 288), train_loss = 0.846, time/batch = 4.422\n",
      "867/1500 (epoch 289), train_loss = 0.945, time/batch = 4.420\n",
      "868/1500 (epoch 289), train_loss = 0.691, time/batch = 4.408\n",
      "869/1500 (epoch 289), train_loss = 0.604, time/batch = 4.491\n",
      "870/1500 (epoch 290), train_loss = 0.854, time/batch = 4.450\n",
      "871/1500 (epoch 290), train_loss = 0.486, time/batch = 4.464\n",
      "872/1500 (epoch 290), train_loss = 0.552, time/batch = 4.461\n",
      "873/1500 (epoch 291), train_loss = 0.910, time/batch = 4.479\n",
      "874/1500 (epoch 291), train_loss = 0.782, time/batch = 4.384\n",
      "875/1500 (epoch 291), train_loss = 0.646, time/batch = 4.438\n",
      "876/1500 (epoch 292), train_loss = 0.993, time/batch = 4.512\n",
      "877/1500 (epoch 292), train_loss = 0.656, time/batch = 4.395\n",
      "878/1500 (epoch 292), train_loss = 0.598, time/batch = 4.409\n",
      "879/1500 (epoch 293), train_loss = 0.978, time/batch = 4.381\n",
      "880/1500 (epoch 293), train_loss = 0.785, time/batch = 4.291\n",
      "881/1500 (epoch 293), train_loss = 0.510, time/batch = 4.500\n",
      "882/1500 (epoch 294), train_loss = 0.931, time/batch = 4.390\n",
      "883/1500 (epoch 294), train_loss = 0.782, time/batch = 4.567\n",
      "884/1500 (epoch 294), train_loss = 0.734, time/batch = 4.451\n",
      "885/1500 (epoch 295), train_loss = 0.817, time/batch = 4.421\n",
      "886/1500 (epoch 295), train_loss = 0.417, time/batch = 4.489\n",
      "887/1500 (epoch 295), train_loss = 0.666, time/batch = 4.508\n",
      "888/1500 (epoch 296), train_loss = 0.941, time/batch = 4.528\n",
      "889/1500 (epoch 296), train_loss = 0.725, time/batch = 4.395\n",
      "890/1500 (epoch 296), train_loss = 0.610, time/batch = 4.441\n",
      "891/1500 (epoch 297), train_loss = 0.810, time/batch = 4.396\n",
      "892/1500 (epoch 297), train_loss = 0.668, time/batch = 4.380\n",
      "893/1500 (epoch 297), train_loss = 0.787, time/batch = 4.496\n",
      "894/1500 (epoch 298), train_loss = 0.818, time/batch = 4.403\n",
      "895/1500 (epoch 298), train_loss = 0.769, time/batch = 4.363\n",
      "896/1500 (epoch 298), train_loss = 0.644, time/batch = 4.507\n",
      "897/1500 (epoch 299), train_loss = 0.978, time/batch = 4.431\n",
      "898/1500 (epoch 299), train_loss = 0.985, time/batch = 4.288\n",
      "899/1500 (epoch 299), train_loss = 0.650, time/batch = 4.470\n",
      "900/1500 (epoch 300), train_loss = 0.967, time/batch = 4.482\n",
      "model saved to model\n",
      "901/1500 (epoch 300), train_loss = 0.581, time/batch = 4.352\n",
      "902/1500 (epoch 300), train_loss = 0.767, time/batch = 4.408\n",
      "903/1500 (epoch 301), train_loss = 1.064, time/batch = 4.331\n",
      "904/1500 (epoch 301), train_loss = 0.673, time/batch = 4.431\n",
      "905/1500 (epoch 301), train_loss = 0.636, time/batch = 4.498\n",
      "906/1500 (epoch 302), train_loss = 0.753, time/batch = 4.337\n",
      "907/1500 (epoch 302), train_loss = 0.543, time/batch = 4.487\n",
      "908/1500 (epoch 302), train_loss = 0.784, time/batch = 4.463\n",
      "909/1500 (epoch 303), train_loss = 0.937, time/batch = 4.488\n",
      "910/1500 (epoch 303), train_loss = 0.704, time/batch = 4.346\n",
      "911/1500 (epoch 303), train_loss = 0.520, time/batch = 4.284\n",
      "912/1500 (epoch 304), train_loss = 0.961, time/batch = 4.445\n",
      "913/1500 (epoch 304), train_loss = 0.800, time/batch = 4.399\n",
      "914/1500 (epoch 304), train_loss = 0.828, time/batch = 4.453\n",
      "915/1500 (epoch 305), train_loss = 0.809, time/batch = 4.391\n",
      "916/1500 (epoch 305), train_loss = 0.825, time/batch = 4.247\n",
      "917/1500 (epoch 305), train_loss = 0.704, time/batch = 4.377\n",
      "918/1500 (epoch 306), train_loss = 0.817, time/batch = 4.322\n",
      "919/1500 (epoch 306), train_loss = 0.605, time/batch = 4.265\n",
      "920/1500 (epoch 306), train_loss = 0.693, time/batch = 4.347\n",
      "921/1500 (epoch 307), train_loss = 0.772, time/batch = 4.318\n",
      "922/1500 (epoch 307), train_loss = 0.560, time/batch = 4.596\n",
      "923/1500 (epoch 307), train_loss = 0.524, time/batch = 4.464\n",
      "924/1500 (epoch 308), train_loss = 0.883, time/batch = 4.649\n",
      "925/1500 (epoch 308), train_loss = 0.469, time/batch = 4.380\n",
      "926/1500 (epoch 308), train_loss = 0.587, time/batch = 4.518\n",
      "927/1500 (epoch 309), train_loss = 0.988, time/batch = 4.475\n",
      "928/1500 (epoch 309), train_loss = 0.665, time/batch = 4.444\n",
      "929/1500 (epoch 309), train_loss = 0.602, time/batch = 4.459\n",
      "930/1500 (epoch 310), train_loss = 0.966, time/batch = 4.686\n",
      "931/1500 (epoch 310), train_loss = 0.869, time/batch = 4.344\n",
      "932/1500 (epoch 310), train_loss = 0.628, time/batch = 4.669\n",
      "933/1500 (epoch 311), train_loss = 0.961, time/batch = 4.467\n",
      "934/1500 (epoch 311), train_loss = 0.673, time/batch = 4.412\n",
      "935/1500 (epoch 311), train_loss = 0.458, time/batch = 4.563\n",
      "936/1500 (epoch 312), train_loss = 0.972, time/batch = 4.809\n",
      "937/1500 (epoch 312), train_loss = 0.558, time/batch = 4.555\n",
      "938/1500 (epoch 312), train_loss = 0.719, time/batch = 4.312\n",
      "939/1500 (epoch 313), train_loss = 0.919, time/batch = 4.391\n",
      "940/1500 (epoch 313), train_loss = 0.700, time/batch = 4.459\n",
      "941/1500 (epoch 313), train_loss = 0.570, time/batch = 4.484\n",
      "942/1500 (epoch 314), train_loss = 1.066, time/batch = 4.467\n",
      "943/1500 (epoch 314), train_loss = 0.977, time/batch = 4.427\n",
      "944/1500 (epoch 314), train_loss = 0.680, time/batch = 4.390\n",
      "945/1500 (epoch 315), train_loss = 0.850, time/batch = 4.438\n",
      "946/1500 (epoch 315), train_loss = 0.933, time/batch = 4.405\n",
      "947/1500 (epoch 315), train_loss = 0.484, time/batch = 4.383\n",
      "948/1500 (epoch 316), train_loss = 0.738, time/batch = 4.530\n",
      "949/1500 (epoch 316), train_loss = 0.817, time/batch = 4.521\n",
      "950/1500 (epoch 316), train_loss = 0.108, time/batch = 4.490\n",
      "model saved to model\n",
      "951/1500 (epoch 317), train_loss = 0.789, time/batch = 4.445\n",
      "952/1500 (epoch 317), train_loss = 0.529, time/batch = 4.507\n",
      "953/1500 (epoch 317), train_loss = 0.551, time/batch = 4.291\n",
      "954/1500 (epoch 318), train_loss = 0.878, time/batch = 4.462\n",
      "955/1500 (epoch 318), train_loss = 0.665, time/batch = 4.532\n",
      "956/1500 (epoch 318), train_loss = 0.590, time/batch = 4.333\n",
      "957/1500 (epoch 319), train_loss = 0.910, time/batch = 4.427\n",
      "958/1500 (epoch 319), train_loss = 0.781, time/batch = 4.583\n",
      "959/1500 (epoch 319), train_loss = 0.199, time/batch = 4.363\n",
      "960/1500 (epoch 320), train_loss = 0.823, time/batch = 4.379\n",
      "961/1500 (epoch 320), train_loss = 0.556, time/batch = 4.459\n",
      "962/1500 (epoch 320), train_loss = 0.672, time/batch = 4.132\n",
      "963/1500 (epoch 321), train_loss = 0.960, time/batch = 4.364\n",
      "964/1500 (epoch 321), train_loss = 0.728, time/batch = 4.441\n",
      "965/1500 (epoch 321), train_loss = 0.546, time/batch = 4.221\n",
      "966/1500 (epoch 322), train_loss = 1.005, time/batch = 4.436\n",
      "967/1500 (epoch 322), train_loss = 0.825, time/batch = 4.421\n",
      "968/1500 (epoch 322), train_loss = 0.644, time/batch = 4.250\n",
      "969/1500 (epoch 323), train_loss = 0.867, time/batch = 4.472\n",
      "970/1500 (epoch 323), train_loss = 0.574, time/batch = 4.452\n",
      "971/1500 (epoch 323), train_loss = 0.583, time/batch = 4.302\n",
      "972/1500 (epoch 324), train_loss = 0.869, time/batch = 4.359\n",
      "973/1500 (epoch 324), train_loss = 0.770, time/batch = 4.431\n",
      "974/1500 (epoch 324), train_loss = 0.615, time/batch = 4.398\n",
      "975/1500 (epoch 325), train_loss = 0.861, time/batch = 4.386\n",
      "976/1500 (epoch 325), train_loss = 0.498, time/batch = 4.432\n",
      "977/1500 (epoch 325), train_loss = 0.730, time/batch = 4.358\n",
      "978/1500 (epoch 326), train_loss = 0.770, time/batch = 4.432\n",
      "979/1500 (epoch 326), train_loss = 0.623, time/batch = 4.452\n",
      "980/1500 (epoch 326), train_loss = 0.719, time/batch = 4.297\n",
      "981/1500 (epoch 327), train_loss = 0.968, time/batch = 4.334\n",
      "982/1500 (epoch 327), train_loss = 0.876, time/batch = 4.460\n",
      "983/1500 (epoch 327), train_loss = 0.777, time/batch = 4.296\n",
      "984/1500 (epoch 328), train_loss = 0.874, time/batch = 4.410\n",
      "985/1500 (epoch 328), train_loss = 0.690, time/batch = 4.476\n",
      "986/1500 (epoch 328), train_loss = 0.594, time/batch = 4.503\n",
      "987/1500 (epoch 329), train_loss = 0.823, time/batch = 4.730\n",
      "988/1500 (epoch 329), train_loss = 0.558, time/batch = 4.443\n",
      "989/1500 (epoch 329), train_loss = 0.859, time/batch = 4.696\n",
      "990/1500 (epoch 330), train_loss = 0.873, time/batch = 4.444\n",
      "991/1500 (epoch 330), train_loss = 0.605, time/batch = 4.421\n",
      "992/1500 (epoch 330), train_loss = 0.581, time/batch = 4.416\n",
      "993/1500 (epoch 331), train_loss = 1.073, time/batch = 4.485\n",
      "994/1500 (epoch 331), train_loss = 0.885, time/batch = 4.424\n",
      "995/1500 (epoch 331), train_loss = 0.640, time/batch = 4.420\n",
      "996/1500 (epoch 332), train_loss = 0.911, time/batch = 4.482\n",
      "997/1500 (epoch 332), train_loss = 0.608, time/batch = 4.801\n",
      "998/1500 (epoch 332), train_loss = 0.564, time/batch = 4.442\n",
      "999/1500 (epoch 333), train_loss = 0.776, time/batch = 4.386\n",
      "1000/1500 (epoch 333), train_loss = 0.698, time/batch = 4.434\n",
      "model saved to model\n",
      "1001/1500 (epoch 333), train_loss = 0.561, time/batch = 4.417\n",
      "1002/1500 (epoch 334), train_loss = 0.905, time/batch = 4.366\n",
      "1003/1500 (epoch 334), train_loss = 0.539, time/batch = 4.657\n",
      "1004/1500 (epoch 334), train_loss = 0.721, time/batch = 4.480\n",
      "1005/1500 (epoch 335), train_loss = 1.059, time/batch = 4.502\n",
      "1006/1500 (epoch 335), train_loss = 0.755, time/batch = 6.171\n",
      "1007/1500 (epoch 335), train_loss = 0.601, time/batch = 4.889\n",
      "1008/1500 (epoch 336), train_loss = 0.907, time/batch = 5.924\n",
      "1009/1500 (epoch 336), train_loss = 0.815, time/batch = 5.394\n",
      "1010/1500 (epoch 336), train_loss = 0.733, time/batch = 5.289\n",
      "1011/1500 (epoch 337), train_loss = 0.815, time/batch = 5.136\n",
      "1012/1500 (epoch 337), train_loss = 0.595, time/batch = 4.810\n",
      "1013/1500 (epoch 337), train_loss = 0.783, time/batch = 4.844\n",
      "1014/1500 (epoch 338), train_loss = 1.000, time/batch = 4.349\n",
      "1015/1500 (epoch 338), train_loss = 0.690, time/batch = 4.885\n",
      "1016/1500 (epoch 338), train_loss = 0.506, time/batch = 4.439\n",
      "1017/1500 (epoch 339), train_loss = 0.818, time/batch = 4.324\n",
      "1018/1500 (epoch 339), train_loss = 0.728, time/batch = 4.581\n",
      "1019/1500 (epoch 339), train_loss = 0.684, time/batch = 4.814\n",
      "1020/1500 (epoch 340), train_loss = 0.775, time/batch = 4.508\n",
      "1021/1500 (epoch 340), train_loss = 0.807, time/batch = 4.445\n",
      "1022/1500 (epoch 340), train_loss = 0.781, time/batch = 4.443\n",
      "1023/1500 (epoch 341), train_loss = 0.802, time/batch = 4.250\n",
      "1024/1500 (epoch 341), train_loss = 0.688, time/batch = 4.358\n",
      "1025/1500 (epoch 341), train_loss = 0.582, time/batch = 4.273\n",
      "1026/1500 (epoch 342), train_loss = 0.933, time/batch = 4.231\n",
      "1027/1500 (epoch 342), train_loss = 0.840, time/batch = 4.433\n",
      "1028/1500 (epoch 342), train_loss = 0.663, time/batch = 4.275\n",
      "1029/1500 (epoch 343), train_loss = 0.825, time/batch = 4.289\n",
      "1030/1500 (epoch 343), train_loss = 0.784, time/batch = 4.430\n",
      "1031/1500 (epoch 343), train_loss = 0.774, time/batch = 4.333\n",
      "1032/1500 (epoch 344), train_loss = 0.893, time/batch = 4.224\n",
      "1033/1500 (epoch 344), train_loss = 0.884, time/batch = 4.443\n",
      "1034/1500 (epoch 344), train_loss = 0.553, time/batch = 4.487\n",
      "1035/1500 (epoch 345), train_loss = 0.629, time/batch = 4.349\n",
      "1036/1500 (epoch 345), train_loss = 0.791, time/batch = 4.373\n",
      "1037/1500 (epoch 345), train_loss = 0.511, time/batch = 4.461\n",
      "1038/1500 (epoch 346), train_loss = 0.788, time/batch = 4.325\n",
      "1039/1500 (epoch 346), train_loss = 0.525, time/batch = 4.444\n",
      "1040/1500 (epoch 346), train_loss = 0.541, time/batch = 4.371\n",
      "1041/1500 (epoch 347), train_loss = 0.912, time/batch = 4.618\n",
      "1042/1500 (epoch 347), train_loss = 0.546, time/batch = 4.393\n",
      "1043/1500 (epoch 347), train_loss = 0.685, time/batch = 4.423\n",
      "1044/1500 (epoch 348), train_loss = 0.633, time/batch = 4.439\n",
      "1045/1500 (epoch 348), train_loss = 0.804, time/batch = 4.345\n",
      "1046/1500 (epoch 348), train_loss = 0.189, time/batch = 4.506\n",
      "1047/1500 (epoch 349), train_loss = 0.727, time/batch = 4.505\n",
      "1048/1500 (epoch 349), train_loss = 0.773, time/batch = 4.396\n",
      "1049/1500 (epoch 349), train_loss = 0.555, time/batch = 4.427\n",
      "1050/1500 (epoch 350), train_loss = 0.980, time/batch = 4.434\n",
      "model saved to model\n",
      "1051/1500 (epoch 350), train_loss = 0.751, time/batch = 4.472\n",
      "1052/1500 (epoch 350), train_loss = 0.650, time/batch = 4.441\n",
      "1053/1500 (epoch 351), train_loss = 0.789, time/batch = 4.504\n",
      "1054/1500 (epoch 351), train_loss = 0.907, time/batch = 4.477\n",
      "1055/1500 (epoch 351), train_loss = 0.642, time/batch = 4.358\n",
      "1056/1500 (epoch 352), train_loss = 0.855, time/batch = 4.495\n",
      "1057/1500 (epoch 352), train_loss = 0.559, time/batch = 4.720\n",
      "1058/1500 (epoch 352), train_loss = 0.738, time/batch = 4.569\n",
      "1059/1500 (epoch 353), train_loss = 0.837, time/batch = 4.427\n",
      "1060/1500 (epoch 353), train_loss = 0.861, time/batch = 4.355\n",
      "1061/1500 (epoch 353), train_loss = 0.652, time/batch = 4.400\n",
      "1062/1500 (epoch 354), train_loss = 0.915, time/batch = 4.454\n",
      "1063/1500 (epoch 354), train_loss = 0.521, time/batch = 4.541\n",
      "1064/1500 (epoch 354), train_loss = 0.589, time/batch = 4.363\n",
      "1065/1500 (epoch 355), train_loss = 0.918, time/batch = 4.712\n",
      "1066/1500 (epoch 355), train_loss = 0.736, time/batch = 4.556\n",
      "1067/1500 (epoch 355), train_loss = 0.642, time/batch = 4.637\n",
      "1068/1500 (epoch 356), train_loss = 0.810, time/batch = 4.413\n",
      "1069/1500 (epoch 356), train_loss = 0.469, time/batch = 4.372\n",
      "1070/1500 (epoch 356), train_loss = 0.560, time/batch = 4.538\n",
      "1071/1500 (epoch 357), train_loss = 0.852, time/batch = 4.785\n",
      "1072/1500 (epoch 357), train_loss = 0.636, time/batch = 4.177\n",
      "1073/1500 (epoch 357), train_loss = 0.530, time/batch = 4.609\n",
      "1074/1500 (epoch 358), train_loss = 0.940, time/batch = 4.353\n",
      "1075/1500 (epoch 358), train_loss = 0.695, time/batch = 4.305\n",
      "1076/1500 (epoch 358), train_loss = 0.598, time/batch = 4.442\n",
      "1077/1500 (epoch 359), train_loss = 0.924, time/batch = 4.360\n",
      "1078/1500 (epoch 359), train_loss = 0.633, time/batch = 4.341\n",
      "1079/1500 (epoch 359), train_loss = 0.587, time/batch = 4.336\n",
      "1080/1500 (epoch 360), train_loss = 0.820, time/batch = 4.637\n",
      "1081/1500 (epoch 360), train_loss = 0.739, time/batch = 4.338\n",
      "1082/1500 (epoch 360), train_loss = 0.595, time/batch = 4.450\n",
      "1083/1500 (epoch 361), train_loss = 0.931, time/batch = 4.502\n",
      "1084/1500 (epoch 361), train_loss = 0.686, time/batch = 4.324\n",
      "1085/1500 (epoch 361), train_loss = 0.944, time/batch = 4.375\n",
      "1086/1500 (epoch 362), train_loss = 1.083, time/batch = 4.471\n",
      "1087/1500 (epoch 362), train_loss = 0.580, time/batch = 4.290\n",
      "1088/1500 (epoch 362), train_loss = 0.600, time/batch = 4.432\n",
      "1089/1500 (epoch 363), train_loss = 1.008, time/batch = 4.380\n",
      "1090/1500 (epoch 363), train_loss = 0.793, time/batch = 4.395\n",
      "1091/1500 (epoch 363), train_loss = 0.550, time/batch = 4.542\n",
      "1092/1500 (epoch 364), train_loss = 0.743, time/batch = 4.415\n",
      "1093/1500 (epoch 364), train_loss = 1.029, time/batch = 4.347\n",
      "1094/1500 (epoch 364), train_loss = 0.740, time/batch = 4.422\n",
      "1095/1500 (epoch 365), train_loss = 0.839, time/batch = 4.399\n",
      "1096/1500 (epoch 365), train_loss = 0.717, time/batch = 4.369\n",
      "1097/1500 (epoch 365), train_loss = 0.705, time/batch = 4.371\n",
      "1098/1500 (epoch 366), train_loss = 0.826, time/batch = 4.396\n",
      "1099/1500 (epoch 366), train_loss = 0.764, time/batch = 4.383\n",
      "1100/1500 (epoch 366), train_loss = 0.230, time/batch = 4.675\n",
      "model saved to model\n",
      "1101/1500 (epoch 367), train_loss = 0.933, time/batch = 4.433\n",
      "1102/1500 (epoch 367), train_loss = 0.556, time/batch = 4.391\n",
      "1103/1500 (epoch 367), train_loss = 0.495, time/batch = 4.379\n",
      "1104/1500 (epoch 368), train_loss = 0.867, time/batch = 4.440\n",
      "1105/1500 (epoch 368), train_loss = 0.855, time/batch = 4.611\n",
      "1106/1500 (epoch 368), train_loss = 0.663, time/batch = 4.680\n",
      "1107/1500 (epoch 369), train_loss = 0.899, time/batch = 4.457\n",
      "1108/1500 (epoch 369), train_loss = 0.504, time/batch = 4.689\n",
      "1109/1500 (epoch 369), train_loss = 0.628, time/batch = 4.394\n",
      "1110/1500 (epoch 370), train_loss = 0.847, time/batch = 4.675\n",
      "1111/1500 (epoch 370), train_loss = 0.524, time/batch = 4.666\n",
      "1112/1500 (epoch 370), train_loss = 0.747, time/batch = 4.319\n",
      "1113/1500 (epoch 371), train_loss = 0.896, time/batch = 4.496\n",
      "1114/1500 (epoch 371), train_loss = 0.510, time/batch = 4.496\n",
      "1115/1500 (epoch 371), train_loss = 0.658, time/batch = 4.231\n",
      "1116/1500 (epoch 372), train_loss = 0.853, time/batch = 4.416\n",
      "1117/1500 (epoch 372), train_loss = 0.639, time/batch = 4.439\n",
      "1118/1500 (epoch 372), train_loss = 0.742, time/batch = 4.401\n",
      "1119/1500 (epoch 373), train_loss = 0.831, time/batch = 4.458\n",
      "1120/1500 (epoch 373), train_loss = 0.598, time/batch = 4.567\n",
      "1121/1500 (epoch 373), train_loss = 0.667, time/batch = 4.387\n",
      "1122/1500 (epoch 374), train_loss = 0.966, time/batch = 4.443\n",
      "1123/1500 (epoch 374), train_loss = 0.704, time/batch = 4.573\n",
      "1124/1500 (epoch 374), train_loss = 0.565, time/batch = 4.399\n",
      "1125/1500 (epoch 375), train_loss = 0.974, time/batch = 4.471\n",
      "1126/1500 (epoch 375), train_loss = 0.692, time/batch = 4.387\n",
      "1127/1500 (epoch 375), train_loss = 0.713, time/batch = 4.254\n",
      "1128/1500 (epoch 376), train_loss = 0.848, time/batch = 4.478\n",
      "1129/1500 (epoch 376), train_loss = 0.636, time/batch = 4.406\n",
      "1130/1500 (epoch 376), train_loss = 0.570, time/batch = 4.414\n",
      "1131/1500 (epoch 377), train_loss = 0.598, time/batch = 4.419\n",
      "1132/1500 (epoch 377), train_loss = 0.815, time/batch = 4.432\n",
      "1133/1500 (epoch 377), train_loss = 0.620, time/batch = 4.281\n",
      "1134/1500 (epoch 378), train_loss = 0.676, time/batch = 4.479\n",
      "1135/1500 (epoch 378), train_loss = 0.643, time/batch = 4.531\n",
      "1136/1500 (epoch 378), train_loss = 0.550, time/batch = 4.373\n",
      "1137/1500 (epoch 379), train_loss = 0.903, time/batch = 4.728\n",
      "1138/1500 (epoch 379), train_loss = 0.607, time/batch = 4.355\n",
      "1139/1500 (epoch 379), train_loss = 0.546, time/batch = 4.477\n",
      "1140/1500 (epoch 380), train_loss = 0.938, time/batch = 4.432\n",
      "1141/1500 (epoch 380), train_loss = 0.590, time/batch = 4.529\n",
      "1142/1500 (epoch 380), train_loss = 0.609, time/batch = 4.315\n",
      "1143/1500 (epoch 381), train_loss = 0.897, time/batch = 4.413\n",
      "1144/1500 (epoch 381), train_loss = 0.745, time/batch = 4.443\n",
      "1145/1500 (epoch 381), train_loss = 0.577, time/batch = 4.648\n",
      "1146/1500 (epoch 382), train_loss = 0.981, time/batch = 4.366\n",
      "1147/1500 (epoch 382), train_loss = 0.596, time/batch = 4.426\n",
      "1148/1500 (epoch 382), train_loss = 0.723, time/batch = 4.480\n",
      "1149/1500 (epoch 383), train_loss = 0.882, time/batch = 4.876\n",
      "1150/1500 (epoch 383), train_loss = 0.467, time/batch = 4.558\n",
      "model saved to model\n",
      "1151/1500 (epoch 383), train_loss = 0.576, time/batch = 4.317\n",
      "1152/1500 (epoch 384), train_loss = 0.908, time/batch = 4.369\n",
      "1153/1500 (epoch 384), train_loss = 0.703, time/batch = 4.408\n",
      "1154/1500 (epoch 384), train_loss = 0.520, time/batch = 4.391\n",
      "1155/1500 (epoch 385), train_loss = 0.784, time/batch = 4.443\n",
      "1156/1500 (epoch 385), train_loss = 0.731, time/batch = 4.670\n",
      "1157/1500 (epoch 385), train_loss = 0.624, time/batch = 4.407\n",
      "1158/1500 (epoch 386), train_loss = 0.952, time/batch = 4.390\n",
      "1159/1500 (epoch 386), train_loss = 0.585, time/batch = 4.441\n",
      "1160/1500 (epoch 386), train_loss = 0.583, time/batch = 4.382\n",
      "1161/1500 (epoch 387), train_loss = 0.927, time/batch = 4.552\n",
      "1162/1500 (epoch 387), train_loss = 0.651, time/batch = 4.450\n",
      "1163/1500 (epoch 387), train_loss = 0.608, time/batch = 4.424\n",
      "1164/1500 (epoch 388), train_loss = 0.887, time/batch = 4.370\n",
      "1165/1500 (epoch 388), train_loss = 0.578, time/batch = 4.298\n",
      "1166/1500 (epoch 388), train_loss = 0.730, time/batch = 4.260\n",
      "1167/1500 (epoch 389), train_loss = 0.964, time/batch = 4.395\n",
      "1168/1500 (epoch 389), train_loss = 0.493, time/batch = 4.391\n",
      "1169/1500 (epoch 389), train_loss = 0.463, time/batch = 4.471\n",
      "1170/1500 (epoch 390), train_loss = 0.894, time/batch = 4.429\n",
      "1171/1500 (epoch 390), train_loss = 0.846, time/batch = 4.499\n",
      "1172/1500 (epoch 390), train_loss = 0.593, time/batch = 4.585\n",
      "1173/1500 (epoch 391), train_loss = 0.953, time/batch = 4.347\n",
      "1174/1500 (epoch 391), train_loss = 0.711, time/batch = 4.472\n",
      "1175/1500 (epoch 391), train_loss = 0.605, time/batch = 4.307\n",
      "1176/1500 (epoch 392), train_loss = 0.933, time/batch = 4.358\n",
      "1177/1500 (epoch 392), train_loss = 0.681, time/batch = 4.447\n",
      "1178/1500 (epoch 392), train_loss = 0.547, time/batch = 4.529\n",
      "1179/1500 (epoch 393), train_loss = 0.931, time/batch = 4.385\n",
      "1180/1500 (epoch 393), train_loss = 0.702, time/batch = 4.489\n",
      "1181/1500 (epoch 393), train_loss = 0.565, time/batch = 4.510\n",
      "1182/1500 (epoch 394), train_loss = 0.861, time/batch = 4.323\n",
      "1183/1500 (epoch 394), train_loss = 0.682, time/batch = 4.506\n",
      "1184/1500 (epoch 394), train_loss = 0.786, time/batch = 4.465\n",
      "1185/1500 (epoch 395), train_loss = 0.887, time/batch = 4.235\n",
      "1186/1500 (epoch 395), train_loss = 0.555, time/batch = 4.526\n",
      "1187/1500 (epoch 395), train_loss = 0.633, time/batch = 4.280\n",
      "1188/1500 (epoch 396), train_loss = 0.775, time/batch = 4.635\n",
      "1189/1500 (epoch 396), train_loss = 0.569, time/batch = 4.377\n",
      "1190/1500 (epoch 396), train_loss = 0.841, time/batch = 4.352\n",
      "1191/1500 (epoch 397), train_loss = 0.969, time/batch = 4.305\n",
      "1192/1500 (epoch 397), train_loss = 0.505, time/batch = 4.637\n",
      "1193/1500 (epoch 397), train_loss = 0.450, time/batch = 4.484\n",
      "1194/1500 (epoch 398), train_loss = 0.842, time/batch = 4.707\n",
      "1195/1500 (epoch 398), train_loss = 0.717, time/batch = 4.501\n",
      "1196/1500 (epoch 398), train_loss = 0.417, time/batch = 4.436\n",
      "1197/1500 (epoch 399), train_loss = 0.865, time/batch = 4.383\n",
      "1198/1500 (epoch 399), train_loss = 0.699, time/batch = 4.718\n",
      "1199/1500 (epoch 399), train_loss = 0.668, time/batch = 4.642\n",
      "1200/1500 (epoch 400), train_loss = 0.841, time/batch = 4.636\n",
      "model saved to model\n",
      "1201/1500 (epoch 400), train_loss = 0.642, time/batch = 4.651\n",
      "1202/1500 (epoch 400), train_loss = 0.603, time/batch = 4.502\n",
      "1203/1500 (epoch 401), train_loss = 0.829, time/batch = 4.479\n",
      "1204/1500 (epoch 401), train_loss = 0.850, time/batch = 4.468\n",
      "1205/1500 (epoch 401), train_loss = 0.281, time/batch = 4.464\n",
      "1206/1500 (epoch 402), train_loss = 1.040, time/batch = 4.358\n",
      "1207/1500 (epoch 402), train_loss = 0.626, time/batch = 4.426\n",
      "1208/1500 (epoch 402), train_loss = 0.478, time/batch = 4.456\n",
      "1209/1500 (epoch 403), train_loss = 1.037, time/batch = 4.415\n",
      "1210/1500 (epoch 403), train_loss = 0.634, time/batch = 4.381\n",
      "1211/1500 (epoch 403), train_loss = 0.358, time/batch = 4.499\n",
      "1212/1500 (epoch 404), train_loss = 0.815, time/batch = 4.339\n",
      "1213/1500 (epoch 404), train_loss = 0.962, time/batch = 4.389\n",
      "1214/1500 (epoch 404), train_loss = 0.675, time/batch = 4.426\n",
      "1215/1500 (epoch 405), train_loss = 0.945, time/batch = 4.517\n",
      "1216/1500 (epoch 405), train_loss = 0.594, time/batch = 4.488\n",
      "1217/1500 (epoch 405), train_loss = 0.558, time/batch = 4.487\n",
      "1218/1500 (epoch 406), train_loss = 0.912, time/batch = 4.415\n",
      "1219/1500 (epoch 406), train_loss = 0.607, time/batch = 4.425\n",
      "1220/1500 (epoch 406), train_loss = 0.564, time/batch = 4.374\n",
      "1221/1500 (epoch 407), train_loss = 0.966, time/batch = 4.572\n",
      "1222/1500 (epoch 407), train_loss = 0.747, time/batch = 4.509\n",
      "1223/1500 (epoch 407), train_loss = 0.704, time/batch = 4.546\n",
      "1224/1500 (epoch 408), train_loss = 0.891, time/batch = 4.384\n",
      "1225/1500 (epoch 408), train_loss = 0.716, time/batch = 4.406\n",
      "1226/1500 (epoch 408), train_loss = 0.568, time/batch = 4.494\n",
      "1227/1500 (epoch 409), train_loss = 0.847, time/batch = 4.434\n",
      "1228/1500 (epoch 409), train_loss = 0.625, time/batch = 4.481\n",
      "1229/1500 (epoch 409), train_loss = 0.511, time/batch = 4.648\n",
      "1230/1500 (epoch 410), train_loss = 0.848, time/batch = 4.342\n",
      "1231/1500 (epoch 410), train_loss = 0.633, time/batch = 4.653\n",
      "1232/1500 (epoch 410), train_loss = 0.538, time/batch = 4.734\n",
      "1233/1500 (epoch 411), train_loss = 0.766, time/batch = 4.313\n",
      "1234/1500 (epoch 411), train_loss = 0.812, time/batch = 4.387\n",
      "1235/1500 (epoch 411), train_loss = 0.544, time/batch = 4.625\n",
      "1236/1500 (epoch 412), train_loss = 0.941, time/batch = 4.713\n",
      "1237/1500 (epoch 412), train_loss = 0.606, time/batch = 4.499\n",
      "1238/1500 (epoch 412), train_loss = 0.581, time/batch = 4.409\n",
      "1239/1500 (epoch 413), train_loss = 0.847, time/batch = 4.345\n",
      "1240/1500 (epoch 413), train_loss = 0.770, time/batch = 4.493\n",
      "1241/1500 (epoch 413), train_loss = 0.339, time/batch = 4.589\n",
      "1242/1500 (epoch 414), train_loss = 0.916, time/batch = 4.290\n",
      "1243/1500 (epoch 414), train_loss = 0.548, time/batch = 4.484\n",
      "1244/1500 (epoch 414), train_loss = 0.706, time/batch = 4.392\n",
      "1245/1500 (epoch 415), train_loss = 0.966, time/batch = 4.442\n",
      "1246/1500 (epoch 415), train_loss = 0.471, time/batch = 4.444\n",
      "1247/1500 (epoch 415), train_loss = 0.701, time/batch = 4.464\n",
      "1248/1500 (epoch 416), train_loss = 0.881, time/batch = 4.383\n",
      "1249/1500 (epoch 416), train_loss = 0.516, time/batch = 4.608\n",
      "1250/1500 (epoch 416), train_loss = 0.499, time/batch = 4.415\n",
      "model saved to model\n",
      "1251/1500 (epoch 417), train_loss = 0.944, time/batch = 4.445\n",
      "1252/1500 (epoch 417), train_loss = 0.602, time/batch = 4.513\n",
      "1253/1500 (epoch 417), train_loss = 0.644, time/batch = 4.831\n",
      "1254/1500 (epoch 418), train_loss = 0.955, time/batch = 4.411\n",
      "1255/1500 (epoch 418), train_loss = 0.786, time/batch = 4.529\n",
      "1256/1500 (epoch 418), train_loss = 0.265, time/batch = 4.467\n",
      "1257/1500 (epoch 419), train_loss = 0.862, time/batch = 4.386\n",
      "1258/1500 (epoch 419), train_loss = 0.680, time/batch = 4.403\n",
      "1259/1500 (epoch 419), train_loss = 0.568, time/batch = 4.499\n",
      "1260/1500 (epoch 420), train_loss = 0.933, time/batch = 4.401\n",
      "1261/1500 (epoch 420), train_loss = 0.459, time/batch = 4.380\n",
      "1262/1500 (epoch 420), train_loss = 0.638, time/batch = 4.457\n",
      "1263/1500 (epoch 421), train_loss = 0.889, time/batch = 4.451\n",
      "1264/1500 (epoch 421), train_loss = 0.802, time/batch = 4.417\n",
      "1265/1500 (epoch 421), train_loss = 0.203, time/batch = 4.453\n",
      "1266/1500 (epoch 422), train_loss = 0.797, time/batch = 4.457\n",
      "1267/1500 (epoch 422), train_loss = 0.639, time/batch = 4.086\n",
      "1268/1500 (epoch 422), train_loss = 0.677, time/batch = 4.485\n",
      "1269/1500 (epoch 423), train_loss = 0.759, time/batch = 4.449\n",
      "1270/1500 (epoch 423), train_loss = 0.658, time/batch = 4.273\n",
      "1271/1500 (epoch 423), train_loss = 0.577, time/batch = 4.495\n",
      "1272/1500 (epoch 424), train_loss = 0.938, time/batch = 4.517\n",
      "1273/1500 (epoch 424), train_loss = 0.505, time/batch = 4.442\n",
      "1274/1500 (epoch 424), train_loss = 0.522, time/batch = 4.427\n",
      "1275/1500 (epoch 425), train_loss = 0.785, time/batch = 4.420\n",
      "1276/1500 (epoch 425), train_loss = 0.571, time/batch = 4.380\n",
      "1277/1500 (epoch 425), train_loss = 0.475, time/batch = 4.807\n",
      "1278/1500 (epoch 426), train_loss = 0.895, time/batch = 4.477\n",
      "1279/1500 (epoch 426), train_loss = 0.572, time/batch = 4.585\n",
      "1280/1500 (epoch 426), train_loss = 0.737, time/batch = 4.557\n",
      "1281/1500 (epoch 427), train_loss = 0.927, time/batch = 4.448\n",
      "1282/1500 (epoch 427), train_loss = 0.606, time/batch = 4.688\n",
      "1283/1500 (epoch 427), train_loss = 0.621, time/batch = 4.837\n",
      "1284/1500 (epoch 428), train_loss = 0.805, time/batch = 4.443\n",
      "1285/1500 (epoch 428), train_loss = 0.869, time/batch = 4.669\n",
      "1286/1500 (epoch 428), train_loss = 0.509, time/batch = 4.491\n",
      "1287/1500 (epoch 429), train_loss = 0.927, time/batch = 4.750\n",
      "1288/1500 (epoch 429), train_loss = 0.539, time/batch = 4.334\n",
      "1289/1500 (epoch 429), train_loss = 0.699, time/batch = 4.489\n",
      "1290/1500 (epoch 430), train_loss = 0.789, time/batch = 4.834\n",
      "1291/1500 (epoch 430), train_loss = 0.547, time/batch = 4.709\n",
      "1292/1500 (epoch 430), train_loss = 0.547, time/batch = 4.576\n",
      "1293/1500 (epoch 431), train_loss = 0.860, time/batch = 4.296\n",
      "1294/1500 (epoch 431), train_loss = 0.839, time/batch = 4.367\n",
      "1295/1500 (epoch 431), train_loss = 0.326, time/batch = 4.515\n",
      "1296/1500 (epoch 432), train_loss = 0.870, time/batch = 4.504\n",
      "1297/1500 (epoch 432), train_loss = 0.690, time/batch = 4.300\n",
      "1298/1500 (epoch 432), train_loss = 0.701, time/batch = 4.432\n",
      "1299/1500 (epoch 433), train_loss = 0.854, time/batch = 4.371\n",
      "1300/1500 (epoch 433), train_loss = 0.716, time/batch = 4.394\n",
      "model saved to model\n",
      "1301/1500 (epoch 433), train_loss = 0.555, time/batch = 4.417\n",
      "1302/1500 (epoch 434), train_loss = 0.829, time/batch = 4.350\n",
      "1303/1500 (epoch 434), train_loss = 0.556, time/batch = 4.134\n",
      "1304/1500 (epoch 434), train_loss = 0.555, time/batch = 4.409\n",
      "1305/1500 (epoch 435), train_loss = 0.562, time/batch = 4.672\n",
      "1306/1500 (epoch 435), train_loss = 0.794, time/batch = 4.379\n",
      "1307/1500 (epoch 435), train_loss = 0.533, time/batch = 4.741\n",
      "1308/1500 (epoch 436), train_loss = 0.872, time/batch = 4.444\n",
      "1309/1500 (epoch 436), train_loss = 0.830, time/batch = 4.603\n",
      "1310/1500 (epoch 436), train_loss = 0.662, time/batch = 4.533\n",
      "1311/1500 (epoch 437), train_loss = 0.789, time/batch = 4.649\n",
      "1312/1500 (epoch 437), train_loss = 0.597, time/batch = 4.473\n",
      "1313/1500 (epoch 437), train_loss = 0.676, time/batch = 4.460\n",
      "1314/1500 (epoch 438), train_loss = 0.946, time/batch = 4.502\n",
      "1315/1500 (epoch 438), train_loss = 0.724, time/batch = 4.619\n",
      "1316/1500 (epoch 438), train_loss = 0.578, time/batch = 4.557\n",
      "1317/1500 (epoch 439), train_loss = 0.806, time/batch = 4.275\n",
      "1318/1500 (epoch 439), train_loss = 0.686, time/batch = 4.547\n",
      "1319/1500 (epoch 439), train_loss = 0.739, time/batch = 4.388\n",
      "1320/1500 (epoch 440), train_loss = 0.969, time/batch = 4.499\n",
      "1321/1500 (epoch 440), train_loss = 0.653, time/batch = 4.668\n",
      "1322/1500 (epoch 440), train_loss = 0.606, time/batch = 4.363\n",
      "1323/1500 (epoch 441), train_loss = 0.990, time/batch = 4.296\n",
      "1324/1500 (epoch 441), train_loss = 0.713, time/batch = 4.549\n",
      "1325/1500 (epoch 441), train_loss = 0.535, time/batch = 4.419\n",
      "1326/1500 (epoch 442), train_loss = 0.813, time/batch = 4.561\n",
      "1327/1500 (epoch 442), train_loss = 0.629, time/batch = 4.937\n",
      "1328/1500 (epoch 442), train_loss = 0.674, time/batch = 4.420\n",
      "1329/1500 (epoch 443), train_loss = 0.802, time/batch = 4.819\n",
      "1330/1500 (epoch 443), train_loss = 0.647, time/batch = 4.606\n",
      "1331/1500 (epoch 443), train_loss = 0.565, time/batch = 4.514\n",
      "1332/1500 (epoch 444), train_loss = 0.917, time/batch = 4.400\n",
      "1333/1500 (epoch 444), train_loss = 0.864, time/batch = 4.484\n",
      "1334/1500 (epoch 444), train_loss = 0.580, time/batch = 4.439\n",
      "1335/1500 (epoch 445), train_loss = 0.880, time/batch = 4.748\n",
      "1336/1500 (epoch 445), train_loss = 0.717, time/batch = 4.646\n",
      "1337/1500 (epoch 445), train_loss = 0.657, time/batch = 4.683\n",
      "1338/1500 (epoch 446), train_loss = 0.873, time/batch = 4.620\n",
      "1339/1500 (epoch 446), train_loss = 0.802, time/batch = 4.698\n",
      "1340/1500 (epoch 446), train_loss = 0.565, time/batch = 4.644\n",
      "1341/1500 (epoch 447), train_loss = 0.779, time/batch = 4.468\n",
      "1342/1500 (epoch 447), train_loss = 0.745, time/batch = 4.511\n",
      "1343/1500 (epoch 447), train_loss = 0.427, time/batch = 4.391\n",
      "1344/1500 (epoch 448), train_loss = 0.854, time/batch = 4.476\n",
      "1345/1500 (epoch 448), train_loss = 0.566, time/batch = 4.402\n",
      "1346/1500 (epoch 448), train_loss = 0.560, time/batch = 4.358\n",
      "1347/1500 (epoch 449), train_loss = 0.903, time/batch = 4.576\n",
      "1348/1500 (epoch 449), train_loss = 0.852, time/batch = 4.482\n",
      "1349/1500 (epoch 449), train_loss = 0.563, time/batch = 4.299\n",
      "1350/1500 (epoch 450), train_loss = 0.812, time/batch = 4.423\n",
      "model saved to model\n",
      "1351/1500 (epoch 450), train_loss = 0.623, time/batch = 4.586\n",
      "1352/1500 (epoch 450), train_loss = 0.745, time/batch = 4.617\n",
      "1353/1500 (epoch 451), train_loss = 0.826, time/batch = 4.671\n",
      "1354/1500 (epoch 451), train_loss = 0.378, time/batch = 4.602\n",
      "1355/1500 (epoch 451), train_loss = 0.540, time/batch = 4.723\n",
      "1356/1500 (epoch 452), train_loss = 0.764, time/batch = 4.513\n",
      "1357/1500 (epoch 452), train_loss = 0.580, time/batch = 4.471\n",
      "1358/1500 (epoch 452), train_loss = 0.527, time/batch = 4.443\n",
      "1359/1500 (epoch 453), train_loss = 0.831, time/batch = 4.447\n",
      "1360/1500 (epoch 453), train_loss = 0.625, time/batch = 4.715\n",
      "1361/1500 (epoch 453), train_loss = 0.624, time/batch = 4.591\n",
      "1362/1500 (epoch 454), train_loss = 0.933, time/batch = 4.516\n",
      "1363/1500 (epoch 454), train_loss = 0.801, time/batch = 4.456\n",
      "1364/1500 (epoch 454), train_loss = 0.641, time/batch = 4.343\n",
      "1365/1500 (epoch 455), train_loss = 0.838, time/batch = 4.480\n",
      "1366/1500 (epoch 455), train_loss = 0.769, time/batch = 4.732\n",
      "1367/1500 (epoch 455), train_loss = 0.535, time/batch = 4.891\n",
      "1368/1500 (epoch 456), train_loss = 0.960, time/batch = 4.342\n",
      "1369/1500 (epoch 456), train_loss = 0.773, time/batch = 4.416\n",
      "1370/1500 (epoch 456), train_loss = 0.596, time/batch = 4.288\n",
      "1371/1500 (epoch 457), train_loss = 0.818, time/batch = 4.572\n",
      "1372/1500 (epoch 457), train_loss = 0.771, time/batch = 4.643\n",
      "1373/1500 (epoch 457), train_loss = 0.391, time/batch = 4.284\n",
      "1374/1500 (epoch 458), train_loss = 0.822, time/batch = 4.405\n",
      "1375/1500 (epoch 458), train_loss = 0.494, time/batch = 4.648\n",
      "1376/1500 (epoch 458), train_loss = 0.526, time/batch = 4.606\n",
      "1377/1500 (epoch 459), train_loss = 0.804, time/batch = 4.391\n",
      "1378/1500 (epoch 459), train_loss = 0.614, time/batch = 4.443\n",
      "1379/1500 (epoch 459), train_loss = 0.550, time/batch = 4.547\n",
      "1380/1500 (epoch 460), train_loss = 0.803, time/batch = 4.751\n",
      "1381/1500 (epoch 460), train_loss = 0.694, time/batch = 4.556\n",
      "1382/1500 (epoch 460), train_loss = 0.660, time/batch = 4.717\n",
      "1383/1500 (epoch 461), train_loss = 0.915, time/batch = 4.334\n",
      "1384/1500 (epoch 461), train_loss = 0.557, time/batch = 4.669\n",
      "1385/1500 (epoch 461), train_loss = 0.544, time/batch = 4.550\n",
      "1386/1500 (epoch 462), train_loss = 0.859, time/batch = 4.586\n",
      "1387/1500 (epoch 462), train_loss = 0.878, time/batch = 4.405\n",
      "1388/1500 (epoch 462), train_loss = 0.556, time/batch = 4.309\n",
      "1389/1500 (epoch 463), train_loss = 0.937, time/batch = 4.409\n",
      "1390/1500 (epoch 463), train_loss = 0.678, time/batch = 4.382\n",
      "1391/1500 (epoch 463), train_loss = 0.653, time/batch = 4.402\n",
      "1392/1500 (epoch 464), train_loss = 0.984, time/batch = 4.692\n",
      "1393/1500 (epoch 464), train_loss = 0.783, time/batch = 4.407\n",
      "1394/1500 (epoch 464), train_loss = 0.541, time/batch = 4.382\n",
      "1395/1500 (epoch 465), train_loss = 0.818, time/batch = 4.436\n",
      "1396/1500 (epoch 465), train_loss = 0.548, time/batch = 4.465\n",
      "1397/1500 (epoch 465), train_loss = 0.627, time/batch = 4.368\n",
      "1398/1500 (epoch 466), train_loss = 0.858, time/batch = 4.466\n",
      "1399/1500 (epoch 466), train_loss = 0.576, time/batch = 4.539\n",
      "1400/1500 (epoch 466), train_loss = 0.533, time/batch = 4.377\n",
      "model saved to model\n",
      "1401/1500 (epoch 467), train_loss = 0.980, time/batch = 4.501\n",
      "1402/1500 (epoch 467), train_loss = 0.678, time/batch = 4.531\n",
      "1403/1500 (epoch 467), train_loss = 0.622, time/batch = 4.383\n",
      "1404/1500 (epoch 468), train_loss = 0.782, time/batch = 4.459\n",
      "1405/1500 (epoch 468), train_loss = 0.536, time/batch = 4.548\n",
      "1406/1500 (epoch 468), train_loss = 0.484, time/batch = 4.475\n",
      "1407/1500 (epoch 469), train_loss = 0.867, time/batch = 4.418\n",
      "1408/1500 (epoch 469), train_loss = 0.473, time/batch = 4.547\n",
      "1409/1500 (epoch 469), train_loss = 0.604, time/batch = 4.424\n",
      "1410/1500 (epoch 470), train_loss = 0.969, time/batch = 4.446\n",
      "1411/1500 (epoch 470), train_loss = 0.612, time/batch = 4.420\n",
      "1412/1500 (epoch 470), train_loss = 0.619, time/batch = 4.468\n",
      "1413/1500 (epoch 471), train_loss = 0.861, time/batch = 4.631\n",
      "1414/1500 (epoch 471), train_loss = 0.531, time/batch = 4.801\n",
      "1415/1500 (epoch 471), train_loss = 0.521, time/batch = 4.538\n",
      "1416/1500 (epoch 472), train_loss = 0.878, time/batch = 4.500\n",
      "1417/1500 (epoch 472), train_loss = 0.654, time/batch = 4.432\n",
      "1418/1500 (epoch 472), train_loss = 0.488, time/batch = 4.429\n",
      "1419/1500 (epoch 473), train_loss = 1.040, time/batch = 4.484\n",
      "1420/1500 (epoch 473), train_loss = 0.543, time/batch = 4.665\n",
      "1421/1500 (epoch 473), train_loss = 0.535, time/batch = 4.518\n",
      "1422/1500 (epoch 474), train_loss = 0.824, time/batch = 4.540\n",
      "1423/1500 (epoch 474), train_loss = 0.590, time/batch = 4.408\n",
      "1424/1500 (epoch 474), train_loss = 0.593, time/batch = 4.521\n",
      "1425/1500 (epoch 475), train_loss = 0.838, time/batch = 4.484\n",
      "1426/1500 (epoch 475), train_loss = 0.550, time/batch = 4.727\n",
      "1427/1500 (epoch 475), train_loss = 0.649, time/batch = 4.549\n",
      "1428/1500 (epoch 476), train_loss = 0.807, time/batch = 4.350\n",
      "1429/1500 (epoch 476), train_loss = 0.672, time/batch = 4.605\n",
      "1430/1500 (epoch 476), train_loss = 0.935, time/batch = 4.792\n",
      "1431/1500 (epoch 477), train_loss = 0.925, time/batch = 4.582\n",
      "1432/1500 (epoch 477), train_loss = 0.799, time/batch = 4.681\n",
      "1433/1500 (epoch 477), train_loss = 0.540, time/batch = 4.530\n",
      "1434/1500 (epoch 478), train_loss = 1.021, time/batch = 4.637\n",
      "1435/1500 (epoch 478), train_loss = 0.772, time/batch = 4.385\n",
      "1436/1500 (epoch 478), train_loss = 0.261, time/batch = 4.769\n",
      "1437/1500 (epoch 479), train_loss = 0.872, time/batch = 4.356\n",
      "1438/1500 (epoch 479), train_loss = 0.745, time/batch = 5.724\n",
      "1439/1500 (epoch 479), train_loss = 0.481, time/batch = 4.497\n",
      "1440/1500 (epoch 480), train_loss = 0.925, time/batch = 4.329\n",
      "1441/1500 (epoch 480), train_loss = 0.746, time/batch = 4.420\n",
      "1442/1500 (epoch 480), train_loss = 0.557, time/batch = 4.486\n",
      "1443/1500 (epoch 481), train_loss = 0.906, time/batch = 4.373\n",
      "1444/1500 (epoch 481), train_loss = 0.625, time/batch = 4.317\n",
      "1445/1500 (epoch 481), train_loss = 0.588, time/batch = 4.474\n",
      "1446/1500 (epoch 482), train_loss = 1.038, time/batch = 4.369\n",
      "1447/1500 (epoch 482), train_loss = 0.662, time/batch = 4.325\n",
      "1448/1500 (epoch 482), train_loss = 0.577, time/batch = 4.615\n",
      "1449/1500 (epoch 483), train_loss = 0.883, time/batch = 4.307\n",
      "1450/1500 (epoch 483), train_loss = 0.549, time/batch = 4.525\n",
      "model saved to model\n",
      "1451/1500 (epoch 483), train_loss = 0.588, time/batch = 4.462\n",
      "1452/1500 (epoch 484), train_loss = 0.787, time/batch = 4.313\n",
      "1453/1500 (epoch 484), train_loss = 0.833, time/batch = 4.499\n",
      "1454/1500 (epoch 484), train_loss = 0.416, time/batch = 4.477\n",
      "1455/1500 (epoch 485), train_loss = 1.007, time/batch = 4.580\n",
      "1456/1500 (epoch 485), train_loss = 0.939, time/batch = 4.802\n",
      "1457/1500 (epoch 485), train_loss = 0.635, time/batch = 4.487\n",
      "1458/1500 (epoch 486), train_loss = 0.990, time/batch = 4.369\n",
      "1459/1500 (epoch 486), train_loss = 0.811, time/batch = 4.413\n",
      "1460/1500 (epoch 486), train_loss = 0.577, time/batch = 4.620\n",
      "1461/1500 (epoch 487), train_loss = 1.017, time/batch = 4.565\n",
      "1462/1500 (epoch 487), train_loss = 0.847, time/batch = 4.521\n",
      "1463/1500 (epoch 487), train_loss = 0.539, time/batch = 4.740\n",
      "1464/1500 (epoch 488), train_loss = 0.812, time/batch = 4.832\n",
      "1465/1500 (epoch 488), train_loss = 0.545, time/batch = 4.464\n",
      "1466/1500 (epoch 488), train_loss = 0.514, time/batch = 4.571\n",
      "1467/1500 (epoch 489), train_loss = 0.827, time/batch = 4.414\n",
      "1468/1500 (epoch 489), train_loss = 0.627, time/batch = 4.509\n",
      "1469/1500 (epoch 489), train_loss = 0.611, time/batch = 4.480\n",
      "1470/1500 (epoch 490), train_loss = 0.791, time/batch = 4.578\n",
      "1471/1500 (epoch 490), train_loss = 0.842, time/batch = 4.474\n",
      "1472/1500 (epoch 490), train_loss = 0.269, time/batch = 4.814\n",
      "1473/1500 (epoch 491), train_loss = 0.932, time/batch = 4.480\n",
      "1474/1500 (epoch 491), train_loss = 0.569, time/batch = 4.453\n",
      "1475/1500 (epoch 491), train_loss = 0.525, time/batch = 4.333\n",
      "1476/1500 (epoch 492), train_loss = 1.013, time/batch = 4.274\n",
      "1477/1500 (epoch 492), train_loss = 0.676, time/batch = 4.499\n",
      "1478/1500 (epoch 492), train_loss = 0.404, time/batch = 4.830\n",
      "1479/1500 (epoch 493), train_loss = 1.033, time/batch = 4.301\n",
      "1480/1500 (epoch 493), train_loss = 0.742, time/batch = 4.523\n",
      "1481/1500 (epoch 493), train_loss = 0.597, time/batch = 4.407\n",
      "1482/1500 (epoch 494), train_loss = 0.818, time/batch = 4.300\n",
      "1483/1500 (epoch 494), train_loss = 0.749, time/batch = 4.329\n",
      "1484/1500 (epoch 494), train_loss = 0.584, time/batch = 4.394\n",
      "1485/1500 (epoch 495), train_loss = 0.771, time/batch = 4.340\n",
      "1486/1500 (epoch 495), train_loss = 0.511, time/batch = 4.405\n",
      "1487/1500 (epoch 495), train_loss = 0.601, time/batch = 4.456\n",
      "1488/1500 (epoch 496), train_loss = 0.959, time/batch = 4.405\n",
      "1489/1500 (epoch 496), train_loss = 0.528, time/batch = 4.488\n",
      "1490/1500 (epoch 496), train_loss = 0.636, time/batch = 4.463\n",
      "1491/1500 (epoch 497), train_loss = 0.879, time/batch = 4.398\n",
      "1492/1500 (epoch 497), train_loss = 0.505, time/batch = 4.684\n",
      "1493/1500 (epoch 497), train_loss = 0.573, time/batch = 4.478\n",
      "1494/1500 (epoch 498), train_loss = 0.937, time/batch = 4.470\n",
      "1495/1500 (epoch 498), train_loss = 0.512, time/batch = 4.503\n",
      "1496/1500 (epoch 498), train_loss = 0.575, time/batch = 4.321\n",
      "1497/1500 (epoch 499), train_loss = 0.925, time/batch = 4.669\n",
      "1498/1500 (epoch 499), train_loss = 0.569, time/batch = 4.487\n",
      "1499/1500 (epoch 499), train_loss = 0.470, time/batch = 4.869\n"
     ]
    }
   ],
   "source": [
    "model = Model(args)\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    #sess.run(tf.assign(model.lr, args.learning_rate))\n",
    "    for e in range(args.num_epochs):\n",
    "        sess.run(tf.assign(model.lr, args.learning_rate * (args.decay_rate ** e)))\n",
    "        data_loader.reset_batch_pointer()\n",
    "        \n",
    "        #v_x, v_y = data_loader.validation_data()\n",
    "        #valid_feed = {x_batch: v_x, y_batch: v_y}\n",
    "        #for b in range(2):\n",
    "        #    i = e * 1 + b\n",
    "        #state = model.cell.zero_state(args.batch_size, tf.float32)\n",
    "        state = model.state_in.eval()\n",
    "        \n",
    "        for b in range(data_loader.num_batches):\n",
    "            i = e * data_loader.num_batches + b\n",
    "            start = time.time()\n",
    "            x, y = data_loader.next_batch()\n",
    "            feed = {model.input_x: x, model.input_y: y, model.state_in: state}\n",
    "            train_loss, _, state = sess.run([model.cost, model.train_op, model.state_out], feed)\n",
    "            #valid_loss, _ = sess.run([total_loss, train_step], valid_feed)\n",
    "            end = time.time()\n",
    "            print(\n",
    "                \"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"  \\\n",
    "                .format(\n",
    "                    i,\n",
    "                    args.num_epochs * data_loader.num_batches,\n",
    "                    e, \n",
    "                    train_loss, end - start))\n",
    "            if (e * data_loader.num_batches + b) % args.save_every == 0 and ((e * data_loader.num_batches + b) > 0):\n",
    "                    checkpoint_path = os.path.join(args.model_dir, 'model')\n",
    "                    saver.save(sess, checkpoint_path)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n",
    "                    with open('checkpoint', \"w\") as raw:\n",
    "                        raw.write('model_checkpoint_path: \"model\"\\nall_model_checkpoint_paths: \"model\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loading model: ', u'model')\n",
      "INFO:tensorflow:Restoring parameters from model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = Model(args, True)\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state('')\n",
    "print(\"loading model: \", ckpt.model_checkpoint_path)\n",
    "\n",
    "saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/Desktop/assignment1/.env/lib/python2.7/site-packages/ipykernel/__main__.py:124: RuntimeWarning: covariance is not positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"408.801663658\" version=\"1.1\" width=\"377.942993273\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs/><rect fill=\"white\" height=\"408.801663658\" width=\"377.942993273\" x=\"0\" y=\"0\"/><path d=\"M27.4370067537,26.2093403459 m2.19427127838,-0.0954219341278 l-1.5338429451,3.27871627808 m1.10014057159,-1.12521562576 m3.11184005737,-0.0631175875664 m4.88952789307,-0.523103237152 m3.45120391846,3.10331344604 m-1.7358423233,2.99221916199 l0.694162082672,1.19836854935 1.95796966553,-3.72929763794 m-2.98349494934,-0.826216316223 l1.64944324493,-1.17158689499 m0.427409553528,2.15099315643 m1.79677314758,-0.260470128059 m0.575229883194,-3.98627510071 m-2.55048770905,-2.15224647522 m-3.1158449173,2.04993782043 l0.285276865959,-0.565280342102 m-0.815009689331,-0.572839736938 l0.682308006287,1.05145111084 m-4.70539703369,4.15264587402 l2.19325351715,-1.88407363892 m-2.21762256622,3.61972122192 m-1.91843338013,0.622172117233 m1.09418964386,3.55784797668 m-1.50285930634,1.71131458282 m0.264730548859,1.98374080658 m2.59615516663,6.22204017639 l1.46759309769,0.970736694336 m0.309828066826,-0.477189588547 m-2.72403831482,0.54118013382 l-0.41330037117,3.0510345459 -2.83314609528,-1.49128952026 m-2.10815162659,0.750973176956 l0.36289730072,0.510579633713 m-0.0819953620434,2.11400642395 l1.65198364258,-1.71394901276 0.347927308083,-1.93802433014 l0.205153131485,0.119902634621 -0.901906108856,-1.05517816544 l-0.217494249344,1.32753286362 -0.537520694733,-3.17193889618 m-0.969335174561,0.0967816114426 m2.77451896667,0.496590328217 m-0.733552074432,1.80903072357 l-2.36509037018,-1.11888513565 m-0.546438407898,1.32119569778 m0.703512239456,4.83542404175 l0.970768356323,-0.456830596924 m-1.79962692261,-0.548813915253 m-0.651191282272,-2.17034721375 m2.07231407166,4.82748260498 m-2.30576610565,-2.17484970093 m4.62659759521,-1.0893491745 m1.82076797485,-0.00500328056514 l1.76506595612,-0.117669892311 m-3.66268348694,-0.294447803497 m1.33783416748,1.18713293076 l-1.77886981964,-1.20227813721 1.08835964203,-3.82575683594 m0.250402665138,-0.0990303397179 l-0.480174016953,3.51925964355 1.2941069603,2.6313249588 l0.53811788559,0.807005786896 m5.46851806641,2.35388965607 l0.36276922226,-0.204116868973 -1.42810630798,0.621154356003 m2.08076000214,1.69660778046 l0.00030355900526,-0.95623998642 m-0.573947429657,1.20729503632 m1.74459381104,1.61805877686 m1.91829795837,4.6151889801 m2.01273269653,0.798934030533 l-1.41890573502,-0.0783299803734 3.29846611023,-1.21066493988 m-1.53821229935,0.700895833969 l2.62584133148,1.28733291626 -1.00058507919,1.12980918884 m3.48891296387,-4.19493331909 m1.56608533859,-3.44025650024 l3.64066162109,-2.48534374237 -0.61970615387,-0.0708215296268 m1.31446800232,0.764616250992 m0.179628944397,2.19082183838 m-4.51715316772,4.11001434326 l-3.12676963806,0.232141709328 -0.189553570747,0.641999769211 m0.401937866211,0.964385223389 l0.507776832581,-4.18876991272 m7.10489425659,0.909760284424 m3.26100234985,0.245736980438 l1.13302135468,1.23568058014 -4.11678848267,-3.53568305969 m0.323453044891,-2.00411396027 l1.52427272797,3.11306648254 m8.72445602417,1.75540122986 l0.619412326813,-0.640721607208 m1.19967498779,-0.787816762924 l0.193485867977,0.72538690567 -0.0639092385769,-0.16687734127 l2.38658351898,0.849255466461 -4.00640296936,2.25995826721 m5.97766036987,-0.951428985596 l1.21424255371,-3.44892082214 m-0.919713783264,-3.0690448761 m4.2493434906,-0.00760127976537 l6.42911605835,1.23732614517 0.73516869545,-1.6021812439 m-1.08260135651,-2.40119190216 l-3.07059173584,-1.87302780151 m-0.604129219055,-0.160349225998 l0.539134120941,3.40236434937 m1.05479431152,1.49073591232 l-0.326118755341,0.943786907196 m-1.83983001709,-0.425687551498 m0.234147238731,-0.229436898232 m1.59930562973,0.514737319946 l-0.94792804718,-1.49615325928 -2.83071613312,-3.39810218811 m-2.16155719757,-3.79782142639 m5.66739845276,1.5898311615 l-1.19859199524,-4.30966339111 2.29598503113,-0.823744678497 m-2.76971092224,1.2392624855 m0.0712827384472,-0.114875936508 l0.799869728088,-2.319231987 m1.4134221077,0.419206190109 l-2.30357475281,-0.780152225494 -0.122389042377,0.603732347488 l1.3861700058,3.01209754944 -1.13623533249,-1.30220222473 l-0.577329921722,-0.899977588654 m0.337571954727,3.36774520874 l1.34441652298,1.39998292923 m-5.42299804687,4.41704788208 m-4.13580284119,2.59424591064 l-0.365079975128,-1.20188884735 1.62504272461,-0.504825639725 l0.653562784195,0.846962356567 m-2.96611099243,-4.44053421021 l0.218667030334,-0.945475482941 m2.08298873901,0.493829441071 l1.52278423309,1.22522468567 2.10685138702,0.120477843285 l-1.2515083313,3.32516021729 m-1.58093252182,-3.21893310547 l0.531325292587,-2.91674118042 0.414213323593,0.666867637634 m3.33866882324,3.71962509155 m-1.3622718811,1.23869657516 l-2.31893615723,0.218019866943 m0.367581748962,1.1512383461 m3.00731964111,3.06836528778 m-2.21234149933,-0.244928503036 l-2.52244796753,-2.10582504272 m3.02764282227,5.55832214355 m2.12961235046,4.17792129517 l1.91413536072,-0.119411599636 m-0.56861615181,0.646424865723 m2.53173923492,4.82592391968 m1.90214805603,3.93938026428 l1.91189861298,0.292440438271 m2.37648181915,-0.739321708679 l-2.86899757385,0.860042572021 m0.102292752266,0.713499927521 m-1.36805305481,-4.07479858398 m1.12369918823,-0.892193889618 l1.18186779022,-0.884062862396 -0.780703926086,-2.03377838135 l0.761977863312,-0.155138516426 -0.409338903427,0.263343572617 l-0.896161174774,-0.938726234436 m-3.39863243103,-4.30111541748 m-2.66003303528,4.44465484619 m-0.0908056378365,0.906627845764 l-0.310412836075,-2.2772315979 m4.67263450623,-0.721468925476 m4.35831756592,6.12200202942 l-2.68369827271,2.79797267914 0.265814399719,-0.0885069668293 l-1.87953586578,4.60468215942 m-3.13544692993,3.39396438599 l-1.35793123245,1.50156650543 m-1.50982017517,-0.914452171326 m1.80050640106,-5.50878219604 l1.13135852814,-2.33213634491 m3.61199493408,4.57770462036 m4.88719482422,8.62744827271 l2.46680526733,2.60584907532 1.60502624512,2.70879669189 m-3.22051544189,-0.531066131592 l-2.37209281921,0.27053527832 -1.75919876099,2.45657348633 l0.797964572906,0.809988689423 m1.4140827179,3.60502319336 l-2.52551078796,5.05637779236 m2.01445407867,6.70537261963 m3.30797042847,4.01368675232 l-0.133456420898,3.30794906616 m-0.391846442223,2.50473022461 l1.33633518219,1.59875402451 m-3.16084213257,-3.78779296875 m0.800323009491,-4.22837638855 m-0.486954593658,3.16960487366 m4.51954612732,3.5522731781 m1.84063606262,-0.970569896698 m2.52015419006,-1.72000331879 m0.790017414093,-3.95515098572 l2.72805233002,0.677380561829 m-2.60955505371,-1.27603874207 l1.71612815857,-1.23119773865 m-1.23102245331,-0.465619945526 l-1.81778507233,-2.13453598022 m-4.99843673706,2.48978729248 m0.96271686554,1.54155454636 l-0.855220222473,-0.333100533485 m-2.40399475098,2.64392662048 m-2.36699886322,-0.910399532318 l4.66132965088,2.63027744293 m5.14290618896,3.12786712646 l-0.694654750824,-3.0535369873 m1.72952728271,3.48983650208 l-0.107017064095,3.58262825012 m3.49018058777,1.84699993134 m0.914975738525,2.56378669739 l-1.57336244583,3.57643661499 m1.26307697296,-1.55857229233 m1.1069106102,1.71300201416 m1.45910081863,1.96460113525 m-4.42759132385,-3.0519952774 l-0.327616667747,2.57950248718 m2.28751182556,-4.50327644348 l0.690610599518,-1.31561489105 m3.27588043213,0.740059661865 m5.242628479,-0.196697020531 l-1.6169713974,-0.0616853296757 -2.10322685242,1.45600967407 l-0.127052164078,1.32901687622 m-0.133342599869,0.880102920532 l2.01194076538,1.19533452988 m-0.93014125824,3.23579521179 l-3.48316764832,-1.49650783539 2.43270511627,-0.417614650726 l3.50816421509,0.298029208183 m0.467375183105,-1.60758876801 m5.7807559967,1.74544487 l-0.191015696526,-1.54596147537 0.524277591705,1.26171703339 l1.09903850555,1.64501304626 m0.984285449982,1.62067909241 m1.32671060562,3.1655582428 l0.353484201431,-0.931050682068 m-5.13847961426,2.5052280426 l1.11199941635,2.58104724884 m1.82170467377,2.23508529663 l0.595929527283,-1.83640823364 -0.929888248444,1.28698396683 l-0.958419704437,0.22416241169 m-0.162672865391,2.19996376038 m1.18540267944,4.70775909424 m6.62460632324,2.06195163727 m3.62907409668,2.14333648682 m2.46567325592,-0.336914205551 m-1.27227153778,-1.81340618134 l-3.75822868347,0.136795783043 -0.136935293674,-0.896209526062 l-0.332816743851,1.45933103561 0.0649478554726,0.878535461426 l1.62960777283,-3.69192619324 -0.231784439087,-2.16982936859 l2.37884292603,-1.12435712814 -2.85861568451,1.09839677811 m1.35556755066,-0.162290072441 m1.84664516449,-0.756035518646 m1.30834598541,1.33751821518 l0.449871921539,-0.251681613922 m0.422693061829,-5.64999160767 m-3.44905319214,0.956998157501 m-3.56485977173,-0.0634236693382 l-0.185882878304,-1.45151948929 -0.841927433014,-0.931552314758 l-3.48511428833,1.75361804962 m0.227886533737,4.75331039429 l6.11981201172,3.13307304382 -0.4306599617,-6.46438293457 l1.09677419662,-1.816847229 m1.50507392883,2.35957431793 m0.911020565033,1.28459653854 l-0.0272079974413,0.467106246948 m-1.39119281769,3.24679222107 l-2.22346458435,0.320729136467 m-0.189902436733,-0.0820509195328 l-2.74153232574,-1.21849803925 -0.227840566635,-2.23355712891 l-2.50062675476,0.164382874966 -0.579113531113,0.908966255188 m5.81951675415,0.453662204742 l-0.493003606796,-0.600797176361 m1.46700582504,-0.506687688828 l0.647694683075,-0.276530957222 m0.279047608376,3.86753501892 m-0.398601293564,5.08053588867 m-0.236760234833,2.65888710022 m-3.14345760345,0.7038834095 l-2.03769893646,1.03438901901 1.45795078278,-0.327825593948 l3.11832237244,-0.185762274265 4.65412368774,-1.61468200684 m1.58849534988,3.40039215088 l0.166486668587,0.742598962784 m-0.215375447273,-1.09270648956 l1.11700906754,-2.28186206818 m0.697109985352,0.693144798279 l-1.79971942902,-2.32032680511 m1.05595722198,-2.39765815735 m-1.50759496689,-3.07774333954 l1.8340051651,2.1120464325 -1.86726760864,-0.0798380613327 m-3.27385139465,-0.820386695862 m1.30784320831,2.71734619141 m0.362937474251,3.45680389404 l-0.280143713951,0.103429937363 m-0.281143712997,-5.3375125885 m0.899360084534,2.78347797394 l1.21177768707,-1.27415428162 m-1.57055568695,-1.7699716568 l-0.676251411438,-1.45440664291 0.0957467198372,-0.418346977234 m-1.26464576721,0.435030412674 l-0.226785039902,0.00848103761673 0.865353107452,-2.81130256653 l0.510114479065,0.935472679138 m-0.102699303627,1.83719482422 m0.935761356354,0.185204958916 l-1.30109004974,-0.411484956741 m-3.97021789551,-3.41786231995 l1.70315704346,-2.11377239227 0.904948139191,0.372472572327 l-1.79492683411,1.07743911743 2.2588470459,-0.174796056747 l-2.96937828064,1.79804363251 m-1.9714969635,-2.52276573181 l2.47142887115,-0.882781791687 m1.10624446869,-2.4900056839 l-0.648084878922,2.4344575882 -3.73283004761,0.147946572304 m-2.57845859528,0.443946838379 m5.51004829407,2.59270820618 l1.68055744171,-1.951836586 3.3506893158,0.714537668228 l0.219118213654,0.0172120854259 -2.48252658844,0.580543327332 m-2.69446144104,-3.04996891022 l0.466800117493,-2.82546577454 m-2.95403823853,1.78160305023 l3.34584236145,-3.10266208649 -0.317076587677,-2.41186599731 l-4.85673866272,-0.53908033371 m-0.0704907774925,-4.09024772644 m-0.518315839767,-1.44504785538 l0.250965571404,3.17670269012 m-5.22826843262,4.74934158325 l1.86262531281,-0.57040886879 m2.43696365356,-1.53638954163 l-1.68925170898,0.386863470078 m-1.60690898895,-0.990496063232 l1.90936355591,2.07183685303 m-2.01935157776,3.32967376709 m2.65212516785,6.4393951416 m2.65401668549,5.11268081665 m3.61645011902,1.74008197784 l0.632240724564,-2.05952911377 -0.126129579544,0.0180701524019 m1.01566896439,0.360030698776 l0.213202881813,-3.23111190796 m-3.83513755798,-2.31767578125 m1.1360956192,0.946703243256 l-1.7173871994,1.03713312149 m3.29693641663,-0.736017274857 m3.6805316925,1.46616783142 m-3.62283172607,0.153311395645 m-2.45628948212,-0.78419213295 m0.359788441658,0.631928014755 l-1.93849182129,-2.41830539703 0.780817556381,-1.68723278046 m3.66427154541,-4.29857559204 m2.11389846802,0.683372402191 l2.67698116302,2.03935470581 m1.23581962585,5.3550453186 l1.06623830795,0.240919494629 1.66499881744,1.07016239166 m-1.04099855423,-1.22320613861 m1.32859792709,0.269036817551 m-2.67589206696,-3.86769485474 m2.13925018311,-1.37252731323 m0.924624633789,-1.79162445068 l-1.45752820969,1.99543533325 -1.31862335205,2.15902347565 l-0.457254362106,-1.61265201569 -2.06385116577,3.07508106232 m7.53715057373,8.48982543945 m0.238575387001,-0.11875770092 l-0.346815729141,0.543292713165 m7.06379318237,-1.26997613907 m-0.753699064255,0.948488998413 l1.68936634064,-3.2886177063 1.01104545593,1.52826576233 m1.37114181519,0.0342147380114 m-0.0152432098985,2.08835945129 l1.6429523468,3.91015510559 0.780476760864,2.68539829254 m-0.266970443726,9.50920028687 l-1.30553970337,1.22168464661 m1.77719135284,5.64884147644 l1.52293863297,2.90075702667 m4.36546897888,2.10319423676 m-0.621866941452,1.22861719131 l1.49945659637,-0.407688188553 0.317592763901,-2.57012557983 m0.919870281219,-0.121332192421 m0.441957139969,2.45855731964 l-0.453736019135,0.656582641602 2.32518482208,2.90962104797 l1.96578178406,-1.37037963867 1.0921339035,-3.1119468689 l-3.0771780014,2.65715370178 0.606463479996,1.42489004135 m0.986346244812,0.331948542595 m4.03701248169,1.06055393219 m-1.23972787857,2.32079410553 m-1.55071239471,2.60447864532 l0.640953540802,3.4011516571 m2.67777042389,-1.00167379379 m-1.381133461,-1.27524518967 m2.45854530334,2.22181091309 l2.32574863434,1.7031578064 -0.00507023409009,0.570684814453 l-0.273884749413,0.631314468384 m-1.62556114197,1.84563426971 l-0.990196037292,-2.63752155304 -0.410074424744,1.37739562988 l-1.32289028168,-0.93505115509 -0.00731914788485,0.189459371567 m-0.100795257092,0.544144105911 m-3.29399490356,-2.16401290894 l-0.317223668098,0.254915046692 -1.29974346161,-1.8472076416 l-1.68599357605,2.03026123047 m-6.34410324097,2.28730163574 m1.11558017731,0.665112113953 m-3.42677154541,-3.71164932251 l-3.10579566956,1.26065807343 m-1.66926021576,3.38413391113 l0.587414360046,-0.366845178604 m0.721346330643,-3.74480895996 m3.66574707031,0.800356483459 l-0.219133520126,-2.94226341248 -2.77493877411,3.0519197464 m3.82080612183,-0.449599456787 l3.0394361496,2.2895160675 -1.86541996002,-2.02530460358 l0.107436192036,-2.63189926147 m-2.65826568604,2.02591972351 l-2.10047187805,0.495599889755 m-3.86084022522,2.41940612793 l-0.7412774086,0.865454387665 m-2.85739421844,0.545833301544 l-0.14178378582,0.181788396835 1.37823009491,-0.892803859711 m1.57009487152,-2.9557384491 m1.92125205994,-0.605606937408 l0.846032714844,1.62180938721 -3.57658348083,-1.3546505928 m-0.408616352081,0.422701406479 l2.3802192688,-3.4886592865 2.47379665375,-3.50508041382 m1.29927005768,2.13106937408 l0.460971593857,1.55876502991 1.75729236603,-1.056590271 l2.85577793121,0.123426926136 0.164339780807,-0.766118240356 m-0.374060606956,-3.83548545837 l-1.10926046371,-3.59438896179 0.12010859251,-3.61397857666 l-2.47681427002,-0.379154324532 m0.884453582764,2.18814678192 l1.35896120071,2.39678974152 -0.304343795776,2.07176856995 m1.17464265823,4.27969284058 l-0.472229385376,-1.24205989838 0.653517150879,0.303403663635 m-1.42104616165,-4.63495559692 m0.186757254601,0.0914445519447 m1.37526340485,-1.12479295731 l1.00324306488,0.278548049927 1.00267314911,-2.63184928894 l-1.73201465607,1.77822246552 0.352986550331,1.39105644226 m-1.9459936142,2.00268821716 m0.401520776749,1.7104352951 m2.01311664581,3.45741500854 m-0.480744171143,-3.47350349426 l-2.32642269135,-1.58415355682 m-0.944618797302,-3.07498397827 l-2.99282531738,-0.106214046478 m-1.08613700867,2.11892223358 l-0.312595820427,1.60739021301 0.01347348243,1.99790496826 m-0.631000900269,3.03213882446 l1.39412021637,1.15088882446 m3.12497787476,1.12642154694 m-4.97387733459,-5.32889976501 m1.37166042328,-0.0117075592279 m4.45248832703,0.0662890315056 l2.43879432678,1.94080371857 2.45068092346,-0.799854183197 m-3.54703903198,-0.210079455376 l-2.05125007629,1.85207462311 0.036776393652,-0.922723770142 m-0.328435540199,4.84394111633 m3.05121059418,1.3874045372 m2.73098258972,4.87177848816 l-0.832354927063,-1.8009809494 -1.92007541656,1.70074424744 l-3.33747406006,-0.567156124115 m1.68552799225,-0.274207639694 m-0.312257313728,6.93678970337 m1.65040187836,4.06501922607 l-2.40541229248,-0.340434741974 -0.166267812252,-1.77666931152 l0.253037405014,-0.930867290497 m2.49257202148,-0.638251304626 m5.17848510742,1.51708879471 l-1.46495838165,-0.0678970873356 -2.40693969727,1.92378921509 m0.264177083969,1.49401464462 l2.13981666565,-0.0477155476809 2.46357936859,-1.40939760208 m0.89384355545,1.12250051498 l1.58867111206,-2.75240516663 m0.106666040421,-1.72982234955 l-1.04746055603,-0.57828207016 m-4.09571151733,0.674142837524 l0.958770561218,2.69045600891 1.34010429382,-0.296553277969 m2.61909694672,-0.104063379765 m0.946446037292,0.671937274933 m-0.812880516052,-1.97039718628 m0.845652008057,1.44755535126 l-2.05865497589,-0.70087480545 m0.402982568741,3.01762084961 l-1.81705684662,1.53142566681 m1.57776451111,-0.560645103455 m-0.261706256866,0.280018353462 m1.23501434326,4.3304901123 m4.43438949585,1.19603595734 m0.258378624916,-0.850165748596 m3.40566482544,-4.77636947632 m-0.59070930481,2.39317054749 m1.90171794891,2.51475162506 l-0.35899412632,-1.5965461731 m0.0468613266945,-3.9965549469 m-1.64748935699,-0.930683135986 m3.17447280884,0.740322113037 l-0.28783056736,0.178220617771 0.537673377991,-3.9145866394 l-1.11459150314,0.82347574234 m-1.18454446793,-3.28073997498 l-1.20452079773,0.104164540768 2.1339799881,-1.62245025635 l-3.85957565308,-3.07273712158 -3.16712112427,3.97979698181 m-1.50094261169,2.19972610474 l-1.69019126892,1.97216339111 -0.709538555145,-0.161473333836 l0.6538210392,0.880883789062 m3.22570838928,2.57553062439 m-1.60913009644,-0.230659031868 l3.06755104065,-0.999856758118 -1.07760896683,-2.25215568542 l-1.88157062531,-0.139581990242 m1.15260744095,1.74977970123 m-3.03609733582,-1.22674007416 l-0.972817230225,0.609299659729 2.41562652588,2.07666969299 l3.71286201477,-1.60849971771 -1.3027929306,-1.19544801712 m3.17286663055,1.53707075119 m2.81637535095,1.48535881042 m-1.85828266144,-2.98988342285 l0.950329589844,-3.72566452026 m0.513149166107,-0.65570063591 l-0.965946006775,-1.41615343094 m-0.525414180756,0.520651292801 l0.358063554764,2.90959968567 -3.78388290405,0.474041748047 m-0.0421061396599,3.88814239502 m5.31713180542,-0.128397727013 m2.38645248413,1.32626228333 m-0.646941280365,-0.633744812012 l5.36996650696,-1.68193054199 1.10408058167,-0.229658937454 l0.356727218628,-0.296365475655 0.716036367416,-1.58316421509 m3.52464523315,-3.71237869263 m-0.534023714066,-1.37188005447 m-3.21906242371,0.256677889824 l-4.37108955383,0.117169439793 m0.861936855316,0.903568649292 m5.86875,3.62344322205 m1.26498613358,0.797232913971 m1.02461805344,-0.714240121841 m2.61162109375,0.61221075058 m-1.08889198303,1.36170558929 m-2.34917793274,-0.625025463104 l1.03790464401,-1.4107213974 1.05882978439,-1.15558853149 l1.41786136627,1.25281581879 m1.6686712265,-0.594617080688 l-1.50310440063,-0.242636847496 -3.43636817932,0.42084646225 m-1.72941493988,3.98309440613 m0.467432260513,2.24555358887 l-0.979420948029,-2.41520214081 2.8482673645,3.95079917908 m1.59247093201,2.92653427124 m4.34018363953,2.91240291595 l3.42610206604,-0.785416030884 m-1.97781162262,-1.2169547081 m1.04473981857,1.94804153442 m3.86334571838,-4.15237350464 l0.177672505379,-2.80354480743 -0.778196239471,2.6802980423 m-3.06462478638,6.17139587402 l-2.31292514801,0.322297048569 -0.243974399567,-1.11707162857 l-3.25963821411,0.283088445663 m-4.03170967102,-3.61657104492 l-0.105616402626,-1.70602321625 1.57414131165,-1.54688034058 m-0.570112323761,-1.36158094406 l-2.4054391861,1.80603961945 -1.68446769714,3.9802482605 l-0.55314207077,2.21176700592 m-1.55817050934,0.300089883804 l-0.476471233368,0.482763195038 -3.29678840637,-0.876422786713 m2.4203660965,-0.455213832855 l-2.14656047821,-3.98528747559 m3.75386123657,-0.0567530035973 l0.0367442071438,-1.49104413986 -1.8792137146,1.88791847229 l-1.15378837585,-1.37233657837 0.0849500596523,1.03355798721 m-2.04219856262,2.71255970001 l0.799853134155,2.75053253174 m-1.41149520874,0.577287864685 m0.314055800438,1.90333480835 m0.379807519913,0.42025437355 m-1.47590923309,-3.19452781677 m-1.88265361786,0.642798042297 m2.63778800964,2.62754936218 l-0.880226707458,-2.90601921082 -0.889605140686,0.0697754859924 m-5.09939041138,-2.03777809143 m-2.48991756439,0.849347019196 m-0.54554567337,1.44389734268 m5.8688659668,4.97892570496 l5.59319229126,0.583967685699 m4.08060150146,2.00713691711 l-2.49309272766,-2.77014274597 0.340895700455,-1.85524311066 l-0.193955647945,0.242998886108 m4.07321968079,1.77747840881 m2.09349460602,-1.07303104401 m3.67285957336,-0.716391038895 l1.32224788666,1.87106399536 m-4.21617584229,-0.0856920778751 m0.658986949921,3.24042015076 l0.887438583374,2.86736011505 2.89140167236,2.33786354065 l-1.03312339783,5.93877182007 m-10.1632507324,-1.2313876152 l-0.509250164032,-1.73371162415 2.08877410889,-1.33212451935 l-3.8971031189,0.294538211823 m-1.11617450714,0.748029947281 m-2.24193611145,-0.138535451889 l-1.21441469193,2.70855140686 m5.69290313721,1.71915359497 m3.52629966736,-4.44589080811 l-0.853389167786,0.487469100952 m3.78497772217,-1.34668140411 m0.465726804733,0.592106056213 m-0.715163898468,0.570207691193 l-2.0669883728,1.54016218185 -1.22580060959,1.43339824677 l-2.44959602356,-0.333793878555 m4.37737350464,7.19985733032 m6.24331855774,7.63827819824 l1.24676055908,3.5604019165 -2.11699314117,1.05273590088 l1.53926734924,4.93319473267 m-1.22966489792,2.30613021851 m-0.441353225708,-2.26414642334 l0.338575839996,0.58044629097 2.03925819397,2.60133781433 l0.903084754944,-0.392601680756 m0.232212400436,0.93579158783 l-0.932673358917,-3.64862632751 -0.847462368011,-1.08394804001 l-1.0734005928,2.78413009644 -0.168496584892,-0.363146448135 m-0.0858048677444,-3.34511642456 m-2.26765346527,-1.04609498978 l2.17641601563,-2.56740150452 -0.435661220551,-0.878322505951 m0.631369686127,-2.27619400024 l1.95837497711,-1.41292819977 0.899043273926,1.28261651993 m0.615330886841,1.9910615921 m2.71125602722,1.91367588043 m3.77366294861,3.45637054443 m6.06660652161,-0.895132160187 m1.74143371582,1.11203584671 m2.819272995,3.76359710693 m6.51496429443,2.59751472473 m-1.51194877625,-4.09859313965 m-0.550284957886,-2.2166343689 l1.39182291031,-2.21186466217 1.85573005676,-1.36221618652 m0.0137625843287,2.11379623413 l-2.71177864075,3.0081817627 m-2.47924079895,-0.575503635406 l0.145657348633,-0.493183803558 m-1.2959651947,0.947899436951 l2.23607368469,-3.15235328674 2.12493495941,-1.13866767883 l0.644779729843,0.990509796143 m3.47483940125,0.671482467651 m2.49848003387,1.36908712387 m-0.945867156982,1.6154882431 m-0.893093299866,5.72802200317 m1.20946836472,5.30445899963 m1.71880893707,-1.07198133469 m0.853766345978,0.92430562973 l0.0773258805275,4.06903266907 -1.22262077332,-0.868768405914 l-1.72394790649,-1.18931388855 m0.565637159348,0.706405687332 m-0.847681522369,2.11628704071 l2.43972682953,-0.392563486099 m-3.08592700958,-2.7062461853 l2.87917995453,-0.833352661133 m6.8044303894,-3.2914100647 l-2.15011405945,1.21125335693 0.994058895111,1.12321624756 m-0.806883239746,0.137636232376 l1.3931892395,-1.2168794632 m2.91957454681,3.6715511322 m0.445449304581,2.24716777802 l-0.0760283946991,0.252001547813 -0.707883453369,4.10248908997 l-1.37725811005,0.380702018738 -2.84849662781,0.189838027954 l0.815519237518,2.5552280426 m2.64492073059,-2.53385314941 l0.573941898346,-0.0835530698299 -0.40250544548,0.912299728394 l-0.235233139992,-0.75564455986 m-0.213369512558,-1.21175823212 m0.987781715393,0.554111242294 l2.2225856781,1.45607833862 1.93601379395,0.972765541077 m1.56812610626,-1.98592700958 l2.08003501892,-0.200436329842 -0.886107444763,0.906513977051 m2.98638801575,2.02586212158 m-0.25098965168,5.76094856262 m-1.95441741943,0.194786298275 m0.723203277588,-2.96928386688 m4.34909095764,2.15357627869 l3.13206195831,3.23591156006 m3.51444549561,4.6076713562 l1.42520236969,4.46644210815 m4.3855430603,7.43218688965 m5.08251724243,2.31059684753 l-4.0885219574,0.909206295013 3.02785987854,-2.98008518219 l0.942636299133,-0.916870212555 -3.07037944794,2.95503330231 m0.73369717598,1.95808143616 l-3.04614543915,-0.559652090073 m2.90872840881,1.89305057526 m-0.597867250443,-0.812765884399 l-2.80691108704,-0.333830332756 m-0.547514820099,1.01520290375 m2.84334335327,0.127387547493 l3.88721847534,-0.0347055971622 -3.79260482788,1.37658205032 l0.897919559479,2.27015972137 0.13444378376,-0.272455787659 l2.16442775726,-0.770813417435 4.62733535767,0.29985332489 m2.69887123108,-1.92557735443 m5.06656990051,5.04190597534 m4.10560150146,0.782965564728 l3.74633522034,0.281150794029 m1.11636333466,-2.31102924347 l0.647370910645,-0.155841803551 -3.36126747131,-1.2627158165 l-1.47607288361,-0.709848690033 -0.403113937378,-0.887033653259 l1.14657974243,-0.991436862946 -3.8204914093,2.04999809265 l-0.46701965332,0.522956848145 m5.27614784241,2.63561420441 l1.12330055237,0.710338973999 m2.68529338837,0.643499469757 m1.54245843887,-1.42404937744 m-2.31325511932,-1.61807041168 l-1.82384662628,2.2055480957 m-2.16555633545,-3.15071716309 m1.56806669235,-0.252150774002 l-1.35278091431,-1.09280223846 0.765205001831,1.10441398621 l2.73997840881,-0.987390136719 1.46279449463,-0.0538125634193 l-3.25834960938,2.82775840759 0.008498506248,0.314572691917 m-1.78218479156,0.616787910461 m-1.45780830383,0.970485877991 l2.21879920959,1.61476249695 -1.74578742981,0.707749891281 m-1.63260974884,-1.59946155548 l-2.76271514893,-1.68050365448 -0.275413751602,-0.861245727539 m1.96790313721,0.515994548798 l2.5518819809,1.22820367813 1.47849359512,-0.797650814056 l0.510273361206,-2.50001373291 m1.1062122345,-1.15161027908 l-1.10039844513,2.07231826782 m-1.81022491455,-5.02380371094 m3.07397289276,2.52827224731 l2.11554641724,-4.00520515442 -3.71861572266,-2.70128669739 m2.79040241241,1.09854564667 m0.700626850128,5.84177017212 l5.0696434021,5.00588989258 m0.277125477791,0.769668388367 l0.842930221558,-1.81290416718 m2.40340328217,-0.199734306335 l-1.20853633881,0.667314624786 3.91998596191,-1.17593917847 m-2.77199249268,2.11734981537 m-2.34303569794,-0.480266284943 m0.107375895977,-0.867982292175 l-0.875539588928,0.271251440048 5.09181175232,-0.962774562836 m-4.01685333252,1.03623685837 l-1.46276330948,1.50284690857 3.79798316956,-0.577436494827 m2.99618530273,2.76399593353 m0.620167970657,-0.431302261353 m3.39321708679,-2.0873796463 m3.86329574585,-0.106234407425 l-0.475555992126,-2.41915245056 0.10669567585,2.63000469208 m-0.750850725174,4.58452796936 l-1.18981466293,-0.00486499294639 1.26105976105,1.04987258911 m2.28177280426,-0.599876213074 l-1.87795028687,-2.92896938324 m0.842853069305,0.890723705292 l0.398337030411,4.08372459412 1.18956575394,-4.20910186768 m-2.47933425903,-4.17918243408 l0.869726371765,0.826211547852 m2.278074646,-0.287690496445 l-0.0374678313732,-0.293417358398 -0.712918233871,-2.2147102356 m0.443336105347,-2.62458324432 l-0.800147819519,1.97644577026 -1.07655696869,3.2954460144 l-0.387416028976,-2.71274089813 m-0.785048913956,1.22470712662 l0.636553049088,1.36306114197 -1.36930217743,-1.87879238129 l-0.645395374298,-1.9448633194 0.0936731636524,-0.514612865448 m2.33885154724,1.89977645874 m4.0433391571,3.15518989563 l0.306693172455,-0.303424811363 -1.67603797913,-0.167792034149 l-1.40276260376,0.281480646133 -3.28876342773,-1.24970769882 m-0.361909270287,2.78975696564 m-0.285957121849,2.20709972382 l1.37629489899,2.47561798096 m-2.6191778183,-1.4945558548 l-2.59605407715,1.26829948425 m-2.84272918701,-3.52781181335 l-1.07684326172,-1.95219726562 0.140372812748,-0.233750391006 l0.236891555786,-2.07297782898 m-0.338382601738,2.47401046753 l0.26496720314,-1.79182128906 -0.821770858765,-1.4704990387 m1.7635433197,0.521775770187 l-2.9578950882,1.44866771698 -0.812127494812,3.82876358032 l-0.820431041718,3.25706214905 m1.16903123856,3.32502822876 m2.86192417145,-0.618931245804 m-1.49031152725,-4.413230896 l-0.921947574615,-3.97540512085 -0.55063085556,1.61431217194 m2.13018245697,-3.11174850464 l0.691897773743,-0.766359329224 m-4.5446395874,-0.269006156921 m-0.712593269348,-0.321278715134 m4.10927429199,-6.26480636597 l0.840574836731,-3.78537597656 0.13518269062,-0.784024000168 l-0.44159321785,0.0871836185455 -2.5275100708,-2.4285326004 m2.31419677734,0.160836684704 l3.01641044617,-0.475695180893 m0.501854133606,6.22193145752 m0.942346668243,-1.11701402664 l2.27094345093,-1.98834533691 0.0159005060792,2.03371543884 m2.59641952515,-2.65075397491 m1.11975135803,5.17537879944 m0.473248577118,-7.12845153809 l0.347120261192,-2.19489059448 -1.63153305054,-0.687242984772 l-0.177603435516,-2.51005001068 m1.73198814392,4.81768379211 m0.256690788269,-0.888297176361 l-0.93225107193,3.5888923645 3.39901351929,3.53921928406 l-1.5190495491,0.349837779999 m-0.254564714432,3.73353004456 m-0.168289756775,-1.96389541626 m0.396162557602,-1.20117816925 m0.999901294708,1.49791822433 l0.580903911591,2.93933563232 m-2.44806175232,0.578868627548 l-3.06053829193,-0.482217884064 0.715636205673,0.722179222107 m0.839158821106,0.730526018143 m2.22864170074,2.11044006348 l-2.3578792572,-3.52624893188 -0.542665672302,-0.217884206772 l0.672394180298,-0.449451494217 -0.457570505142,-3.29633407593 m-2.56518516541,-0.189679741859 l-0.458359527588,-0.153440666199 m-0.684397125244,-0.875708580017 l1.02565689087,-1.9671749115 m-2.42790622711,-2.4696313858 m5.04079284668,1.01973342896 m2.8453710556,-2.67625217438 l-0.706770849228,-0.0443842738867 m-3.78944206238,-3.90900230408 l-2.48105602264,-0.678903865814 m-0.642300653458,-0.439974117279 l-0.298257350922,0.899854278564 1.08648376465,-2.81389846802 l-2.16047306061,-0.294411063194 m-0.314598608017,-1.33935117722 l0.0499504983425,-1.97798500061 m-3.54908294678,-2.7270860672 m3.5444732666,2.32285861969 m-0.33394613266,-0.378646993637 m3.44768829346,-0.701391935349 l3.63352928162,-2.12473964691 -0.166834425926,-2.3573381424 l2.41538143158,-4.03235778809 m0.402552700043,1.20471458435 m7.85298538208,-1.27718582153 l-2.17221946716,2.65606098175 m-0.809436035156,2.94463062286 m-1.2740064621,3.74313964844 l-2.15656242371,0.78711104393 m-1.23457984924,0.341460561752 m-0.0314263731241,-4.41612434387 m0.923055267334,-0.700848197937 m-0.886221599579,3.50126152039 m-0.833884334564,-4.12906951904 m-0.00118008051068,2.19268817902 m2.39319972992,3.89406776428 m-2.22461986542,1.16729640961 l1.31746864319,-2.35125427246 m3.05633163452,-0.0210822448134 l2.98052501678,-0.0889945924282 2.63444137573,1.62425231934 l-1.2989068985,-1.86631278992 m1.33930101395,1.30084028244 m5.05022392273,0.368964958191 m2.98543376923,4.58930549622 m-0.932249927521,-1.182218647 l0.533921432495,1.03341236115 m-0.408801364899,-2.96447467804 l-2.67229003906,-0.39503569603 0.211228346825,0.649123239517 m-3.78053741455,-3.51075363159 m-1.70097942352,2.57928085327 l-1.13765258789,-2.70781860352 m-3.12515239716,0.806279277802 m2.87578315735,2.60807037354 m1.58948516846,-0.44286532402 m-1.81560726166,-0.23256084919 m-0.949475193024,-1.75579109192 m0.364873695374,2.98106365204 l-2.18988208771,2.6993768692 1.63654670715,-2.41487865448 m-0.500427722931,-5.44809112549 l-3.42526931763,0.891670417786 -2.15269947052,2.18462142944 l2.7607208252,1.45089187622 m0.30545437336,1.5175775528 l-1.44875564575,-0.0930620312691 -2.20645637512,-2.43442325592 l-0.303247475624,-1.43928442001 0.146275115013,-1.25742759705 l-0.069756937027,-2.67986221313 m-1.85361118317,-0.112374258041 m-1.17317094803,1.23614654541 m-0.251990056038,-1.45222358704 l2.50127487183,-0.253983449936 m-0.836335277557,-0.651153612137 m-4.18015861511,0.563918495178 m-1.5426281929,-0.10057605505 m3.45839691162,-0.795998573303 m2.83626976013,2.21606445312 l0.3786039114,1.61634407043 m-3.56164855957,-2.05443649292 l0.722701120377,-0.700024032593 m2.15016002655,1.63977394104 m-1.48224411011,-1.19477748871 l0.115895569324,-2.88025474548 0.266647696495,0.779061126709 l3.69930992126,0.375037550926 m-4.46391143799,-3.16599636078 m-0.0334980636835,-3.8430065155 l0.856609725952,2.40805740356 m-3.67014541626,2.09293670654 l-1.30028343201,2.02394447327 1.69902591705,-1.08799953461 l0.81209526062,2.02148799896 -1.58365707397,-0.705538320541 m-1.17294006348,0.551970243454 l4.20868606567,-4.67414550781 m1.30474815369,2.7710269928 m-0.109875822067,-4.32021484375 l-0.941924762726,-0.0759407818317 0.872416687012,-0.926643180847 l-0.621089410782,2.50196361542 m-1.1100725174,1.21911869049 l1.35377292633,1.04766521454 0.650241947174,0.464562034607 m1.19341335297,0.133720636368 l0.411717224121,-0.0371649801731 m-1.22416200638,0.648364114761 m0.562720966339,2.09092788696 m0.38664290905,2.34025173187 m1.15023622513,-2.28518695831 m-2.00058078766,-0.640683555603 l0.705143737793,2.03730163574 m3.35075302124,1.30625829697 m2.96073894501,0.988516139984 m-0.874510192871,-1.36885976791 l-0.679271650314,1.0584649086 -1.38965053558,-0.926071929932 m3.40986633301,3.09161987305 l0.739386177063,4.55515975952 m-3.64882278442,-5.85507888794 m0.872373485565,-2.68288002014 l2.79172363281,-2.00468654633 1.84059696198,1.15484790802 l-0.774604272842,2.67031002045 m-4.13237953186,-0.297829651833 m-0.931153392792,-1.84429092407 m4.20780525208,-2.11179199219 l0.710388612747,-0.744658851624 m2.76076145172,5.82081565857 m3.05232009888,4.01435546875 m-0.79014339447,2.71500244141 m5.16788253784,-0.382678985596 l0.506459140778,-1.06243400574 m-1.44694480896,0.812174797058 l0.952908420563,-0.0316100001335 -0.384389877319,-0.227319145203 l-2.21289634705,3.19668369293 0.405583763123,-0.701152229309 m-0.119331610203,2.55971317291 m-2.73401565552,-3.87859802246 l-1.52117118835,0.0682779014111 m2.68563652039,-1.95135002136 m-1.01348724365,2.8691242218 m-1.76318778992,-2.81056365967 l0.358536458015,1.82916297913 m-0.307544517517,-1.5354013443 m0.873671913147,2.28747577667 l0.635502099991,-0.273136019707 m3.07315673828,2.75439243317 l-1.42740736008,-2.8654882431 -0.521874856949,5.82849960327 l0.144064545631,-1.62093238831 m0.696068334579,-0.647779941559 l2.78368110657,0.846326065063 2.09274501801,-2.42340965271 m3.0119682312,1.09290714264 m1.93878631592,3.75404510498 l2.40760879517,4.15965690613 1.00056085587,-1.16542062759 m1.89498214722,2.85173339844 m-1.53263492584,1.73866291046 l-0.738503074646,0.108267354965 2.44028930664,-0.840517902374 m-1.29151306152,2.11019535065 l-0.641377973557,-0.00809459984303 -0.890153980255,-1.714320755 l0.646373748779,-0.365534543991 -1.19905776978,-0.142968964577 l2.45068206787,1.16729650497 1.23839960098,0.789821910858 l-3.68600120544,0.505489778519 -1.9614824295,-0.2951785326 m-1.36483516693,0.330859303474 l2.19920349121,2.39194641113 m4.04372367859,-0.232832098007 l-2.47272567749,0.226497602463 0.381291866302,-0.157432866096 l-0.0558813631535,1.66547241211 m-2.75824260712,0.859730052948 m2.25331268311,1.39266223907 m-2.57325592041,0.362922143936 m1.32271175385,2.82550239563 l-0.597653675079,3.4245513916 -2.97988090515,-1.49258670807 l0.726020860672,-1.56060161591 1.64773921967,0.0412926346064 m1.8571056366,0.316171526909 m1.05435142517,3.28722877502 m0.86867351532,4.16951446533 l3.48074264526,3.40190200806 m-0.52544927597,-3.20107116699 l-3.68615264893,2.1554353714 0.557169151306,-1.49567995071 l2.57531452179,-1.5847650528 m0.141665327549,0.0498014658689 l-0.830419921875,0.487789392471 2.03340053558,1.86319046021 l2.60124359131,1.69016227722 0.00617974698544,4.40769119263 l-2.32116603851,0.234340453148 -2.08566455841,1.41885385513 l1.34062614441,-1.54089641571 m0.581243419647,-3.12007923126 l-0.228955030441,-0.474720907211 m1.74302806854,0.0239241793752 l3.61381149292,-1.68789329529 m0.998645210266,-0.456311511993 l1.84291877747,-1.61748142242 2.7609079361,-4.2883228302 m1.40224876404,-1.76349716187 m2.22535610199,-0.655131864548 l-1.06768226624,0.58575720787 2.66976547241,4.38832015991 m4.5228427887,3.76683578491 m-0.264821529388,-0.602373409271 m0.456361055374,-0.690904712677 l-4.35363349915,0.92082529068 -0.610084724426,-0.822737884521 l-2.00334739685,0.630748510361 5.5414680481,-2.95014362335 l-2.44508247375,1.98244152069 1.14606571198,1.53438482285 m0.800931167603,4.2593914032 m-1.22494049072,0.683156681061 l-3.38377227783,0.070252007246 0.66908288002,2.02485294342 m1.89408569336,-2.5309469223 l2.80728607178,0.217571687698 -2.5582862854,0.638925457001 m-0.467346382141,1.6291601181 m0.551885509491,1.80124416351 l1.6319196701,3.3056842804 -0.388840174675,0.5881316185 l0.957830524445,-0.626128292084 -0.99778585434,0.286930584908 m-3.80153465271,-0.209195709229 m1.28813657761,-0.227269077301 m0.639627981186,2.45300273895 m-3.33551750183,0.344266700745 l-2.67151565552,0.739789915085 m0.755111932755,2.72264881134 \" fill=\"none\" stroke=\"black\" stroke-width=\"1\"/></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sample_stroke():\n",
    "    [strokes, params] = model.sample(sess, 1200)\n",
    "    draw_strokes(strokes, factor=10, svg_filename = 'sample'+'.normal.svg')\n",
    "\n",
    "sample_stroke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
